version: '3.9'

services:
  # GPU Model Service using TensorRT-LLM REST API Server
  gpu-model-service:
    build:
      context: .
      dockerfile: docker/Dockerfile.trt-server # We use a new Dockerfile for the server
    container_name: ai-gpu-service-trt
    restart: unless-stopped
    ports:
      # Exposing the API port to the host for direct access/testing
      - "127.0.0.1:8000:8000"
    volumes:
      # Mount the pre-built TensorRT engine (read-only)
      - /mnt/ai-data/models/trtllm-engine/Mistral-7B-v0.3-int4-awq:/app/trt_engine:ro
      # Mount the original HF model for the tokenizer (read-only)
      - /mnt/ai-data/models/hf/Mistral-7B-v0.3:/app/tokenizer:ro
      # Volume for logs
      - ./logs/gpu:/app/logs
    networks:
      - ai-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    # The command to start the persistent REST API server
    command: >
      python3 /app/rest_api_server.py
      --engine_dir /app/trt_engine
      --tokenizer_dir /app/tokenizer
      --host 0.0.0.0
      --port 8000
    healthcheck:
      # The server provides a /health endpoint
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 300s # Give the model plenty of time to load

  cpu_server:
    build:
      context: .
      dockerfile: Dockerfile.cpu
    container_name: llama_cpp_server
    ports:
      - "8001:8001"
    volumes:
      - ./src:/app/src
      - ./models/gguf_models:/app/models # Mount your GGUF models here
    command: >
      python src/cpu_server.py
      --model /app/models/llama-3.1-8b-instruct.Q4_K_M.gguf
      --host 0.0.0.0
      --port 8001
      --n_threads 16
    restart: unless-stopped
  
  # Open WebUI Service
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    restart: unless-stopped
    ports:
      - "3000:8080" # Access WebUI from http://localhost:3000
    volumes:
      - open-webui-data:/app/backend/data
    networks:
      - ai-network
    # This tells Open WebUI how to find and configure the backend model
    environment:
      # Set the base URL for the OpenAI-compatible API
      - 'OPENAI_API_BASE_URL=http://gpu-model-service:8000/v1'
      # Since the API is internal to Docker, no key is needed
      - 'OPENAI_API_KEY=NONE'
      # Automatically import the model configuration into Open WebUI on startup
      - 'WEBUI_IMPORTS=[{"name":"Mistral 7B (TRT-LLM)","model":"mistral-7b-v0.3-int4-awq","baseURL":"http://gpu-model-service:8000/v1","apiKey":"NONE"}]'
    depends_on:
      gpu-model-service:
        condition: service_healthy # Wait for the GPU service to be healthy before starting

# Define a shared volume for Open WebUI data
volumes:
  open-webui-data:

# Define the internal network for services to communicate
networks:
  ai-network:
    driver: bridge
    internal: false # Set to false to allow pulling images, can be true in production
    ipam:
      config:
        - subnet: 172.20.0.0/16
