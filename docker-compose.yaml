version: '3.9'

services:
  llama-cpu:
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - IPC_LOCK
    deploy:
      resources:
        limits:
          cpus: '12' 
          memory: 96G # Increased for single high-performance instance
    ulimits:
      memlock:
        soft: -1
        hard: -1
    build:
      context: .
      dockerfile: docker/llama-cpu/Dockerfile.llama-cpu
    container_name: llama-cpu # Single latency-optimized CPU inference
    restart: unless-stopped
    cpuset: "0-11" 
    environment:
      - SERVER_PORT=8001
      # Model path for inference
      - MODEL_PATH=${LLAMA_CPU_MODEL}
      - THREADS=12 # 12 threads for optimal performance
      - THREADS_BATCH=12
    ports:
      # API port binding
      - "127.0.0.1:8001:8001"
    read_only: true 
    volumes:
      # Model storage mount
      - /mnt/ai-data/models/:/app/models:ro
      # Log storage
      - ./logs/cpu:/app/logs
    networks:
      - ai-network
    healthcheck:
      # Service health monitoring
      test: ["CMD", "curl", "-f", "http://localhost:8001/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 180s # Give the CPU model time to load into RAM

  llama-gpu:
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    ulimits:
      memlock:
        soft: -1
        hard: -1
    build:
      context: .
      dockerfile: docker/Dockerfile.llama-gpu
    container_name: llama-gpu
    restart: unless-stopped
    # runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - CUDA_VISIBLE_DEVICES=0
      - SERVER_PORT=8004
      - MODEL_PATH=${LLAMA_GPU_MODEL}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu, compute, utility]
    ports:
      # GPU API port binding
      - "127.0.0.1:8004:8004"
    read_only: true
    tmpfs:
      - /tmp:size=4G,mode=1777  # CUDA cache and temp files
    volumes:
      # Model storage mount
      - /mnt/ai-data/models/:/app/models:ro
      # Log storage
      - ./logs/gpu:/app/logs
    networks:
      - ai-network
    healthcheck:
      # Service health monitoring
      test: ["CMD", "curl", "-f", "http://localhost:8004/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 180s # Give the GPU model time to load into VRAM 

  # vLLM GPU Service - Optimized for RTX 5090
  vllm-gpu:
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    ulimits:
      memlock:
        soft: -1
        hard: -1
    build:
      context: .
      dockerfile: docker/Dockerfile.vllm-gpu
    container_name: vllm-gpu
    restart: unless-stopped
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - CUDA_VISIBLE_DEVICES=0
      - MODEL_PATH=${VLLM_GPU_MODEL}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu, compute, utility]
    ports:
      # vLLM API port binding
      - "127.0.0.1:8005:8005"
    read_only: true
    tmpfs:
      - /tmp:size=4G,mode=1777  # For CUDA cache and temp files
    volumes:
      # Model storage mount
      - /mnt/ai-data/models/:/app/models:ro
      # Log storage
      - ./logs/vllm:/app/logs
    networks:
      - ai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8005/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 300s # vLLM needs time to load the model

  
  # Open WebUI Service
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    restart: unless-stopped
    ports:
      - "3000:8080" # Access WebUI from http://localhost:3000
    read_only: true
    volumes:
      - open-webui-data:/app/backend/data
    networks:
      - ai-network
    # Backend service configuration
    environment:
      # API endpoint configuration
      - 'OPENAI_API_BASE_URL=http://llama-gpu:8004/v1'
      # Internal API access
      - 'OPENAI_API_KEY=NONE'
      # Model configuration import
      - 'WEBUI_IMPORTS=[{"name":"qwen3 coder","model":"qwen3 coder","baseURL":"http://llama-gpu:8004/v1","apiKey":"NONE"}]'
    # depends_on:
    #   llama-gpu:
    #     condition: service_healthy # Wait for the GPU service to be healthy before starting

  # ComfyUI with FLUX.1-dev for image generation
  comfyui-flux:
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    ulimits:
      memlock:
        soft: -1
        hard: -1
    build:
      context: .
      dockerfile: docker/comfyui-flux/Dockerfile.comfyui-flux
    container_name: comfyui-flux
    restart: unless-stopped
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - CUDA_VISIBLE_DEVICES=0
      - COMFYUI_PORT=8188
      - MODEL_BASE_PATH=/app/models
      - OUTPUT_PATH=/app/output
      - VRAM_MODE=${FLUX_VRAM_MODE:-highvram}
    deploy:
      resources:
        limits:
          memory: 24G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu, compute, utility]
    ports:
      # ComfyUI web interface
      - "127.0.0.1:8188:8188"
    # read_only disabled as ComfyUI needs write access for manager and settings
    # Security maintained through: non-root user, resource limits, network isolation
    tmpfs:
      - /tmp:size=4G,mode=1777
      - /app/temp:size=2G,mode=1777
      - /app/input:size=1G,mode=1777
      - /app/.cache:size=1G,mode=1777
      - /app/user:size=100M,mode=1777
    volumes:
      # Model storage mount
      - /mnt/ai-data/models:/app/models:ro
      # Output directory for generated images
      - ./outputs/comfyui:/app/output
      # Log storage
      - ./logs/comfyui:/app/logs
      # ComfyUI workflows
      - ./workflows:/app/user/default/workflows
    networks:
      - ai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8188/system_stats"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 180s

# Persistent storage
volumes:
  open-webui-data:

# Service network
networks:
  ai-network:
    driver: bridge
    internal: false # Set to false to allow pulling images, can be true in production
    ipam:
      config:
        - subnet: 172.20.0.0/16


