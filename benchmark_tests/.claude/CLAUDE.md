# Benchmark Tests - Core Requirements & Testing Documentation

## PROJECT STATUS: Core Calibration Focus

**Primary Goal:** Calibrate @benchmark_tests/evaluator/ for production deployment
**Current Priority:** Systematic evaluator calibration (base easyâ†’hard, then instruct easyâ†’hard)
**Status:** Scripts organized, core modules created, testing required

---

# âš ï¸ CRITICAL EXECUTION POLICY âš ï¸

## NEVER RUN PYTHON OR CURL COMMANDS DIRECTLY 

**ðŸš¨ MANDATORY RULE: All Python debugging/logic commands AND curl commands to Docker containers MUST be added to @Makefile first, then executed via `make` commands**

### âŒ FORBIDDEN:
```bash
python3 -c "some debug code"
python some_script.py  
python3 -m some.module
curl http://localhost:8004/health
curl -X POST http://localhost:8004/v1/completions -d '{"prompt":"test"}'
```

### âœ… REQUIRED PROCESS:
1. Add debug/curl command to @Makefile with descriptive target name
2. Use `make your-debug-target` to execute
3. This ensures consistent execution environment and project standards

### Examples:
```makefile
debug-paths:
	python3 -c "import os; print('Current dir:', os.getcwd())"

test-domain-discovery:
	python3 -c "from benchmark_runner import TestSuiteManager; print('Testing domain discovery...')"

check-server-health:
	curl -s http://localhost:8004/health | jq . || echo "Server not responding"

test-completion-api:
	curl -s -X POST -H "Content-Type: application/json" \
	-d '{"prompt":"Hello","max_tokens":5}' \
	http://localhost:8004/v1/completions | jq .
```

**This policy ensures all execution follows project conventions and maintains consistency for both Python execution and Docker container interaction.**

---

## CODE ATTRIBUTION POLICY

**ðŸš¨ NEVER ADD ATTRIBUTION LINES TO FILES**

**Forbidden Attribution Patterns:**
- `Author: Claude Code`
- `Version: X.X.X` 
- `**Auditor:** Claude Code`
- `Generated by Claude Code`
- `Created by Claude Code`
- Any AI-generated attribution headers
- Any mention of "Claude Code" as creator/author/auditor

**Why:** This codebase maintains clean, professional code without AI attribution markers. All code should appear as natural, human-written code without revealing AI assistance.

**Enforcement:** The repository has been systematically cleaned of all attribution lines. Do not re-introduce them in any new files or modifications.

---

## TESTING REQUIREMENTS

### **ðŸš¨ CRITICAL STATUS: TESTING INFRASTRUCTURE CRISIS**
- **13/819 functional tests FAILING** (Priority P0 - blocking all production work)
- **57% code coverage** (Target: 90% - UNACCEPTABLE gap)
- **vLLM backend detection BROKEN** (concurrent functionality unusable)

**ðŸ“‹ For detailed crisis analysis and 4-week tactical resolution plan, see:**
**@docs/TESTING_INFRASTRUCTURE_CRISIS_RESPONSE_PLAN.md**

### **Test Structure & Coverage Requirements:**
```
tests/
â”œâ”€â”€ unit/          - Individual component tests (TARGET: 90%+ coverage)
â”œâ”€â”€ integration/   - End-to-end workflow tests  
â”œâ”€â”€ functional/    - Live server tests (MUST PASS 100% - NO EXCEPTIONS)
â””â”€â”€ calibration/   - Production calibration validation
```

### **Priority Test Creation (REQUIRED):**
1. **Core Modules** (0-20% coverage â†’ 95%): cognitive_validation.py, resource_manager.py
2. **Calibration Engine** (20% coverage â†’ 90%): calibration_engine.py, production_calibration.py  
3. **Scripts Organization**: Verify reorganized scripts/ import paths work correctly
4. **vLLM Concurrent Testing**: Fix backend detection and concurrent execution with vLLM

### **Mandatory Regression Testing:**
```bash
# CRITICAL: Must pass 100% before ANY production work
make test-functional-validation
make test-regression-full

# Coverage validation
pytest --cov=evaluator --cov=core --cov-fail-under=90
```

**Success Criteria**: See CRITICAL SUCCESS METRICS section below for complete checklist

---

## EVALUATOR INTEGRATION STATUS

### Currently Available Sophisticated Evaluators:
- âœ… PatternBasedEvaluator - Working, integrated
- â“ CulturalAuthenticityAnalyzer - Partially integrated  
- â“ EnhancedUniversalEvaluator - Framework ready, needs testing
- â“ BiasAnalysis/EvaluationAggregator - Framework ready, needs testing

### Integration Requirements:
- Replace basic heuristic scoring with sophisticated evaluator results
- Ensure evaluator results serialize properly for storage
- Handle evaluator failures gracefully with fallback scoring
- Statistical validation of evaluator consistency

---

## CALIBRATION METHODOLOGY

### Statistical Requirements:
- **Multi-sample validation:** 3-5 runs per test for statistical significance
- **Confidence intervals:** Â±95% confidence for pattern detection
- **Effect size calculation:** Cohen's d for practical significance
- **Cross-validation:** Ensure patterns are robust across test sets

### Quality Thresholds:
- **âœ… Excellent Calibration:** Â±2 points from target (within statistical variance)
- **ðŸŸ¡ Good Calibration:** Â±5 points from target (production acceptable)
- **ðŸŸ  Needs Calibration:** Â±10 points from target (requires adjustment)
- **âŒ Calibration Broken:** >10 points from target (system failure)

### Production Readiness Criteria:
- **Base Models:** 70%+ tests achieve Good Calibration or better
- **Instruct Models:** 70%+ tests achieve Good Calibration or better  
- **Cross-Domain Consistency:** <10% variance between domains
- **Statistical Significance:** p<0.05 for detected patterns

---

## IMPLEMENTATION PRIORITIES

### **ðŸš¨ CRITICAL: TESTING-FIRST APPROACH ðŸš¨**

**MANDATORY PREREQUISITE**: No production domain benchmarking until testing foundation is solid

### **Phase 1: Testing Infrastructure Crisis Resolution (CURRENT PRIORITY)**
**ðŸ“‹ Detailed tactical plan:** @docs/TESTING_INFRASTRUCTURE_CRISIS_RESPONSE_PLAN.md

**Goal**: 100% functional test success + 90% code coverage + vLLM concurrent testing
- **Week 1-2**: Fix 13 failing tests, resolve backend detection issues
- **Week 3-4**: Achieve 90% coverage, optimize performance (eliminate 300s/180s timeouts)
- **Timeline**: 4-week systematic resolution with daily milestones

### **Phase 2: Production Domain Calibration** âœ… **READY**
**Prerequisites**: âœ… All Phase 1 success criteria achieved

**Systematic Base Model Calibration:**
- **Command**: `make systematic-base-calibration`
- **Progression**: Automatic easy â†’ medium â†’ hard with halt-on-failure  
- **Domains**: reasoning, creativity, language, social, knowledge, integration
- **Quality Gates**: â‰¥70 score (production acceptable), â‰¥80 score (excellent)

**Instruct Model Calibration:**
- Easy â†’ Medium â†’ Hard progression building on base model results
- Cross-domain consistency validation
- Production readiness assessment

### **Phase 3: Production Deployment**
- Statistical validation reporting
- Performance benchmarking across hardware configurations  
- Production deployment documentation

---

## ðŸŽ¯ CRITICAL SUCCESS METRICS (MUST ACHIEVE BEFORE DOMAIN BENCHMARKING)

### **Phase 1 Completion Criteria:**
- **âœ… 0/819 test failures** (currently 13 failing)
- **âœ… 90%+ total test coverage** (currently 57% - UNACCEPTABLE) 
- **âœ… 95%+ core/ module coverage** (many currently 0-20%)
- **âœ… 100% functional/ test success** (tests/functional/ MUST pass completely)
- **âœ… vLLM concurrent functionality tested** (fix test_backend_type_detection_vllm)
- **âœ… All timeout issues resolved** (300s, 180s concurrent execution failures)
- **âœ… Missing test data created** ('linux_test_01', 'basic_01', 'pattern_completion')
- **âœ… Backend detection working** for both llama.cpp and vLLM

### **Mandatory Validation Commands:**
```bash
# MUST pass 100% before ANY domain benchmarking
make test-functional-validation
make test-regression-full

# Coverage must meet requirements
pytest --cov=evaluator --cov=core --cov-fail-under=90
```

### **ðŸš¨ DEVELOPMENT POLICY ðŸš¨**
**NO production domains/ benchmarking until ALL success metrics achieved**
**NO exceptions - testing infrastructure MUST be solid first**

---

## vLLM CONCURRENT TESTING REQUIREMENTS

**ðŸš¨ CRITICAL ISSUE**: vLLM backend detection broken - `detect_backend_type()` returns 'llama.cpp' instead of 'vLLM'

**Key Requirements:**
- **Backend Detection**: Automatic llama.cpp vs vLLM detection for concurrent execution
- **Performance Validation**: vLLM concurrent execution must outperform sequential
- **Timeout Resolution**: Fix 300s/180s timeout failures in concurrent tests

**ðŸ“‹ Detailed implementation strategy and specific failing tests:** @docs/TESTING_INFRASTRUCTURE_CRISIS_RESPONSE_PLAN.md (Category D: Backend Detection Issues)

---

## PERFORMANCE REQUIREMENTS

### Hardware Optimization (RTX 5090 + AMD 9950X + 128GB DDR5):
- **Response Time:** <30 seconds per domain evaluation
- **Memory Usage:** <16GB peak usage during evaluation
- **GPU Utilization:** Efficient GGUF model loading and inference
- **Concurrent Processing:** Support for multiple domain evaluation

### Scalability Requirements:
- **26,000+ Test Suite:** Full suite completion within reasonable time
- **Cross-Model Comparison:** Support for multiple model endpoints
- **Result Storage:** Efficient storage and retrieval of evaluation results
- **Pattern Analysis:** Real-time cognitive pattern detection

---

## DOCUMENTATION REQUIREMENTS

### Code Documentation:
- **Docstrings:** All public methods with parameter and return type documentation
- **Type Hints:** Complete type annotations for all functions and classes
- **Examples:** Working code examples for each major component
- **Error Handling:** Documentation of all exception cases

### User Documentation:
- **Calibration Guide:** Step-by-step calibration procedures
- **API Reference:** Complete reference for all evaluator interfaces  
- **Troubleshooting:** Common issues and solutions
- **Performance Tuning:** Optimization guidelines for different hardware

---

## QUALITY ASSURANCE

**âœ… Comprehensive Quality Audit Completed**  
For detailed system analysis, issue prioritization, and implementation roadmap, see:
- **ðŸ“„ @docs/COMPREHENSIVE_QUALITY_AUDIT.md** - Complete system analysis with prioritized fixes
- **ðŸ“‹ @docs/TESTING_INFRASTRUCTURE_CRISIS_RESPONSE_PLAN.md** - Tactical plan for resolving 13 failing tests and achieving 90% coverage
- **ðŸ“– @docs/guides/MAKEFILE_REFERENCE.md** - Complete command documentation

### Quality Status:
- **Overall Assessment:** GOOD (7.5/10) - Production ready with critical fixes applied
- **Critical Issues:** âœ… Resolved (backend detection, missing scripts, smart concurrency)
- **Performance Tests:** âœ… Created for core modules  
- **Unit Tests:** âœ… Added for missing core modules
- **Smart Concurrency:** âœ… Implemented with backend detection (llama.cpp vs vLLM)

### Key Improvements Implemented:
- **ðŸ” Backend Detection:** Automatic llama.cpp vs vLLM detection for concurrency control
- **ðŸ“ Missing Scripts:** Created systematic_base_calibration.py and multi_model_benchmarking.py
- **âš¡ Performance Tests:** Comprehensive performance validation for 26k+ test suite scalability
- **ðŸ§ª Unit Tests:** Complete coverage for production_calibration.py and benchmarking_engine.py

---

**ðŸš¨ CRITICAL REMINDER:** TESTING INFRASTRUCTURE MUST BE FIXED FIRST! 
- âœ… 100% functional test success (currently 13/819 failing)
- âœ… 90% code coverage (currently 57% - UNACCEPTABLE) 
- âœ… vLLM concurrent functionality fully tested
- **NO domains/ benchmarking until ALL tests pass and coverage requirements are met**
- **NO exceptions - fix the foundation before building on top of it**