# Benchmark Tests - Core Requirements & Testing Documentation

## PROJECT STATUS: Core Calibration Focus

**Primary Goal:** Calibrate @benchmark_tests/evaluator/ for production deployment
**Current Priority:** Systematic evaluator calibration (base easy→hard, then instruct easy→hard)
**Status:** Scripts organized, core modules created, testing required

---

# ⚠️ CRITICAL EXECUTION POLICY ⚠️

## NEVER RUN PYTHON OR CURL COMMANDS DIRECTLY 

**🚨 MANDATORY RULE: All Python debugging/logic commands AND curl commands to Docker containers MUST be added to @Makefile first, then executed via `make` commands**

### ❌ FORBIDDEN:
```bash
python3 -c "some debug code"
python some_script.py  
python3 -m some.module
curl http://localhost:8004/health
curl -X POST http://localhost:8004/v1/completions -d '{"prompt":"test"}'
```

### ✅ REQUIRED PROCESS:
1. Add debug/curl command to @Makefile with descriptive target name
2. Use `make your-debug-target` to execute
3. This ensures consistent execution environment and project standards

### Examples:
```makefile
debug-paths:
	python3 -c "import os; print('Current dir:', os.getcwd())"

test-domain-discovery:
	python3 -c "from benchmark_runner import TestSuiteManager; print('Testing domain discovery...')"

check-server-health:
	curl -s http://localhost:8004/health | jq . || echo "Server not responding"

test-completion-api:
	curl -s -X POST -H "Content-Type: application/json" \
	-d '{"prompt":"Hello","max_tokens":5}' \
	http://localhost:8004/v1/completions | jq .
```

**This policy ensures all execution follows project conventions and maintains consistency for both Python execution and Docker container interaction.**

---

## CODE ATTRIBUTION POLICY

**🚨 NEVER ADD ATTRIBUTION LINES TO FILES**

**Forbidden Attribution Patterns:**
- `Author: Claude Code`
- `Version: X.X.X` 
- `**Auditor:** Claude Code`
- `Generated by Claude Code`
- `Created by Claude Code`
- Any AI-generated attribution headers
- Any mention of "Claude Code" as creator/author/auditor

**Why:** This codebase maintains clean, professional code without AI attribution markers. All code should appear as natural, human-written code without revealing AI assistance.

**Enforcement:** The repository has been systematically cleaned of all attribution lines. Do not re-introduce them in any new files or modifications.

---

## TESTING REQUIREMENTS

### **🚨 CRITICAL STATUS: TESTING INFRASTRUCTURE CRISIS**
- **13/819 functional tests FAILING** (Priority P0 - blocking all production work)
- **57% code coverage** (Target: 90% - UNACCEPTABLE gap)
- **vLLM backend detection BROKEN** (concurrent functionality unusable)

**📋 For detailed crisis analysis and 4-week tactical resolution plan, see:**
**@docs/TESTING_INFRASTRUCTURE_CRISIS_RESPONSE_PLAN.md**

### **Test Structure & Coverage Requirements:**
```
tests/
├── unit/          - Individual component tests (TARGET: 90%+ coverage)
├── integration/   - End-to-end workflow tests  
├── functional/    - Live server tests (MUST PASS 100% - NO EXCEPTIONS)
└── calibration/   - Production calibration validation
```

### **Priority Test Creation (REQUIRED):**
1. **Core Modules** (0-20% coverage → 95%): cognitive_validation.py, resource_manager.py
2. **Calibration Engine** (20% coverage → 90%): calibration_engine.py, production_calibration.py  
3. **Scripts Organization**: Verify reorganized scripts/ import paths work correctly
4. **vLLM Concurrent Testing**: Fix backend detection and concurrent execution with vLLM

### **Mandatory Regression Testing:**
```bash
# CRITICAL: Must pass 100% before ANY production work
make test-functional-validation
make test-regression-full

# Coverage validation
pytest --cov=evaluator --cov=core --cov-fail-under=90
```

**Success Criteria**: See CRITICAL SUCCESS METRICS section below for complete checklist

---

## EVALUATOR INTEGRATION STATUS

### Currently Available Sophisticated Evaluators:
- ✅ PatternBasedEvaluator - Working, integrated
- ❓ CulturalAuthenticityAnalyzer - Partially integrated  
- ❓ EnhancedUniversalEvaluator - Framework ready, needs testing
- ❓ BiasAnalysis/EvaluationAggregator - Framework ready, needs testing

### Integration Requirements:
- Replace basic heuristic scoring with sophisticated evaluator results
- Ensure evaluator results serialize properly for storage
- Handle evaluator failures gracefully with fallback scoring
- Statistical validation of evaluator consistency

---

## CALIBRATION METHODOLOGY

### Statistical Requirements:
- **Multi-sample validation:** 3-5 runs per test for statistical significance
- **Confidence intervals:** ±95% confidence for pattern detection
- **Effect size calculation:** Cohen's d for practical significance
- **Cross-validation:** Ensure patterns are robust across test sets

### Quality Thresholds:
- **✅ Excellent Calibration:** ±2 points from target (within statistical variance)
- **🟡 Good Calibration:** ±5 points from target (production acceptable)
- **🟠 Needs Calibration:** ±10 points from target (requires adjustment)
- **❌ Calibration Broken:** >10 points from target (system failure)

### Production Readiness Criteria:
- **Base Models:** 70%+ tests achieve Good Calibration or better
- **Instruct Models:** 70%+ tests achieve Good Calibration or better  
- **Cross-Domain Consistency:** <10% variance between domains
- **Statistical Significance:** p<0.05 for detected patterns

---

## IMPLEMENTATION PRIORITIES

### **🚨 CRITICAL: TESTING-FIRST APPROACH 🚨**

**MANDATORY PREREQUISITE**: No production domain benchmarking until testing foundation is solid

### **Phase 1: Testing Infrastructure Crisis Resolution (CURRENT PRIORITY)**
**📋 Detailed tactical plan:** @docs/TESTING_INFRASTRUCTURE_CRISIS_RESPONSE_PLAN.md

**Goal**: 100% functional test success + 90% code coverage + vLLM concurrent testing
- **Week 1-2**: Fix 13 failing tests, resolve backend detection issues
- **Week 3-4**: Achieve 90% coverage, optimize performance (eliminate 300s/180s timeouts)
- **Timeline**: 4-week systematic resolution with daily milestones

### **Phase 2: Production Domain Calibration** ✅ **READY**
**Prerequisites**: ✅ All Phase 1 success criteria achieved

**Systematic Base Model Calibration:**
- **Command**: `make systematic-base-calibration`
- **Progression**: Automatic easy → medium → hard with halt-on-failure  
- **Domains**: reasoning, creativity, language, social, knowledge, integration
- **Quality Gates**: ≥70 score (production acceptable), ≥80 score (excellent)

**Instruct Model Calibration:**
- Easy → Medium → Hard progression building on base model results
- Cross-domain consistency validation
- Production readiness assessment

### **Phase 3: Production Deployment**
- Statistical validation reporting
- Performance benchmarking across hardware configurations  
- Production deployment documentation

---

## 🎯 CRITICAL SUCCESS METRICS (MUST ACHIEVE BEFORE DOMAIN BENCHMARKING)

### **Phase 1 Completion Criteria:**
- **✅ 0/819 test failures** (currently 13 failing)
- **✅ 90%+ total test coverage** (currently 57% - UNACCEPTABLE) 
- **✅ 95%+ core/ module coverage** (many currently 0-20%)
- **✅ 100% functional/ test success** (tests/functional/ MUST pass completely)
- **✅ vLLM concurrent functionality tested** (fix test_backend_type_detection_vllm)
- **✅ All timeout issues resolved** (300s, 180s concurrent execution failures)
- **✅ Missing test data created** ('linux_test_01', 'basic_01', 'pattern_completion')
- **✅ Backend detection working** for both llama.cpp and vLLM

### **Mandatory Validation Commands:**
```bash
# MUST pass 100% before ANY domain benchmarking
make test-functional-validation
make test-regression-full

# Coverage must meet requirements
pytest --cov=evaluator --cov=core --cov-fail-under=90
```

### **🚨 DEVELOPMENT POLICY 🚨**
**NO production domains/ benchmarking until ALL success metrics achieved**
**NO exceptions - testing infrastructure MUST be solid first**

---

## vLLM CONCURRENT TESTING REQUIREMENTS

**🚨 CRITICAL ISSUE**: vLLM backend detection broken - `detect_backend_type()` returns 'llama.cpp' instead of 'vLLM'

**Key Requirements:**
- **Backend Detection**: Automatic llama.cpp vs vLLM detection for concurrent execution
- **Performance Validation**: vLLM concurrent execution must outperform sequential
- **Timeout Resolution**: Fix 300s/180s timeout failures in concurrent tests

**📋 Detailed implementation strategy and specific failing tests:** @docs/TESTING_INFRASTRUCTURE_CRISIS_RESPONSE_PLAN.md (Category D: Backend Detection Issues)

---

## PERFORMANCE REQUIREMENTS

### Hardware Optimization (RTX 5090 + AMD 9950X + 128GB DDR5):
- **Response Time:** <30 seconds per domain evaluation
- **Memory Usage:** <16GB peak usage during evaluation
- **GPU Utilization:** Efficient GGUF model loading and inference
- **Concurrent Processing:** Support for multiple domain evaluation

### Scalability Requirements:
- **26,000+ Test Suite:** Full suite completion within reasonable time
- **Cross-Model Comparison:** Support for multiple model endpoints
- **Result Storage:** Efficient storage and retrieval of evaluation results
- **Pattern Analysis:** Real-time cognitive pattern detection

---

## DOCUMENTATION REQUIREMENTS

### Code Documentation:
- **Docstrings:** All public methods with parameter and return type documentation
- **Type Hints:** Complete type annotations for all functions and classes
- **Examples:** Working code examples for each major component
- **Error Handling:** Documentation of all exception cases

### User Documentation:
- **Calibration Guide:** Step-by-step calibration procedures
- **API Reference:** Complete reference for all evaluator interfaces  
- **Troubleshooting:** Common issues and solutions
- **Performance Tuning:** Optimization guidelines for different hardware

---

## QUALITY ASSURANCE

**✅ Comprehensive Quality Audit Completed**  
For detailed system analysis, issue prioritization, and implementation roadmap, see:
- **📄 @docs/COMPREHENSIVE_QUALITY_AUDIT.md** - Complete system analysis with prioritized fixes
- **📋 @docs/TESTING_INFRASTRUCTURE_CRISIS_RESPONSE_PLAN.md** - Tactical plan for resolving 13 failing tests and achieving 90% coverage
- **📖 @docs/guides/MAKEFILE_REFERENCE.md** - Complete command documentation

### Quality Status:
- **Overall Assessment:** GOOD (7.5/10) - Production ready with critical fixes applied
- **Critical Issues:** ✅ Resolved (backend detection, missing scripts, smart concurrency)
- **Performance Tests:** ✅ Created for core modules  
- **Unit Tests:** ✅ Added for missing core modules
- **Smart Concurrency:** ✅ Implemented with backend detection (llama.cpp vs vLLM)

### Key Improvements Implemented:
- **🔍 Backend Detection:** Automatic llama.cpp vs vLLM detection for concurrency control
- **📝 Missing Scripts:** Created systematic_base_calibration.py and multi_model_benchmarking.py
- **⚡ Performance Tests:** Comprehensive performance validation for 26k+ test suite scalability
- **🧪 Unit Tests:** Complete coverage for production_calibration.py and benchmarking_engine.py

---

**🚨 CRITICAL REMINDER:** TESTING INFRASTRUCTURE MUST BE FIXED FIRST! 
- ✅ 100% functional test success (currently 13/819 failing)
- ✅ 90% code coverage (currently 57% - UNACCEPTABLE) 
- ✅ vLLM concurrent functionality fully tested
- **NO domains/ benchmarking until ALL tests pass and coverage requirements are met**
- **NO exceptions - fix the foundation before building on top of it**