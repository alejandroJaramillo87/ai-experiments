# Dockerfile.gpu: Optimized for TensorRT-LLM FP4 Inference on RTX 5090 (Mid-2025)
# Based on official NVIDIA README for Qwen models.
# Target Model: Qwen/Qwen3-30B-A3B

# -----------------------------------------------------------------------------------
# Stage 1: Builder Environment
# Converts Hugging Face weights and compiles the model into a TensorRT FP4 engine.
# -----------------------------------------------------------------------------------
FROM nvcr.io/nvidia/pytorch:25.06-py3 AS builder



ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1
ENV HF_HOME="/root/.cache/huggingface"

# Install build tools and clone the TensorRT-LLM repository.
RUN apt-get update && apt-get install -y --no-install-recommends \
    git \
    git-lfs \
    && rm -rf /var/lib/apt/lists/* \
    && git lfs install

# We clone the main branch to get the latest features for new hardware.
RUN git clone -b main https://github.com/NVIDIA/TensorRT-LLM.git /opt/tensorrt_llm
WORKDIR /opt/tensorrt_llm/examples/models/core/qwen

# Install Python dependencies for the build process.
# The examples folder has its own requirements.
RUN pip install --no-cache-dir -r requirements.txt

# --- STEP 1: Convert Hugging Face model to TensorRT-LLM checkpoint ---
# This step uses the 'convert_checkpoint.py' script as specified in the README.
# It reads the HF model from the mounted volume and saves the converted weights.
# For a single GPU setup, moe_tp_size and moe_ep_size are both 1.
RUN python qwen/convert_checkpoint.py \
      --model_dir /mnt/ai-data/models/hf/Qwen3-30B-A3B-Base \
      --output_dir /app/converted_checkpoint \
      --dtype float16 \
      --moe_tp_size 1 \
      --moe_ep_size 1

# --- STEP 2: Build the TensorRT Engine from the converted checkpoint ---
# This uses the trtllm-build command with FP4 quantization.
# The resulting engine is the final, highly optimized artifact.
# --- STEP 2: Build the TensorRT Engine from the AWQ checkpoint ---
# This uses the trtllm-build command with INT4_AWQ quantization.
RUN trtllm-build \
      --checkpoint_dir /app/converted_checkpoint \
      --output_dir /app/trt_engine \
      --max_batch_size 32 \
      --max_input_len 2048 \
      --max_output_len 2048 \
      --paged_kv_cache \
      --remove_input_padding \
      --use_inflight_batching \
      --quant_mode int4_awq \
      --moe_tp_size 1 \
      --moe_ep_size 1


# --- STEP 3: Prepare runtime files ---
# Copy the essential tokenizer files from the source model directory.
# These are needed by the inference script in the final image.
RUN mkdir -p /app/tokenizer
RUN cp /mnt/ai-data/models/hf/Qwen3-30B-A3B-Base/tokenizer* /app/tokenizer/
RUN cp /mnt/ai-data/models/hf/Qwen3-30B-A3B-Base/merges.txt /app/tokenizer/
RUN cp /mnt/ai-data/models/hf/Qwen3-30B-A3B-Base/vocab.json /app/tokenizer/


# -----------------------------------------------------------------------------------
# Stage 2: Final Runtime Image
# Minimal, secure, and fast. Contains only the engine, tokenizer, and run script.
# -----------------------------------------------------------------------------------
FROM nvcr.io/nvidia/tensorrtllm/python:25.06-py3-runtime

LABEL description="Optimized runtime for Qwen3-30B-A3B FP4 inference."

ENV PYTHONUNBUFFERED=1
ENV NVIDIA_VISIBLE_DEVICES=0

# Create a non-root user for security
RUN groupadd --gid 1000 aiuser \
    && useradd --uid 1000 --gid 1000 -m --shell /bin/bash aiuser

WORKDIR /app
RUN chown -R aiuser:aiuser /app

USER aiuser

# Copy the compiled engine, tokenizer files, and inference script from the builder
COPY --from=builder --chown=aiuser:aiuser /app/trt_engine /app/trt_engine
COPY --from=builder --chown=aiuser:aiuser /app/tokenizer /app/tokenizer
COPY --chown=aiuser:aiuser ./src/run_inference.py /app/run_inference.py

# Install runtime dependencies (transformers for the tokenizer, and tensorrt_llm_bindings)
RUN pip install --no-cache-dir transformers==4.41.2 sentencepiece==0.2.0

# Default command to run inference on a sample prompt
CMD ["python", "/app/run_inference.py"]


python quantize.py --model_dir /mnt/ai-data/models/hf/Qwen3-30B-A3B-Base --qformat int4_awq --awq_block_size 64 --tp_size 4 --output_dir /mnt/ai-data/models/quantized