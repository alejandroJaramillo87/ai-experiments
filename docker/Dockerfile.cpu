# --- Stage 1: The Builder ---
# This stage installs all build tools and compiles the optimized C++ code.
# It is temporary and will be discarded after the build is complete.
FROM python:3.12-slim AS builder

LABEL stage="builder"

# Set build environment variables for AMD Zen 5 architecture
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV CFLAGS="-march=znver5 -mtune=znver5 -O3 -ffast-math -fno-finite-math-only -mavx512f -mavx512vl -mavx512bw -mavx512dq -mavx512cd -mavx512vnni -mavx512vbmi -mavx512vbmi2 -mavx512ifma -mavx512vpopcntdq"
ENV CXXFLAGS="${CFLAGS}"
ENV CC=gcc-14
ENV CXX=g++-14

# Install all build-time system dependencies
RUN echo "deb http://deb.debian.org/debian unstable main" > /etc/apt/sources.list.d/sid.list && \
    apt-get update && \
    apt-get install -y --no-install-recommends \
        gcc-14 g++-14 gfortran-14 build-essential cmake git curl \
        libssl-dev libomp-dev libopenblas-dev liblapack-dev libblis-dev software-properties-common && \
    rm -rf /var/lib/apt/lists/*

# Copy and install AMD Optimized CPU Libraries (AOCL)
# COPY docker/aocl-linux-gcc-5.1.0_1_amd64.deb /tmp/aocl-linux-gcc-5.1.0_1_amd64.deb
# RUN dpkg -i /tmp/aocl-linux-gcc-5.1.0_1_amd64.deb || apt-

# <<< FIX: All AOCL steps are now in a single, atomic RUN command
COPY docker/aocl-linux-gcc-5.1.0_1_amd64.deb /tmp/aocl.deb
RUN PKG_NAME=$(dpkg-deb -f /tmp/aocl.deb Package) && \
    dpkg -i /tmp/aocl.deb || apt-get install -f -y && \
    AOCL_LIB_PATH=$(dpkg -L ${PKG_NAME} | grep 'libblis.so$' | xargs dirname | head -n 1) && \
    echo "Found AOCL package '${PKG_NAME}' with libs at: ${AOCL_LIB_PATH}" && \
    ln -s ${AOCL_LIB_PATH} /opt/aocl_libs && \
    rm /tmp/aocl.deb


# Set environment variables using the new, consistent symlink path
ENV AOCL_ROOT=/opt/aocl_libs
ENV LD_LIBRARY_PATH=${AOCL_ROOT}:${LD_LIBRARY_PATH}

# <<< FIX: Corrected cmake command with proper syntax
RUN git clone https://github.com/ggerganov/llama.cpp.git /tmp/llama.cpp && \
    cd /tmp/llama.cpp && \
    mkdir build && cd build && \
    cmake .. \
        -DBUILD_SHARED_LIBS=ON \
        -DLLAMA_NATIVE=ON \
        -DLLAMA_BLAS=ON \
        -DLLAMA_BLAS_VENDOR=BLIS \
        -DLLAMA_CURL=OFF && \
    make -j16 && \
    make install

# --- Stage 2: The Final Runtime Image ---
# This is the minimal, clean image that will actually run the server.
FROM python:3.12-slim

LABEL description="Container for running a llama.cpp REST API server on Zen 5."

# Set environment variables
ENV PYTHONUNBUFFERED=1
ENV AOCL_ROOT=/opt/AMD/aocl
ENV LD_LIBRARY_PATH=${AOCL_ROOT}/lib:${LD_LIBRARY_PATH}
ENV CC=gcc-14
ENV CXX=g++-14

WORKDIR /app

# Install only the necessary RUNTIME system libraries
RUN echo "deb http://deb.debian.org/debian unstable main" > /etc/apt/sources.list.d/sid.list && \
    apt-get update && \
    apt-get install -y --no-install-recommends \
        libgomp1 \
        gcc-14 \
        g++-14 && \
    rm -rf /var/lib/apt/lists/*

# Copy the essential compiled artifacts FROM THE BUILDER stage
COPY --from=builder /usr/local/lib/libllama.so /usr/local/lib/libllama.so
COPY --from=builder ${AOCL_ROOT}/ ${AOCL_ROOT}/

# Install Python dependencies. llama-cpp-python will find and use the pre-compiled libllama.so
COPY docker/requirements-cpu.txt .
RUN ldconfig && \
    CMAKE_ARGS="-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=BLIS" pip install --no-cache-dir -r requirements-cpu.txt

# Copy the application scripts
COPY src/cpu_server.py .

# Create a non-root user and set permissions
RUN groupadd -r aiuser -g 1001 && useradd -r -g aiuser -u 1001 -m -s /bin/bash aiuser && \
    chown -R aiuser:aiuser /app
USER aiuser

# Expose the network port
EXPOSE 8001

# Set the default command.
