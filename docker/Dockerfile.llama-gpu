# --- Stage 1: The Builder ---
# This stage compiles the CUDA-optimized C++ server executable.
FROM nvidia/cuda:13.0.1-devel-ubuntu24.04 AS builder

LABEL stage="builder"

# Set build environment variables for NVIDIA RTX 5090 (Blackwell architecture)
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV CUDA_DOCKER_ARCH=sm_120 
ENV TORCH_CUDA_ARCH_LIST="12.0"
ENV CUDA_ARCHITECTURES=120
ENV CUDAFLAGS="-O3 --use_fast_math -arch=sm_120"
ENV LDFLAGS="-Wl,-O3 -Wl,--as-needed -L/usr/local/cuda/lib64/stubs"


# Install specific CUDA components for optimal performance
RUN apt-get update && \
    apt-get install -y --no-install-recommends --allow-change-held-packages \
        gcc g++ build-essential cmake git curl ca-certificates \
        libssl-dev pkg-config python3 python3-pip \
        ninja-build ccache \
        cuda-driver-dev-13-0 \
        cuda-cudart-dev-13-0 \
        cuda-cupti-dev-13-0 \
        cuda-nvml-dev-13-0 \
        cuda-nvtx-13-0 \
        cudnn9-cuda-13-0 \
        libcublas-dev-13-0 && \
    rm -rf /var/lib/apt/lists/*

# Set CUDA environment variables
ENV PATH=/usr/local/cuda/bin:${PATH}
ENV LD_LIBRARY_PATH=/usr/local/cuda/lib64/stubs:/usr/local/cuda/lib64:${LD_LIBRARY_PATH}
ENV CUDA_HOME=/usr/local/cuda
ENV PKG_CONFIG_PATH=/usr/local/cuda/lib64/pkgconfig:${PKG_CONFIG_PATH}

# Enable CUDA memory pooling for better performance
ENV CUDA_MODULE_LOADING=LAZY
ENV CUDA_DEVICE_ORDER=FASTEST_FIRST

# Create a symlink for the linker to find the CUDA stub library
RUN ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1


# Build llama.cpp with CUDA optimizations
RUN rm -rf /tmp/llama.cpp && \
    git clone  https://github.com/ggerganov/llama.cpp.git /tmp/llama.cpp && \
    cd /tmp/llama.cpp && \
    mkdir build && cd build && \
    cmake .. \
        -DCMAKE_BUILD_TYPE=Release \
        -DCMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc \
        -DCMAKE_CUDA_ARCHITECTURES=120 \
        -DCMAKE_SHARED_LINKER_FLAGS="-L/usr/local/cuda/lib64/stubs" \
        -DGGML_CUDA=ON \
        -DGGML_CUDA_FORCE_CUBLAS=ON \
        -DGGML_CUDA_F16=ON \
        -DGGML_CUDA_GRAPHS=ON \
        -DGGML_CUDA_PEER_MAX_BATCH_SIZE=256 \
        -DGGML_CUDA_NO_VMM=OFF \
        -DGGML_NATIVE=ON \
        -DGGML_LTO=ON \
        -DGGML_BUILD_TESTS=OFF \
        -DGGML_BUILD_EXAMPLES=OFF \
        -DLLAMA_CURL=OFF \
        -DGGML_CCACHE=ON \
        -DGGML_CUDA_DMMV_X=64 \
        -DGGML_CUDA_MMV_Y=2 \
        -DGGML_CUDA_GRAPHS=ON \
        -DGGML_CUDA_KQUANTS_ITER=2  \
         -DGGML_CUDA_FA_ALL_QUANTS=ON && \
     cmake --build . --target llama-server --config Release -j$(nproc)


# --- Stage 2: The Final Runtime Image ---
FROM nvidia/cuda:13.0.1-runtime-ubuntu24.04

LABEL description="Container for running the CUDA-accelerated llama.cpp REST API server."

# Install only the necessary RUNTIME system libraries
RUN apt-get update && \
    apt-get install -y --no-install-recommends --allow-change-held-packages \
        ca-certificates \
        libgomp1 \
        cudnn9-cuda-13-0 \
        libcublas-13-0 \
        cuda-driver-dev-13-0 \
        cuda-cupti-13-0 \
        cuda-nvtx-13-0 && \
    rm -rf /var/lib/apt/lists/*



# Set runtime CUDA environment
ENV PATH=/usr/local/cuda/bin:${PATH}
ENV LD_LIBRARY_PATH=/usr/local/cuda/lib64:${LD_LIBRARY_PATH}
ENV CUDA_MODULE_LOADING=LAZY
ENV CUDA_DEVICE_ORDER=FASTEST_FIRST
ENV CUDA_VISIBLE_DEVICES=0

# Performance tuning for RTX 5090 (Blackwell, 32GB VRAM)
ENV CUDA_LAUNCH_BLOCKING=0
ENV CUDNN_LOGINFO_DBG=0
ENV CUDNN_LOGDEST_DBG=stderr
ENV CUDA_CACHE_DISABLE=0
ENV CUDA_CACHE_PATH=/tmp/cuda_cache
ENV CUDA_FORCE_PTX_JIT=0
# Enable CUDA graphs for improved performance
ENV GGML_CUDA_GRAPHS=1
# Optimize for single stream processing
ENV GGML_CUDA_MAX_STREAMS=1
# Force synchronous operations for lower latency
ENV GGML_CUDA_FORCE_SYNC=0

# ============================================================================
# GPU OPTIMIZATION NOTES FOR RTX 5090
#
# The following runtime environment variable optimizations were tested but
# provided no performance improvement for llama.cpp (stayed at ~287 tokens/sec):
# - Memory pool optimizations (CUDA_CACHE_MAXSIZE, CUDA_ALLOCATOR_BACKEND, etc.)
# - Tensor core settings (CUDA_TENSOR_CORES, CUBLAS_WORKSPACE_CONFIG, etc.)
# - Blackwell architecture settings (CUDA_L2_PERSISTING_SIZE, etc.)
#
# These optimizations may benefit PyTorch/Python workloads but llama.cpp is
# already highly optimized at compile time through CMAKE flags.
#
# IMPORTANT: Clock locking causes 38% performance degradation - do not enable!
# ============================================================================

# --- PHASE 1: Memory Pool Optimizations ---
# NOTE: These optimizations were tested but provided no performance benefit for llama.cpp
# They may help PyTorch/Python workloads but llama.cpp is already optimized at compile time
# Benchmark results: ~286-287 tokens/sec (no improvement from baseline)
# ENV CUDA_CACHE_MAXSIZE=2147483648  # 2GB kernel cache
# ENV CUDA_ALLOCATOR_BACKEND=cudaMallocAsync  # Better memory allocation
# ENV CUDA_MALLOC_ASYNC_POOLS=1  # Enable async memory pools
# ENV PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,garbage_collection_threshold:0.95,max_split_size_mb:2048

# --- PHASE 2: Tensor Core Optimizations ---
# NOTE: Tested but no performance improvement for llama.cpp (already uses tensor cores via compile flags)
# ENV CUDA_TENSOR_CORES=1  # Enable tensor cores
# ENV CUBLAS_WORKSPACE_CONFIG=:4096:8  # Optimal workspace for tensor cores
# ENV CUDNN_TENSOR_OPS=1  # Enable cuDNN tensor operations  

# --- PHASE 3: Blackwell Architecture Specific ---
# NOTE: Tested but no performance improvement for llama.cpp
# ENV CUDA_L2_PERSISTING_SIZE=100663296  # Use full 96MB L2 cache
# ENV CUDA_DEVICE_MAX_CONNECTIONS=32  # Better concurrent kernels
# ENV CUDA_COPY_SPLIT_THRESHOLD=256  # MB threshold for split copies  

# --- PHASE 4: FP8 Support ---
# TODO: Test with FP8-compatible models only
# ENV CUDA_FP8_E4M3=1  # FP8 E4M3 format
# ENV CUDA_FP8_E5M2=1  # FP8 E5M2 format
# ENV TRANSFORMER_ENGINE_FP8=1  # Transformer engine support

# --- Clock Management ---
# NOTE: CUDA_AUTO_BOOST tested but no performance impact
# ENV CUDA_AUTO_BOOST=0  # Disable auto-boost (keep consistent clocks)
# IMPORTANT: Do NOT add clock locking - causes 38% performance loss on RTX 5090

# NVIDIA GPU settings
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility,graphics
ENV NVIDIA_REQUIRE_CUDA="cuda>=13.0"

# Create a non-root user with a dedicated group and home directory
RUN groupadd -r appuser && useradd -r -g appuser -d /app -s /sbin/nologin -c "Application User" appuser

# Switch to the non-root user
USER appuser

WORKDIR /app

# Copy the server executable from the builder stage
COPY --from=builder --chown=appuser:appuser /tmp/llama.cpp/build/bin/llama-server /app/server
COPY --from=builder --chown=appuser:appuser /tmp/llama.cpp/build/bin/* /app/

# Create cache directory
RUN mkdir -p /tmp/cuda_cache

# Expose the network port
EXPOSE 8004

# Run the CUDA-accelerated server
# Adjust n-gpu-layers based on model size and available VRAM (RTX 5090 has 32GB)
CMD ["./server", \
    "--model", "/app/models/gguf/gpt-oss-20b-GGUF/gpt-oss-20b-UD-Q8_K_XL.gguf", \
    "--host", "0.0.0.0", \
    "--port", "8004", \
    "--n-gpu-layers", "999", \
    "--ctx-size", "65536", \
    "--batch-size", "2048", \
    "--ubatch-size", "512", \
    "--threads", "1", \
    "--threads-batch", "1", \
    # "--cont-batching", \
    "--metrics", \
    "--no-warmup", \
    "--threads-http", "4", \
    "--flash-attn", "on", \
    # "--mlock", \
    "--no-mmap", \
    "--main-gpu", "0", \
    "--parallel", "1" \
    # "--sequences", "1" \
]