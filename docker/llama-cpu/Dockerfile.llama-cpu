# --- Stage 1: The Builder ---
# This stage compiles the optimized C++ server executable.
FROM python:3.12-slim AS builder

LABEL stage="builder"

# Set build environment variables for AMD Zen 5 architecture
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV CFLAGS="-march=znver5 -mtune=znver5 -O3 -ffast-math -fno-finite-math-only -mavx512f -mavx512vl -mavx512bw -mavx512dq -mavx512cd -mavx512vnni -mavx512vbmi -mavx512vbmi2 -mavx512ifma -mavx512vpopcntdq"
ENV CXXFLAGS="${CFLAGS}"
ENV CC=gcc-14
ENV CXX=g++-14
ENV LDFLAGS="-Wl,-O3 -Wl,--as-needed"


# Install all build-time system dependencies
RUN echo "deb http://deb.debian.org/debian unstable main" > /etc/apt/sources.list.d/sid.list && \
    apt-get update && \
    apt-get install -y --no-install-recommends \
        gcc-14 g++-14 gfortran-14 build-essential cmake git curl \
        libssl-dev libomp-dev  libblis-dev software-properties-common pkg-config && \
    rm -rf /var/lib/apt/lists/*

# Install AMD Optimized CPU Libraries (AOCL)
COPY docker/llama-cpu/aocl-linux-gcc-5.1.0_1_amd64.deb /tmp/aocl.deb
RUN PKG_NAME=$(dpkg-deb -f /tmp/aocl.deb Package) && \
    dpkg -i /tmp/aocl.deb || apt-get install -f -y && \
    AOCL_LIB_PATH=$(dpkg -L ${PKG_NAME} | grep 'libblis.so$' | xargs dirname | head -n 1) && \
    echo "Found AOCL package '${PKG_NAME}' with libs at: ${AOCL_LIB_PATH}" && \
    ln -s ${AOCL_LIB_PATH} /opt/aocl_libs && \
    rm /tmp/aocl.deb

# Set environment variables using the new, consistent symlink path
ENV AOCL_ROOT=/opt/aocl_libs
ENV LD_LIBRARY_PATH=${AOCL_ROOT}:${LD_LIBRARY_PATH}

# <<< ADD THIS BLOCK TO CREATE THE SYMLINK BRIDGE >>>
# This "tricks" the linker into using AOCL while satisfying the standard BLAS name.
RUN ln -s /opt/aocl_libs/libblis.so /opt/aocl_libs/libblas.so.3

# Huge Pages Support:
# llama.cpp cannot directly mmap files from hugetlbfs filesystem.
# This wrapper intercepts mmap() calls and when it detects a hugetlbfs file:
# 1. Allocates anonymous memory with MAP_HUGETLB
# 2. Reads the file contents into that memory
# 3. Returns the anonymous memory pointer to llama.cpp
# This allows models on hugetlbfs to benefit from huge pages (reduced TLB pressure)

# Build the hugepage mmap wrapper for hugetlbfs support
# The && operator ensures build fails if compilation errors occur
COPY docker/llama-cpu/hugepage_mmap_wrapper.cpp /tmp/
RUN g++-14 -shared -fPIC -O3 -Wall -o /tmp/hugepage_mmap_wrapper.so /tmp/hugepage_mmap_wrapper.cpp -ldl && \
    echo "Built hugepage_mmap_wrapper.so"

# Build llama.cpp with optimizations (no patches needed)
RUN rm -rf /tmp/llama.cpp && \
    git clone --depth 1  https://github.com/ggerganov/llama.cpp.git /tmp/llama.cpp && \
    cd /tmp/llama.cpp && \
    mkdir build && cd build && \
    cmake .. \
        -DCMAKE_BUILD_TYPE=Release \
        -DGGML_CUDA=OFF \
        -DGGML_BLAS=ON \
        -DGGML_BLAS_VENDOR=Generic \
        -DGGML_SHARED_LIBS=OFF \
        -DGGML_NATIVE=ON \
        -DGGML_LTO=ON \
        -DGGML_BUILD_TESTS=OFF \
        -DGGML_BUILD_EXAMPLES=OFF \
        -DLLAMA_CURL=OFF \
        -DGGML_CCACHE=ON && \
    cmake --build . --config Release -j$(nproc)


# --- Stage 2: The Final Runtime Image ---
FROM debian:unstable-slim

LABEL description="Container for running the native llama.cpp REST API server."

# Install only the necessary RUNTIME system libraries
RUN apt-get update && apt-get install -y --no-install-recommends \
    libgomp1 \
    python3 \
    && rm -rf /var/lib/apt/lists/*

# Copy the compiled AOCL libraries from the builder stage
COPY --from=builder /opt/aocl_libs /opt/aocl_libs

# # Set the library path so the system can find the AOCL libs
ENV LD_LIBRARY_PATH=/opt/aocl_libs

# Create a non-root user with a dedicated group and home directory
RUN groupadd -r appuser && useradd -r -g appuser -d /app -s /sbin/nologin -c "Application User" appuser

# Switch to the non-root user
USER appuser

WORKDIR /app

# Copy the server executable and other necessary files, setting ownership
COPY --from=builder --chown=appuser:appuser /tmp/llama.cpp/build/bin/llama-server /app/server
COPY --from=builder --chown=appuser:appuser /tmp/llama.cpp/build/bin/* /app/
# Copy the hugepage wrapper library
COPY --from=builder --chown=appuser:appuser /tmp/hugepage_mmap_wrapper.so /app/
# Copy entrypoint script
COPY --chown=appuser:appuser docker/llama-cpu/entrypoint.sh /app/entrypoint.sh

# Optimal settings
ENV OMP_NUM_THREADS=12
ENV OMP_PROC_BIND=true
ENV OMP_PLACES=cores

# Make entrypoint executable
USER root
RUN chmod +x /app/entrypoint.sh
USER appuser

# Expose the network port (dynamic based on environment)
EXPOSE 8001

# Use entrypoint script for parameterized server configuration
ENTRYPOINT ["/app/entrypoint.sh"]


