I have auto-accept mode enabled. Please evaluate LLM benchmark test results comprehensively and autonomously.

TASK OVERVIEW:
Analyze LLM benchmark test results to provide actionable insights on model performance, identify patterns, and generate a detailed evaluation report. Focus on @tests/benchmark_tests/test_results_gpt-oss-20b/ and @tests/benchmark_tests/test_results_qwen3/ and @tests/benchmark_tests/test_runner.py. Do NOT use evaluation scores from @tests/benchmark_tests/reasoning_evaluator.py - conduct your own independent evaluation of the raw benchmark results.

SPECIFIC INSTRUCTIONS:
1. **Discovery Phase**
   - Read and analyze @tests/benchmark_tests/test_results_gpt-oss-20b/ directory contents
   - Read and analyze @tests/benchmark_tests/test_results_qwen3/ directory contents  
   - Examine @tests/benchmark_tests/test_runner.py to understand test methodology and structure
   - Identify all result files (JSON, CSV, TXT, logs, etc.) in the specified directories
   - Map file formats and data structures across both model result sets
   - Use TodoWrite to track all discovered files and their contents

2. **Data Analysis Tasks**
   - Parse and consolidate raw results from both gpt-oss-20b and qwen3 result directories
   - Calculate your own performance metrics (accuracy, response quality, task completion rates, etc.)
   - Compare gpt-oss-20b vs qwen3 performance across all available benchmarks
   - Identify best and worst performing areas for each model
   - Analyze performance across different benchmark categories (reasoning, coding, math, etc.)
   - Calculate statistical significance of performance differences between the two models
   - Ignore any pre-computed scores from reasoning_evaluator.py - base analysis only on raw outputs

3. **Pattern Recognition**
   - Identify domains where each model excels or struggles
   - Find correlations between test types and model performance patterns
   - Detect performance anomalies or unexpected results in either model
   - Analyze consistency of performance within each model's results
   - Compare response patterns and quality differences

4. **Generate Insights**
   - Rank models by overall performance across all benchmarks
   - Provide domain-specific recommendations (when to use gpt-oss-20b vs qwen3)
   - Identify performance gaps and improvement opportunities for each model  
   - Flag any concerning results, outliers, or failure patterns
   - Assess relative strengths and weaknesses

5. **Create Deliverables**
   - Generate a comprehensive evaluation report comparing both models (markdown format)
   - Create detailed summary tables comparing key metrics between gpt-oss-20b and qwen3
   - Generate visualizations if possible (comparison charts, performance graphs)
   - Provide actionable recommendations for model selection based on use case
   - Create an executive summary with key comparative findings

AUTONOMOUS EXECUTION GUIDELINES:
- Work through each task systematically without asking for permission
- Use TodoWrite/TodoRead to track progress through the analysis
- Focus exclusively on the specified directories and test_runner.py for context
- If you encounter unfamiliar file formats, use your best judgment to parse them
- Give me a brief progress update after completing each major section
- If you find missing data, note it but continue with available data from the specified sources
- Save all outputs to appropriately named files
- Conduct all evaluations independently - do not reference or use reasoning_evaluator.py scores

OUTPUT REQUIREMENTS:
- Main report: `gpt-oss-20b_vs_qwen3_evaluation_report.md`
- Summary tables: `model_comparison_summary_tables.md` 
- Raw analysis data: `processed_benchmark_comparison_data.json`
- Executive summary: `executive_summary_model_comparison.md`

Begin the evaluation now. Start by examining the test_runner.py to understand the benchmark structure, then systematically analyze both model result directories to generate your independent comparative evaluation.