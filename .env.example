# Model paths for AI inference containers
# These paths are relative to the container's /app/models mount point

# CPU Inference Containers
LLAMA_CPU0_MODEL=/app/models/gguf/your-model-here.gguf
LLAMA_CPU1_MODEL=/app/models/gguf/your-model-here.gguf
LLAMA_CPU2_MODEL=/app/models/gguf/your-model-here.gguf

# GPU Inference Containers
LLAMA_GPU_MODEL=/app/models/gguf/your-model-here.gguf
VLLM_GPU_MODEL=/app/models/safetensors/your-model-directory

# Additional configuration (optional)
# You can add other environment variables here as needed
# THREADS=8
# CTX_SIZE=32768