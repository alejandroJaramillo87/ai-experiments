# requirements-gpu.txt - For TensorRT-LLM Inference (Mid-2025)
# Core web framework
fastapi==0.115.14
uvicorn[standard]==0.34.3
pydantic==2.9.0

# TensorRT-LLM specific libraries (check exact version compatibility with NGC image)
tensorrt_llm==0.12.0 # Speculative version for mid-2025, check NGC
# You generally don't need 'torch' for TRT-LLM *inference* as TRT-LLM handles the graph.
# If your pre/post-processing still heavily uses PyTorch tensors, keep it, but generally remove.
# torch==2.7.0+cu121 # Only if needed for non-TRT-LLM parts of your code.
# transformers and sentence-transformers for tokenization/pre-processing
transformers>=4.53.0
sentence-transformers>=3.1.0

# Essential tokenization and model utilities
sentencepiece>=0.2.0
einops>=0.8.1
safetensors>=0.4.0

# Utilities
numpy>=2.1.0
scipy>=1.14.0
requests>=2.32.0
aiofiles>=24.1.0
python-multipart>=0.0.12

# Monitoring
psutil>=6.0.0
pynvml>=11.5.0